{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If GPU is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = 13\n",
    "set_values = [4,5]\n",
    "model_name = 's_'+str(shift)+'_'+str(set_values)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with scaling\n",
    "df_human = pd.read_csv(\"data_human_filtered.csv\")\n",
    "df_human = df_human.drop(columns='id')\n",
    "file = \"data_model_v2.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "#df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#df = df[columns_to_keep]\n",
    "# Step 1: Separate 'id' and 'trial' columns from the rest of the data\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "\n",
    "# Step 2: Apply MinMaxScaler to the remaining columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "\n",
    "# Step 3: Merge 'id' and 'trial' columns with the scaled data\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "\n",
    "# Step 4: create variable t-1\n",
    "# List of columns to shift\n",
    "columns_to_shift = ['s_1', 's_2', 's_3','s_4']  # List all column names here\n",
    "\n",
    "# Create shifted columns for each column in the list\n",
    "for col in columns_to_shift:\n",
    "    new_col_name = col + '_minus_' + str(shift)\n",
    "    scaled_df[new_col_name] = scaled_df[col].shift(shift)\n",
    "    scaled_df[new_col_name] = scaled_df[new_col_name].fillna(1)  # Fill NaN in the first row with 1\n",
    "    scaled_df[new_col_name] = scaled_df[new_col_name].astype(int)\n",
    "\n",
    "#desired_column_order = [col for col in scaled_df.columns if col not in columns_to_shift] + columns_to_shift\n",
    "#scaled_df = scaled_df[desired_column_order]\n",
    "selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "scaled_df = scaled_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = scaled_df[-scaled_df['trial'].isin(set_values)].drop(columns=['id', 'trial'])\n",
    "test_set = scaled_df[scaled_df['trial']==set_values[0]].drop(columns=['id', 'trial'])\n",
    "val_set = scaled_df[scaled_df['trial']==set_values[1]].drop(columns=['id', 'trial'])\n",
    "full_set = scaled_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "\n",
    "\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "# Create a DataLoader\n",
    "#batch_size = 5561#67  # Set your desired batch size\n",
    "#shuffle = False  # Set to False to preserve the order of your data\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)\n",
    "        #Xavier initialization for GRU weights\n",
    "        #for name, param in self.gru.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        init.xavier_uniform_(param.data)\n",
    "        #    elif 'bias' in name:\n",
    "        #        init.constant_(param.data, 0.0)\n",
    "                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sotfplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.sotfplus(out[:,-1]))\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out, h\n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "        #weight = next(self.parameters()).data\n",
    "        #hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        #return hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size > 1:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]  # Update input_t with the new hidden state for the next layer\n",
    "            hidden_states.append(new_hidden_states)\n",
    "        \n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        # Apply BatchNorm to the last hidden state\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        #all_hidden_states = torch.cat(hidden_states, dim=1)  # Stack all hidden states across time steps\n",
    "        \n",
    "        \n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "        #out = self.fc(self.softplus(all_hidden_states[:, -1]))  # Use the last hidden state for prediction\n",
    "        #probs = torch.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "        #preds = torch.round(probs)  # Convert probabilities to binary predictions\n",
    "        #out = F.sigmoid(out, dim=1)\n",
    "        \n",
    "        return out, last_hidden_states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =251\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.973007082939148, train acc: 0.5710994075049375, val loss: 0.9252921342849731, val acc: 0.6156462585034014\n",
      "Epoch 50, train loss: 0.1900331974029541, train acc: 0.9609529295589203, val loss: 0.2454943209886551, val acc: 0.95578231292517\n",
      "Epoch 100, train loss: 0.1826012283563614, train acc: 0.9609529295589203, val loss: 0.21370013058185577, val acc: 0.95578231292517\n",
      "Epoch 150, train loss: 0.18041697144508362, train acc: 0.9609529295589203, val loss: 0.20905563235282898, val acc: 0.95578231292517\n",
      "Epoch 200, train loss: 0.1794201135635376, train acc: 0.9609529295589203, val loss: 0.21031486988067627, val acc: 0.95578231292517\n",
      "Epoch 250, train loss: 0.17882075905799866, train acc: 0.9609529295589203, val loss: 0.21055826544761658, val acc: 0.95578231292517\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_losses, val_accs, train_predicted, val_predicted, train_probs, val_probs= run_training(\n",
    "    train_dataloader, val_dataloader=val_dataloader, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95578231292517"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_1_minus_13</th>\n",
       "      <th>s_2_minus_13</th>\n",
       "      <th>s_3_minus_13</th>\n",
       "      <th>s_4_minus_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6071</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6072</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6073</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6076 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      s_1_minus_13  s_2_minus_13  s_3_minus_13  s_4_minus_13\n",
       "0                1             1             1             1\n",
       "1                1             1             1             1\n",
       "2                1             1             1             1\n",
       "3                1             1             1             1\n",
       "4                1             1             1             1\n",
       "...            ...           ...           ...           ...\n",
       "6071             1             1             0             0\n",
       "6072             1             1             0             0\n",
       "6073             1             1             0             0\n",
       "6074             1             1             0             0\n",
       "6075             1             1             0             0\n",
       "\n",
       "[6076 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the folder if it doesn't exist\n",
    "#os.makedirs(save_folder, exist_ok=True)\n",
    "#os.makedirs(folder_path, exist_ok=True)\n",
    "# Assuming your model is named 'model'\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "# Specify the folder path and the model filename\n",
    "folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "model_filename = model_name + '.pth'  \n",
    "\n",
    "# Combine the folder path and model filename\n",
    "full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "# Save the model to the specified folder\n",
    "torch.save(state_dict, full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUCellNet(\n",
       "  (gru): GRUCell(4, 34)\n",
       "  (fc): Linear(in_features=34, out_features=4, bias=True)\n",
       "  (softplus): Softplus(beta=1, threshold=20)\n",
       "  (relu): ReLU()\n",
       "  (gru_cells): ModuleList(\n",
       "    (0): GRUCell(4, 34)\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved weights into the model\n",
    "#torch.save(model.state_dict(), 'model_weights.pth')\n",
    "#zmodel = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model.load_state_dict(torch.load(full_model))\n",
    "model.eval()  # Set the model in evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          s_1       s_2       s_3       s_4\n",
      "0    0.984346  0.997641  0.981565  0.918865\n",
      "1    0.984346  0.997641  0.981565  0.918865\n",
      "2    0.984346  0.997641  0.981565  0.918865\n",
      "3    0.984346  0.997641  0.981565  0.918865\n",
      "4    0.984346  0.997641  0.981565  0.918865\n",
      "..        ...       ...       ...       ...\n",
      "430  0.160832  0.157984  0.155199  0.997321\n",
      "431  0.160832  0.157984  0.155199  0.997321\n",
      "432  0.160832  0.157984  0.155199  0.997321\n",
      "433  0.160832  0.157984  0.155199  0.997321\n",
      "434  0.160832  0.157984  0.155199  0.997321\n",
      "\n",
      "[435 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "# Initialize an empty list to store predictions\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in test_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        preds = torch.round(probabilities)\n",
    "\n",
    "    # Append predictions to the list\n",
    "    all_preds.append(preds)\n",
    "    all_probs.append(probabilities)\n",
    "\n",
    "# Concatenate the predicted batches\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "all_preds_array = all_preds.numpy()\n",
    "all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "columns = ['s_1','s_2','s_3','s_4']\n",
    "all_probs_df = pd.DataFrame(all_probs_array)\n",
    "all_probs_df.columns = columns\n",
    "print(all_probs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each output state:\n",
      "0.9775862068965517\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor of predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "# Calculate accuracy for each output state\n",
    "accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "print(\"Accuracy for each output state:\")\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 6)) \n",
    "# Add labels and a title\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.subplot(2, 1, 1)  # 2 rows, 1 column, plot 1\n",
    "plt.title(\"Probabilities heatmap for \"+model_name+' test set')  # Replace with your title\n",
    "sns.heatmap(all_probs_df.transpose(),linewidth=.001, cmap=\"magma\")\n",
    "plt.xlabel(\"Timesteps\")  # Replace with your x-axis label\n",
    "plt.ylabel(\"States probabilities\")  # Replace with your y-axis label\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.heatmap(y_test.transpose(),linewidth=.001, cmap=\"magma\")\n",
    "plt.xlabel(\"Timesteps\")  # Replace with your x-axis label\n",
    "plt.ylabel(\"True States\")  # Replace with your y-axis label\n",
    "\n",
    "\n",
    "# Define the folder where you want to save the plot\n",
    "save_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/plots'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "#os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Specify the full file path including the folder to save the plot\n",
    "save_path = os.path.join(save_folder,model_name+'_test.png')\n",
    "\n",
    "# Save the plot to the specified folder and file\n",
    "plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "plt.savefig(model_name+'_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          s_1       s_2       s_3       s_4\n",
      "0    0.160832  0.157984  0.155199  0.997321\n",
      "1    0.160832  0.157984  0.155199  0.997321\n",
      "2    0.160832  0.157984  0.155199  0.997321\n",
      "3    0.160832  0.157984  0.155199  0.997321\n",
      "4    0.160832  0.157984  0.155199  0.997321\n",
      "..        ...       ...       ...       ...\n",
      "436  0.312212  0.282847  0.990955  0.281307\n",
      "437  0.312212  0.282847  0.990955  0.281307\n",
      "438  0.312212  0.282847  0.990955  0.281307\n",
      "439  0.312212  0.282847  0.990955  0.281307\n",
      "440  0.312212  0.282847  0.990955  0.281307\n",
      "\n",
      "[441 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "# Initialize an empty list to store predictions\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in val_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        preds = torch.round(probabilities)\n",
    "\n",
    "    # Append predictions to the list\n",
    "    all_preds.append(preds)\n",
    "    all_probs.append(probabilities)\n",
    "\n",
    "# Concatenate the predicted batches\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "all_preds_array = all_preds.numpy()\n",
    "all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "columns = ['s_1','s_2','s_3','s_4']\n",
    "all_probs_df = pd.DataFrame(all_probs_array)\n",
    "all_probs_df.columns = columns\n",
    "print(all_probs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 6)) \n",
    "# Add labels and a title\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.subplot(2, 1, 1)  # 2 rows, 1 column, plot 1\n",
    "plt.title(\"Probabilities heatmap for \"+model_name+' val set')  # Replace with your title\n",
    "sns.heatmap(all_probs_df.transpose(),linewidth=.001, cmap=\"magma\")\n",
    "plt.xlabel(\"Timesteps\")  # Replace with your x-axis label\n",
    "plt.ylabel(\"States probabilities\")  # Replace with your y-axis label\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.heatmap(y_val.transpose(),linewidth=.001, cmap=\"magma\")\n",
    "plt.xlabel(\"Timesteps\")  # Replace with your x-axis label\n",
    "plt.ylabel(\"True States\")  # Replace with your y-axis label\n",
    "\n",
    "\n",
    "# Define the folder where you want to save the plot\n",
    "save_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/plots'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "#os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "# Specify the full file path including the folder to save the plot\n",
    "save_path = os.path.join(save_folder,model_name+'_val.png')\n",
    "\n",
    "# Save the plot to the specified folder and file\n",
    "#plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n",
    "\n",
    "#plt.savefig(model_name+'_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_1_minus_13</th>\n",
       "      <th>s_2_minus_13</th>\n",
       "      <th>s_3_minus_13</th>\n",
       "      <th>s_4_minus_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     s_1_minus_13  s_2_minus_13  s_3_minus_13  s_4_minus_13\n",
       "0               0             0             0             1\n",
       "1               0             0             0             1\n",
       "2               0             0             0             1\n",
       "3               0             0             0             1\n",
       "4               0             0             0             1\n",
       "..            ...           ...           ...           ...\n",
       "436             0             0             1             0\n",
       "437             0             0             1             0\n",
       "438             0             0             1             0\n",
       "439             0             0             1             0\n",
       "440             0             0             1             0\n",
       "\n",
       "[441 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each output state:\n",
      "0.9775862068965517\n"
     ]
    }
   ],
   "source": [
    "#shift = 10\n",
    "#model_name = 's_minus_10'\n",
    "\n",
    "# Specify the folder path and the model filename\n",
    "folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "model_filename = model_name + '.pth'  \n",
    "\n",
    "# Combine the folder path and model filename\n",
    "full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Initialize an empty list to store predictions\n",
    "all_preds = []\n",
    "all_probs = []\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in test_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        preds = torch.round(probabilities)\n",
    "\n",
    "    # Append predictions to the list\n",
    "    all_preds.append(preds)\n",
    "    all_probs.append(probabilities)\n",
    "\n",
    "# Concatenate the predicted batches\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "all_preds_array = all_preds.numpy()\n",
    "all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "columns = ['s_1','s_2','s_3','s_4']\n",
    "all_probs_df = pd.DataFrame(all_probs_array)\n",
    "all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "# Convert the tensor of predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_test.columns, index=y_test.index)\n",
    "# Calculate accuracy for each output state\n",
    "accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "print(\"Accuracy for each output state:\")\n",
    "print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     s_1  s_2  s_3  s_4\n",
      "0    1.0  1.0  1.0  1.0\n",
      "1    1.0  1.0  1.0  1.0\n",
      "2    1.0  1.0  1.0  1.0\n",
      "3    1.0  1.0  1.0  1.0\n",
      "4    1.0  1.0  1.0  1.0\n",
      "..   ...  ...  ...  ...\n",
      "430  0.0  0.0  0.0  1.0\n",
      "431  0.0  0.0  0.0  1.0\n",
      "432  0.0  0.0  0.0  1.0\n",
      "433  0.0  0.0  0.0  1.0\n",
      "434  0.0  0.0  0.0  1.0\n",
      "\n",
      "[435 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 46\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46, 49,\n",
       "       52])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1,53,step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.2294036149978638, train acc: 0.5429146177916853, val loss: 1.1537772417068481, val acc: 0.675\n",
      "Epoch 50, train loss: 0.060177240520715714, train acc: 0.9984726568320668, val loss: 0.10350076109170914, val acc: 0.9984375\n",
      "Epoch 100, train loss: 0.01865452714264393, train acc: 0.9984726568320668, val loss: 0.01799849048256874, val acc: 0.9984375\n",
      "Epoch 150, train loss: 0.01383298821747303, train acc: 0.9984726568320668, val loss: 0.013840831816196442, val acc: 0.9984375\n",
      "Epoch 200, train loss: 0.012035188265144825, train acc: 0.9984726568320668, val loss: 0.01263814140111208, val acc: 0.9984375\n",
      "Value 's_m_1_val1: 0.9984375' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.03216571360826492, train acc: 0.9938521517468886, val loss: 0.026989759877324104, val acc: 0.9936305732484076\n",
      "Epoch 50, train loss: 0.026039229705929756, train acc: 0.9938521517468886, val loss: 0.03237958997488022, val acc: 0.9936305732484076\n",
      "Epoch 100, train loss: 0.024298347532749176, train acc: 0.9938521517468886, val loss: 0.03526053577661514, val acc: 0.9936305732484076\n",
      "Epoch 150, train loss: 0.023125918582081795, train acc: 0.9938521517468886, val loss: 0.03684090077877045, val acc: 0.9936305732484076\n",
      "Epoch 200, train loss: 0.022265926003456116, train acc: 0.9938521517468886, val loss: 0.03835349529981613, val acc: 0.9936305732484076\n",
      "Value 's_m_4_val1: 0.9936305732484076' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.033520620316267014, train acc: 0.9891730798249585, val loss: 0.04509840905666351, val acc: 0.987012987012987\n",
      "Epoch 50, train loss: 0.030059343203902245, train acc: 0.989965293496303, val loss: 0.06820231676101685, val acc: 0.9805194805194806\n",
      "Epoch 100, train loss: 0.02894490770995617, train acc: 0.9901916402595443, val loss: 0.07546151429414749, val acc: 0.9788961038961039\n",
      "Epoch 150, train loss: 0.02813742123544216, train acc: 0.9904179870227856, val loss: 0.07973766326904297, val acc: 0.9756493506493507\n",
      "Epoch 200, train loss: 0.027522698044776917, train acc: 0.9906820582465671, val loss: 0.08413178473711014, val acc: 0.9756493506493507\n",
      "Value 's_m_7_val1: 0.9886363636363636' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.03631937503814697, train acc: 0.9861427486712224, val loss: 0.08355273306369781, val acc: 0.9768211920529801\n",
      "Epoch 50, train loss: 0.03389725461602211, train acc: 0.9895596051632498, val loss: 0.1121223121881485, val acc: 0.9718543046357616\n",
      "Epoch 100, train loss: 0.03320283815264702, train acc: 0.9899392558845862, val loss: 0.1164388358592987, val acc: 0.9701986754966887\n",
      "Epoch 150, train loss: 0.032693445682525635, train acc: 0.990167046317388, val loss: 0.11707060784101486, val acc: 0.9718543046357616\n",
      "Epoch 200, train loss: 0.032302822917699814, train acc: 0.9903189066059226, val loss: 0.11801725625991821, val acc: 0.9718543046357616\n",
      "Value 's_m_10_val1: 0.9801324503311258' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.03979234769940376, train acc: 0.98677976463396, val loss: 0.10955704748630524, val acc: 0.9763513513513513\n",
      "Epoch 50, train loss: 0.03792184963822365, train acc: 0.9880406541341892, val loss: 0.14211224019527435, val acc: 0.9712837837837838\n",
      "Epoch 100, train loss: 0.03744311258196831, train acc: 0.988346324316063, val loss: 0.14161813259124756, val acc: 0.9712837837837838\n",
      "Epoch 150, train loss: 0.03710649535059929, train acc: 0.9883845330887971, val loss: 0.14132294058799744, val acc: 0.9712837837837838\n",
      "Epoch 200, train loss: 0.03685353323817253, train acc: 0.9884227418615315, val loss: 0.14087942242622375, val acc: 0.9712837837837838\n",
      "Value 's_m_13_val1: 0.9763513513513513' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.044185955077409744, train acc: 0.9849253960929087, val loss: 0.11828077584505081, val acc: 0.9758620689655172\n",
      "Epoch 50, train loss: 0.042758867144584656, train acc: 0.9862328872481156, val loss: 0.14625610411167145, val acc: 0.9758620689655172\n",
      "Epoch 100, train loss: 0.04241590201854706, train acc: 0.9866174434702354, val loss: 0.1481073796749115, val acc: 0.9724137931034482\n",
      "Epoch 150, train loss: 0.04218875244259834, train acc: 0.9866174434702354, val loss: 0.14857880771160126, val acc: 0.9724137931034482\n",
      "Epoch 200, train loss: 0.04202231392264366, train acc: 0.9866558990924473, val loss: 0.14852692186832428, val acc: 0.9724137931034482\n",
      "Value 's_m_16_val1: 0.9758620689655172' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.049749236553907394, train acc: 0.9828920885586004, val loss: 0.11916317790746689, val acc: 0.977112676056338\n",
      "Epoch 50, train loss: 0.04868749901652336, train acc: 0.9835113794705063, val loss: 0.1418733447790146, val acc: 0.9683098591549296\n",
      "Epoch 100, train loss: 0.04839862510561943, train acc: 0.9837823192444651, val loss: 0.14529435336589813, val acc: 0.9665492957746479\n",
      "Epoch 150, train loss: 0.04822228103876114, train acc: 0.9839371419724415, val loss: 0.145650252699852, val acc: 0.9647887323943662\n",
      "Epoch 200, train loss: 0.048099350184202194, train acc: 0.9839371419724415, val loss: 0.14568087458610535, val acc: 0.9647887323943662\n",
      "Value 's_m_19_val1: 0.977112676056338' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05641308054327965, train acc: 0.9802088203210223, val loss: 0.11775929480791092, val acc: 0.9694244604316546\n",
      "Epoch 50, train loss: 0.055656835436820984, train acc: 0.980091943275674, val loss: 0.1435367614030838, val acc: 0.9676258992805755\n",
      "Epoch 100, train loss: 0.05540955811738968, train acc: 0.9804425744117189, val loss: 0.1434650868177414, val acc: 0.9676258992805755\n",
      "Epoch 150, train loss: 0.05526145175099373, train acc: 0.9804036153966028, val loss: 0.14416635036468506, val acc: 0.9676258992805755\n",
      "Epoch 200, train loss: 0.05516084283590317, train acc: 0.9804425744117189, val loss: 0.14462722837924957, val acc: 0.9676258992805755\n",
      "Value 's_m_22_val1: 0.9694244604316546' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0643458366394043, train acc: 0.9764705882352941, val loss: 0.11462654173374176, val acc: 0.9724264705882353\n",
      "Epoch 50, train loss: 0.06385717540979385, train acc: 0.9765882352941176, val loss: 0.12945623695850372, val acc: 0.9705882352941176\n",
      "Epoch 100, train loss: 0.06364508718252182, train acc: 0.9769411764705882, val loss: 0.1348211020231247, val acc: 0.96875\n",
      "Epoch 150, train loss: 0.0635211169719696, train acc: 0.9769803921568627, val loss: 0.13668294250965118, val acc: 0.96875\n",
      "Epoch 200, train loss: 0.06343697011470795, train acc: 0.9770588235294118, val loss: 0.13756826519966125, val acc: 0.96875\n",
      "Value 's_m_25_val1: 0.9724264705882353' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07398880273103714, train acc: 0.9731959576819833, val loss: 0.11137944459915161, val acc: 0.9736842105263158\n",
      "Epoch 50, train loss: 0.07371833175420761, train acc: 0.9733538607295121, val loss: 0.11777763068675995, val acc: 0.9736842105263158\n",
      "Epoch 100, train loss: 0.07354993373155594, train acc: 0.9733538607295121, val loss: 0.12107186764478683, val acc: 0.9736842105263158\n",
      "Epoch 150, train loss: 0.07344526052474976, train acc: 0.9733933364913943, val loss: 0.12299280613660812, val acc: 0.9736842105263158\n",
      "Epoch 200, train loss: 0.07337464392185211, train acc: 0.9734328122532765, val loss: 0.12421303987503052, val acc: 0.9718045112781954\n",
      "Value 's_m_28_val1: 0.9736842105263158' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08470597118139267, train acc: 0.9691225560324273, val loss: 0.09846913069486618, val acc: 0.9769230769230769\n",
      "Epoch 50, train loss: 0.08456569910049438, train acc: 0.9692020346526784, val loss: 0.10138475894927979, val acc: 0.9769230769230769\n",
      "Epoch 100, train loss: 0.0844675600528717, train acc: 0.9692020346526784, val loss: 0.1025368720293045, val acc: 0.9769230769230769\n",
      "Epoch 150, train loss: 0.08440113067626953, train acc: 0.969241773962804, val loss: 0.10329955071210861, val acc: 0.9769230769230769\n",
      "Epoch 200, train loss: 0.08435465395450592, train acc: 0.969241773962804, val loss: 0.10382822901010513, val acc: 0.9769230769230769\n",
      "Value 's_m_31_val1: 0.9769230769230769' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09523878991603851, train acc: 0.9653944631140983, val loss: 0.08641553670167923, val acc: 0.9822834645669292\n",
      "Epoch 50, train loss: 0.09517626464366913, train acc: 0.9654344695151225, val loss: 0.08797280490398407, val acc: 0.9803149606299213\n",
      "Epoch 100, train loss: 0.09513075649738312, train acc: 0.9654344695151225, val loss: 0.08826515823602676, val acc: 0.9803149606299213\n",
      "Epoch 150, train loss: 0.09509855508804321, train acc: 0.9654344695151225, val loss: 0.08846783638000488, val acc: 0.9803149606299213\n",
      "Epoch 200, train loss: 0.09507540613412857, train acc: 0.9654344695151225, val loss: 0.08861316740512848, val acc: 0.9803149606299213\n",
      "Value 's_m_34_val1: 0.9822834645669292' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10735613852739334, train acc: 0.9610923151280812, val loss: 0.07021452486515045, val acc: 0.9858870967741935\n",
      "Epoch 50, train loss: 0.10732930153608322, train acc: 0.9610923151280812, val loss: 0.07132784277200699, val acc: 0.9858870967741935\n",
      "Epoch 100, train loss: 0.1073092445731163, train acc: 0.9610923151280812, val loss: 0.07139933854341507, val acc: 0.9858870967741935\n",
      "Epoch 150, train loss: 0.1072949543595314, train acc: 0.9610923151280812, val loss: 0.0714469850063324, val acc: 0.9858870967741935\n",
      "Epoch 200, train loss: 0.10728441923856735, train acc: 0.9610923151280812, val loss: 0.07148203253746033, val acc: 0.9858870967741935\n",
      "Value 's_m_37_val1: 0.9858870967741935' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12104331701993942, train acc: 0.9564071370640713, val loss: 0.056656643748283386, val acc: 0.9917355371900827\n",
      "Epoch 50, train loss: 0.12103205174207687, train acc: 0.9564071370640713, val loss: 0.05758073925971985, val acc: 0.9917355371900827\n",
      "Epoch 100, train loss: 0.1210237666964531, train acc: 0.9564071370640713, val loss: 0.05759776011109352, val acc: 0.9917355371900827\n",
      "Epoch 150, train loss: 0.1210181787610054, train acc: 0.9564071370640713, val loss: 0.05760641768574715, val acc: 0.9917355371900827\n",
      "Epoch 200, train loss: 0.12101409584283829, train acc: 0.9564476885644769, val loss: 0.05761289596557617, val acc: 0.9917355371900827\n",
      "Value 's_m_40_val1: 0.9917355371900827' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13456718623638153, train acc: 0.9518618324350808, val loss: 0.046999555081129074, val acc: 0.9978813559322034\n",
      "Epoch 50, train loss: 0.13456319272518158, train acc: 0.9518618324350808, val loss: 0.04778483137488365, val acc: 0.9978813559322034\n",
      "Epoch 100, train loss: 0.13456091284751892, train acc: 0.9518618324350808, val loss: 0.047789111733436584, val acc: 0.9978813559322034\n",
      "Epoch 150, train loss: 0.1345592588186264, train acc: 0.9518618324350808, val loss: 0.04778995364904404, val acc: 0.9978813559322034\n",
      "Epoch 200, train loss: 0.13455837965011597, train acc: 0.9518618324350808, val loss: 0.04779006540775299, val acc: 0.9978813559322034\n",
      "Value 's_m_43_val1: 0.9978813559322034' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14819982647895813, train acc: 0.9475826344351258, val loss: 0.04673134163022041, val acc: 0.9956521739130435\n",
      "Epoch 50, train loss: 0.1481991857290268, train acc: 0.9475826344351258, val loss: 0.047427307814359665, val acc: 0.9956521739130435\n",
      "Epoch 100, train loss: 0.1481986939907074, train acc: 0.9475826344351258, val loss: 0.047430142760276794, val acc: 0.9956521739130435\n",
      "Epoch 150, train loss: 0.14819853007793427, train acc: 0.9475826344351258, val loss: 0.047430045902729034, val acc: 0.9956521739130435\n",
      "Epoch 200, train loss: 0.14819836616516113, train acc: 0.9475826344351258, val loss: 0.04743004962801933, val acc: 0.9956521739130435\n",
      "Value 's_m_46_val1: 0.9956521739130435' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16322638094425201, train acc: 0.9433681073025335, val loss: 0.05309236794710159, val acc: 0.9888392857142857\n",
      "Epoch 50, train loss: 0.16322626173496246, train acc: 0.9433681073025335, val loss: 0.053699247539043427, val acc: 0.9888392857142857\n",
      "Epoch 100, train loss: 0.1632261723279953, train acc: 0.9433681073025335, val loss: 0.05370165780186653, val acc: 0.9888392857142857\n",
      "Epoch 150, train loss: 0.1632261723279953, train acc: 0.9433681073025335, val loss: 0.05370168015360832, val acc: 0.9888392857142857\n",
      "Epoch 200, train loss: 0.16322611272335052, train acc: 0.9433681073025335, val loss: 0.053701672703027725, val acc: 0.9888392857142857\n",
      "Value 's_m_49_val1: 0.9888392857142857' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17966397106647491, train acc: 0.9387610471902618, val loss: 0.05479712784290314, val acc: 0.9885321100917431\n",
      "Epoch 50, train loss: 0.17966395616531372, train acc: 0.9387610471902618, val loss: 0.05542222410440445, val acc: 0.9885321100917431\n",
      "Epoch 100, train loss: 0.17966395616531372, train acc: 0.9387610471902618, val loss: 0.05542479828000069, val acc: 0.9885321100917431\n",
      "Epoch 150, train loss: 0.17966395616531372, train acc: 0.9387610471902618, val loss: 0.05542486906051636, val acc: 0.9885321100917431\n",
      "Epoch 200, train loss: 0.17966395616531372, train acc: 0.9387610471902618, val loss: 0.05542486906051636, val acc: 0.9885321100917431\n",
      "Value 's_m_52_val1: 0.9885321100917431' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09227753430604935, train acc: 0.9808435286542176, val loss: 0.10865631699562073, val acc: 0.9768588770864947\n",
      "Epoch 50, train loss: 0.09227751195430756, train acc: 0.9808435286542176, val loss: 0.10304634273052216, val acc: 0.9768588770864947\n",
      "Epoch 100, train loss: 0.09227751195430756, train acc: 0.9808435286542176, val loss: 0.10302730649709702, val acc: 0.9768588770864947\n",
      "Epoch 150, train loss: 0.09227750450372696, train acc: 0.9808435286542176, val loss: 0.10302725434303284, val acc: 0.9768588770864947\n",
      "Epoch 200, train loss: 0.09227750450372696, train acc: 0.9808435286542176, val loss: 0.10302725434303284, val acc: 0.9768588770864947\n",
      "Value 's_m_1_val2: 0.9768588770864947' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07971949875354767, train acc: 0.9834683954619124, val loss: 0.08474653214216232, val acc: 0.9801829268292683\n",
      "Epoch 50, train loss: 0.07971949875354767, train acc: 0.9834683954619124, val loss: 0.08518868684768677, val acc: 0.9798018292682927\n",
      "Epoch 100, train loss: 0.07971949875354767, train acc: 0.9834683954619124, val loss: 0.08519073575735092, val acc: 0.9798018292682927\n",
      "Epoch 150, train loss: 0.07971949130296707, train acc: 0.9834683954619124, val loss: 0.08519073575735092, val acc: 0.9798018292682927\n",
      "Epoch 200, train loss: 0.07971949130296707, train acc: 0.9834683954619124, val loss: 0.08519073575735092, val acc: 0.9798018292682927\n",
      "Value 's_m_4_val2: 0.9801829268292683' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06901060789823532, train acc: 0.985394908616188, val loss: 0.06932947784662247, val acc: 0.9831546707503829\n",
      "Epoch 50, train loss: 0.06901061534881592, train acc: 0.985394908616188, val loss: 0.06964614242315292, val acc: 0.9831546707503829\n",
      "Epoch 100, train loss: 0.06901061534881592, train acc: 0.985394908616188, val loss: 0.06964756548404694, val acc: 0.9831546707503829\n",
      "Epoch 150, train loss: 0.06901061534881592, train acc: 0.985394908616188, val loss: 0.06964757293462753, val acc: 0.9831546707503829\n",
      "Epoch 200, train loss: 0.06901061534881592, train acc: 0.985394908616188, val loss: 0.06964757293462753, val acc: 0.9831546707503829\n",
      "Value 's_m_7_val2: 0.9831546707503829' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06167847290635109, train acc: 0.9866496878080842, val loss: 0.05757983401417732, val acc: 0.9865384615384616\n",
      "Epoch 50, train loss: 0.06167847290635109, train acc: 0.9866496878080842, val loss: 0.057796724140644073, val acc: 0.9865384615384616\n",
      "Epoch 100, train loss: 0.06167847290635109, train acc: 0.9866496878080842, val loss: 0.057797621935606, val acc: 0.9865384615384616\n",
      "Epoch 150, train loss: 0.06167847290635109, train acc: 0.9866496878080842, val loss: 0.057797621935606, val acc: 0.9865384615384616\n",
      "Epoch 200, train loss: 0.06167847290635109, train acc: 0.9866496878080842, val loss: 0.057797621935606, val acc: 0.9865384615384616\n",
      "Value 's_m_10_val2: 0.9865384615384616' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05732998251914978, train acc: 0.9868050959629384, val loss: 0.0490451380610466, val acc: 0.9884080370942813\n",
      "Epoch 50, train loss: 0.05732998251914978, train acc: 0.9868050959629384, val loss: 0.049253858625888824, val acc: 0.9884080370942813\n",
      "Epoch 100, train loss: 0.05732998251914978, train acc: 0.9868050959629384, val loss: 0.04925479739904404, val acc: 0.9884080370942813\n",
      "Epoch 150, train loss: 0.05732998251914978, train acc: 0.9868050959629384, val loss: 0.04925479739904404, val acc: 0.9884080370942813\n",
      "Epoch 200, train loss: 0.05732998251914978, train acc: 0.9868050959629384, val loss: 0.04925479739904404, val acc: 0.9884080370942813\n",
      "Value 's_m_13_val2: 0.9884080370942813' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05613056197762489, train acc: 0.9856714428523825, val loss: 0.04404782876372337, val acc: 0.9879658385093167\n",
      "Epoch 50, train loss: 0.05613056197762489, train acc: 0.9856714428523825, val loss: 0.04423408955335617, val acc: 0.9879658385093167\n",
      "Epoch 100, train loss: 0.05613056197762489, train acc: 0.9856714428523825, val loss: 0.04423486813902855, val acc: 0.9879658385093167\n",
      "Epoch 150, train loss: 0.05613056197762489, train acc: 0.9856714428523825, val loss: 0.04423486813902855, val acc: 0.9879658385093167\n",
      "Epoch 200, train loss: 0.05613056197762489, train acc: 0.9856714428523825, val loss: 0.04423486813902855, val acc: 0.9879658385093167\n",
      "Value 's_m_16_val2: 0.9879658385093167' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05794956162571907, train acc: 0.9838926174496644, val loss: 0.041775837540626526, val acc: 0.9867394695787831\n",
      "Epoch 50, train loss: 0.05794956162571907, train acc: 0.9838926174496644, val loss: 0.04192282631993294, val acc: 0.9867394695787831\n",
      "Epoch 100, train loss: 0.05794956162571907, train acc: 0.9838926174496644, val loss: 0.041923437267541885, val acc: 0.9867394695787831\n",
      "Epoch 150, train loss: 0.05794956162571907, train acc: 0.9838926174496644, val loss: 0.041923437267541885, val acc: 0.9867394695787831\n",
      "Epoch 200, train loss: 0.05794956162571907, train acc: 0.9838926174496644, val loss: 0.041923437267541885, val acc: 0.9867394695787831\n",
      "Value 's_m_19_val2: 0.9867394695787831' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0625745952129364, train acc: 0.9803565393714092, val loss: 0.040769342333078384, val acc: 0.9855015673981191\n",
      "Epoch 50, train loss: 0.0625745952129364, train acc: 0.9803565393714092, val loss: 0.04087941721081734, val acc: 0.9855015673981191\n",
      "Epoch 100, train loss: 0.0625745952129364, train acc: 0.9803565393714092, val loss: 0.04087981581687927, val acc: 0.9855015673981191\n",
      "Epoch 150, train loss: 0.0625745952129364, train acc: 0.9803565393714092, val loss: 0.04087981581687927, val acc: 0.9855015673981191\n",
      "Epoch 200, train loss: 0.0625745952129364, train acc: 0.9803565393714092, val loss: 0.04087981581687927, val acc: 0.9855015673981191\n",
      "Value 's_m_22_val2: 0.9855015673981191' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06974607706069946, train acc: 0.9765571817562968, val loss: 0.03976696729660034, val acc: 0.984251968503937\n",
      "Epoch 50, train loss: 0.06974607706069946, train acc: 0.9765571817562968, val loss: 0.03981907293200493, val acc: 0.9838582677165354\n",
      "Epoch 100, train loss: 0.06974607706069946, train acc: 0.9765571817562968, val loss: 0.03981924429535866, val acc: 0.9838582677165354\n",
      "Epoch 150, train loss: 0.06974607706069946, train acc: 0.9765571817562968, val loss: 0.03981924429535866, val acc: 0.9838582677165354\n",
      "Epoch 200, train loss: 0.06974607706069946, train acc: 0.9765571817562968, val loss: 0.03981924429535866, val acc: 0.9838582677165354\n",
      "Value 's_m_25_val2: 0.984251968503937' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07944932579994202, train acc: 0.972488858416181, val loss: 0.039435870945453644, val acc: 0.9833860759493671\n",
      "Epoch 50, train loss: 0.07944932579994202, train acc: 0.972488858416181, val loss: 0.03945288434624672, val acc: 0.9837816455696202\n",
      "Epoch 100, train loss: 0.07944932579994202, train acc: 0.972488858416181, val loss: 0.03945286571979523, val acc: 0.9837816455696202\n",
      "Epoch 150, train loss: 0.07944932579994202, train acc: 0.972488858416181, val loss: 0.03945286571979523, val acc: 0.9837816455696202\n",
      "Epoch 200, train loss: 0.07944932579994202, train acc: 0.972488858416181, val loss: 0.03945286571979523, val acc: 0.9837816455696202\n",
      "Value 's_m_28_val2: 0.9837816455696202' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.090584896504879, train acc: 0.9682320441988951, val loss: 0.038987062871456146, val acc: 0.9829093799682035\n",
      "Epoch 50, train loss: 0.090584896504879, train acc: 0.9682320441988951, val loss: 0.03896759822964668, val acc: 0.9841017488076311\n",
      "Epoch 100, train loss: 0.090584896504879, train acc: 0.9682320441988951, val loss: 0.03896743431687355, val acc: 0.9841017488076311\n",
      "Epoch 150, train loss: 0.090584896504879, train acc: 0.9682320441988951, val loss: 0.03896743431687355, val acc: 0.9841017488076311\n",
      "Epoch 200, train loss: 0.090584896504879, train acc: 0.9682320441988951, val loss: 0.03896743431687355, val acc: 0.9841017488076311\n",
      "Value 's_m_31_val2: 0.9841017488076311' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10177701711654663, train acc: 0.964304347826087, val loss: 0.03837224468588829, val acc: 0.9840255591054313\n",
      "Epoch 50, train loss: 0.10177701711654663, train acc: 0.964304347826087, val loss: 0.03832339122891426, val acc: 0.9840255591054313\n",
      "Epoch 100, train loss: 0.10177701711654663, train acc: 0.964304347826087, val loss: 0.03832310810685158, val acc: 0.9840255591054313\n",
      "Epoch 150, train loss: 0.10177701711654663, train acc: 0.964304347826087, val loss: 0.03832310810685158, val acc: 0.9840255591054313\n",
      "Epoch 200, train loss: 0.10177701711654663, train acc: 0.964304347826087, val loss: 0.03832310810685158, val acc: 0.9840255591054313\n",
      "Value 's_m_34_val2: 0.9840255591054313' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11451598256826401, train acc: 0.9598808689558515, val loss: 0.03801807761192322, val acc: 0.9839486356340289\n",
      "Epoch 50, train loss: 0.11451598256826401, train acc: 0.9598808689558515, val loss: 0.03791709616780281, val acc: 0.9839486356340289\n",
      "Epoch 100, train loss: 0.11451598256826401, train acc: 0.9598808689558515, val loss: 0.03791658207774162, val acc: 0.9839486356340289\n",
      "Epoch 150, train loss: 0.11451598256826401, train acc: 0.9598808689558515, val loss: 0.03791658207774162, val acc: 0.9839486356340289\n",
      "Epoch 200, train loss: 0.11451598256826401, train acc: 0.9598808689558515, val loss: 0.03791658207774162, val acc: 0.9839486356340289\n",
      "Value 's_m_37_val2: 0.9839486356340289' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12892088294029236, train acc: 0.9550829509354042, val loss: 0.037982404232025146, val acc: 0.9838709677419355\n",
      "Epoch 50, train loss: 0.12892088294029236, train acc: 0.9550829509354042, val loss: 0.03782294690608978, val acc: 0.9834677419354839\n",
      "Epoch 100, train loss: 0.12892088294029236, train acc: 0.9550829509354042, val loss: 0.03782215714454651, val acc: 0.9834677419354839\n",
      "Epoch 150, train loss: 0.12892088294029236, train acc: 0.9550829509354042, val loss: 0.03782215714454651, val acc: 0.9834677419354839\n",
      "Epoch 200, train loss: 0.12892088294029236, train acc: 0.9550829509354042, val loss: 0.03782215714454651, val acc: 0.9834677419354839\n",
      "Value 's_m_40_val2: 0.9838709677419355' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1431121975183487, train acc: 0.9504800853485064, val loss: 0.04045160114765167, val acc: 0.9817666126418152\n",
      "Epoch 50, train loss: 0.1431121975183487, train acc: 0.9504800853485064, val loss: 0.04024071246385574, val acc: 0.9817666126418152\n",
      "Epoch 100, train loss: 0.1431121975183487, train acc: 0.9504800853485064, val loss: 0.040239688009023666, val acc: 0.9817666126418152\n",
      "Epoch 150, train loss: 0.1431121975183487, train acc: 0.9504800853485064, val loss: 0.040239691734313965, val acc: 0.9817666126418152\n",
      "Epoch 200, train loss: 0.1431121975183487, train acc: 0.9504800853485064, val loss: 0.040239691734313965, val acc: 0.9817666126418152\n",
      "Value 's_m_43_val2: 0.9817666126418152' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15758663415908813, train acc: 0.9462110354711573, val loss: 0.0436960831284523, val acc: 0.9792345276872965\n",
      "Epoch 50, train loss: 0.15758663415908813, train acc: 0.9462110354711573, val loss: 0.04339497163891792, val acc: 0.9792345276872965\n",
      "Epoch 100, train loss: 0.15758663415908813, train acc: 0.9462110354711573, val loss: 0.04339350759983063, val acc: 0.9792345276872965\n",
      "Epoch 150, train loss: 0.15758663415908813, train acc: 0.9462110354711573, val loss: 0.04339350759983063, val acc: 0.9792345276872965\n",
      "Epoch 200, train loss: 0.15758663415908813, train acc: 0.9462110354711573, val loss: 0.04339350759983063, val acc: 0.9792345276872965\n",
      "Value 's_m_46_val2: 0.9792345276872965' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17362666130065918, train acc: 0.9416064981949458, val loss: 0.047780804336071014, val acc: 0.9766775777414075\n",
      "Epoch 50, train loss: 0.17362666130065918, train acc: 0.9416064981949458, val loss: 0.047387901693582535, val acc: 0.9770867430441899\n",
      "Epoch 100, train loss: 0.17362666130065918, train acc: 0.9416064981949458, val loss: 0.04738600552082062, val acc: 0.9770867430441899\n",
      "Epoch 150, train loss: 0.17362666130065918, train acc: 0.9416064981949458, val loss: 0.04738600552082062, val acc: 0.9770867430441899\n",
      "Epoch 200, train loss: 0.17362666130065918, train acc: 0.9416064981949458, val loss: 0.04738600552082062, val acc: 0.9770867430441899\n",
      "Value 's_m_49_val2: 0.9770867430441899' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19099804759025574, train acc: 0.936977082575482, val loss: 0.0533575564622879, val acc: 0.9745065789473685\n",
      "Epoch 50, train loss: 0.19099804759025574, train acc: 0.936977082575482, val loss: 0.05287185311317444, val acc: 0.9745065789473685\n",
      "Epoch 100, train loss: 0.19099804759025574, train acc: 0.936977082575482, val loss: 0.0528695210814476, val acc: 0.9745065789473685\n",
      "Epoch 150, train loss: 0.19099804759025574, train acc: 0.936977082575482, val loss: 0.0528695210814476, val acc: 0.9745065789473685\n",
      "Epoch 200, train loss: 0.19099804759025574, train acc: 0.936977082575482, val loss: 0.0528695210814476, val acc: 0.9745065789473685\n",
      "Value 's_m_52_val2: 0.9745065789473685' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08439218252897263, train acc: 0.9814354512971881, val loss: 0.08238200843334198, val acc: 0.9815668202764977\n",
      "Epoch 50, train loss: 0.08439218252897263, train acc: 0.9814354512971881, val loss: 0.06742378324270248, val acc: 0.9827188940092166\n",
      "Epoch 100, train loss: 0.08439218252897263, train acc: 0.9814354512971881, val loss: 0.06736437976360321, val acc: 0.9827188940092166\n",
      "Epoch 150, train loss: 0.08439218252897263, train acc: 0.9814354512971881, val loss: 0.06736414134502411, val acc: 0.9827188940092166\n",
      "Epoch 200, train loss: 0.08439218252897263, train acc: 0.9814354512971881, val loss: 0.06736414134502411, val acc: 0.9827188940092166\n",
      "Value 's_m_1_val3: 0.9827188940092166' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07236793637275696, train acc: 0.9841673182173573, val loss: 0.053930751979351044, val acc: 0.9878190255220418\n",
      "Epoch 50, train loss: 0.07236793637275696, train acc: 0.9841673182173573, val loss: 0.0545990876853466, val acc: 0.9878190255220418\n",
      "Epoch 100, train loss: 0.07236793637275696, train acc: 0.9841673182173573, val loss: 0.05460217967629433, val acc: 0.9878190255220418\n",
      "Epoch 150, train loss: 0.07236793637275696, train acc: 0.9841673182173573, val loss: 0.054602187126874924, val acc: 0.9878190255220418\n",
      "Epoch 200, train loss: 0.07236793637275696, train acc: 0.9841673182173573, val loss: 0.054602187126874924, val acc: 0.9878190255220418\n",
      "Value 's_m_4_val3: 0.9878190255220418' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06225176528096199, train acc: 0.9865811427671966, val loss: 0.04434218257665634, val acc: 0.991822429906542\n",
      "Epoch 50, train loss: 0.06225176528096199, train acc: 0.9865811427671966, val loss: 0.04480357468128204, val acc: 0.991822429906542\n",
      "Epoch 100, train loss: 0.06225176528096199, train acc: 0.9865811427671966, val loss: 0.04480566456913948, val acc: 0.991822429906542\n",
      "Epoch 150, train loss: 0.06225176528096199, train acc: 0.9865811427671966, val loss: 0.04480566829442978, val acc: 0.991822429906542\n",
      "Epoch 200, train loss: 0.06225176528096199, train acc: 0.9865811427671966, val loss: 0.04480566829442978, val acc: 0.991822429906542\n",
      "Value 's_m_7_val3: 0.991822429906542' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05546778067946434, train acc: 0.9883536681983838, val loss: 0.0387113019824028, val acc: 0.9911764705882353\n",
      "Epoch 50, train loss: 0.05546778067946434, train acc: 0.9883536681983838, val loss: 0.03902960568666458, val acc: 0.9911764705882353\n",
      "Epoch 100, train loss: 0.05546778067946434, train acc: 0.9883536681983838, val loss: 0.03903104364871979, val acc: 0.9911764705882353\n",
      "Epoch 150, train loss: 0.05546778067946434, train acc: 0.9883536681983838, val loss: 0.03903104364871979, val acc: 0.9911764705882353\n",
      "Epoch 200, train loss: 0.05546778067946434, train acc: 0.9883536681983838, val loss: 0.03903104364871979, val acc: 0.9911764705882353\n",
      "Value 's_m_10_val3: 0.9911764705882353' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05174509808421135, train acc: 0.9886744297336099, val loss: 0.0368267185986042, val acc: 0.9893364928909952\n",
      "Epoch 50, train loss: 0.05174509808421135, train acc: 0.9886744297336099, val loss: 0.03704012185335159, val acc: 0.9899289099526066\n",
      "Epoch 100, train loss: 0.05174509808421135, train acc: 0.9886744297336099, val loss: 0.03704112768173218, val acc: 0.9899289099526066\n",
      "Epoch 150, train loss: 0.05174509808421135, train acc: 0.9886744297336099, val loss: 0.03704112768173218, val acc: 0.9899289099526066\n",
      "Epoch 200, train loss: 0.05174509808421135, train acc: 0.9886744297336099, val loss: 0.03704112768173218, val acc: 0.9899289099526066\n",
      "Value 's_m_13_val3: 0.9899289099526066' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.051288213580846786, train acc: 0.987273165248113, val loss: 0.036561235785484314, val acc: 0.9880668257756563\n",
      "Epoch 50, train loss: 0.051288213580846786, train acc: 0.987273165248113, val loss: 0.036637768149375916, val acc: 0.9886634844868735\n",
      "Epoch 100, train loss: 0.051288213580846786, train acc: 0.987273165248113, val loss: 0.03663813695311546, val acc: 0.9886634844868735\n",
      "Epoch 150, train loss: 0.051288213580846786, train acc: 0.987273165248113, val loss: 0.03663813695311546, val acc: 0.9886634844868735\n",
      "Epoch 200, train loss: 0.051288213580846786, train acc: 0.987273165248113, val loss: 0.03663813695311546, val acc: 0.9886634844868735\n",
      "Value 's_m_16_val3: 0.9886634844868735' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0537739172577858, train acc: 0.9845189975747777, val loss: 0.03879356384277344, val acc: 0.9867788461538461\n",
      "Epoch 50, train loss: 0.0537739172577858, train acc: 0.9845189975747777, val loss: 0.03871192783117294, val acc: 0.9867788461538461\n",
      "Epoch 100, train loss: 0.0537739172577858, train acc: 0.9845189975747777, val loss: 0.038711581379175186, val acc: 0.9867788461538461\n",
      "Epoch 150, train loss: 0.0537739172577858, train acc: 0.9845189975747777, val loss: 0.038711581379175186, val acc: 0.9867788461538461\n",
      "Epoch 200, train loss: 0.0537739172577858, train acc: 0.9845189975747777, val loss: 0.038711581379175186, val acc: 0.9867788461538461\n",
      "Value 's_m_19_val3: 0.9867788461538461' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.058654725551605225, train acc: 0.9812388083998047, val loss: 0.04749750718474388, val acc: 0.9812348668280871\n",
      "Epoch 50, train loss: 0.058654725551605225, train acc: 0.9812388083998047, val loss: 0.047252777963876724, val acc: 0.9812348668280871\n",
      "Epoch 100, train loss: 0.058654725551605225, train acc: 0.9812388083998047, val loss: 0.04725167155265808, val acc: 0.9812348668280871\n",
      "Epoch 150, train loss: 0.058654725551605225, train acc: 0.9812388083998047, val loss: 0.04725167155265808, val acc: 0.9812348668280871\n",
      "Epoch 200, train loss: 0.058654725551605225, train acc: 0.9812388083998047, val loss: 0.04725167155265808, val acc: 0.9812348668280871\n",
      "Value 's_m_22_val3: 0.9812348668280871' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06595019996166229, train acc: 0.9776675954761515, val loss: 0.05721841752529144, val acc: 0.975609756097561\n",
      "Epoch 50, train loss: 0.06595019996166229, train acc: 0.9776675954761515, val loss: 0.05679731070995331, val acc: 0.9762195121951219\n",
      "Epoch 100, train loss: 0.06595019996166229, train acc: 0.9776675954761515, val loss: 0.056795358657836914, val acc: 0.9762195121951219\n",
      "Epoch 150, train loss: 0.06595019996166229, train acc: 0.9776675954761515, val loss: 0.056795358657836914, val acc: 0.9762195121951219\n",
      "Epoch 200, train loss: 0.06595019996166229, train acc: 0.9776675954761515, val loss: 0.056795358657836914, val acc: 0.9762195121951219\n",
      "Value 's_m_25_val3: 0.9762195121951219' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07567935436964035, train acc: 0.973923089618749, val loss: 0.06914418190717697, val acc: 0.9705159705159705\n",
      "Epoch 50, train loss: 0.07567935436964035, train acc: 0.973923089618749, val loss: 0.06854423880577087, val acc: 0.9705159705159705\n",
      "Epoch 100, train loss: 0.07567935436964035, train acc: 0.973923089618749, val loss: 0.06854141503572464, val acc: 0.9705159705159705\n",
      "Epoch 150, train loss: 0.07567935436964035, train acc: 0.973923089618749, val loss: 0.06854141503572464, val acc: 0.9705159705159705\n",
      "Epoch 200, train loss: 0.07567935436964035, train acc: 0.973923089618749, val loss: 0.06854141503572464, val acc: 0.9705159705159705\n",
      "Value 's_m_28_val3: 0.9705159705159705' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.086540587246418, train acc: 0.9700016619577863, val loss: 0.08346899598836899, val acc: 0.9647277227722773\n",
      "Epoch 50, train loss: 0.086540587246418, train acc: 0.9700016619577863, val loss: 0.08269047737121582, val acc: 0.9647277227722773\n",
      "Epoch 100, train loss: 0.086540587246418, train acc: 0.9700016619577863, val loss: 0.08268684148788452, val acc: 0.9647277227722773\n",
      "Epoch 150, train loss: 0.086540587246418, train acc: 0.9700016619577863, val loss: 0.08268684148788452, val acc: 0.9647277227722773\n",
      "Epoch 200, train loss: 0.086540587246418, train acc: 0.9700016619577863, val loss: 0.08268684148788452, val acc: 0.9647277227722773\n",
      "Value 's_m_31_val3: 0.9647277227722773' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09701184183359146, train acc: 0.9663598326359832, val loss: 0.09898757934570312, val acc: 0.9588528678304239\n",
      "Epoch 50, train loss: 0.09701184183359146, train acc: 0.9663598326359832, val loss: 0.09803862124681473, val acc: 0.9594763092269327\n",
      "Epoch 100, train loss: 0.09701184183359146, train acc: 0.9663598326359832, val loss: 0.09803418815135956, val acc: 0.9594763092269327\n",
      "Epoch 150, train loss: 0.09701184183359146, train acc: 0.9663598326359832, val loss: 0.09803418815135956, val acc: 0.9594763092269327\n",
      "Epoch 200, train loss: 0.09701184183359146, train acc: 0.9663598326359832, val loss: 0.09803418815135956, val acc: 0.9594763092269327\n",
      "Value 's_m_34_val3: 0.9594763092269327' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10886377841234207, train acc: 0.9622872071464689, val loss: 0.11641449481248856, val acc: 0.9535175879396985\n",
      "Epoch 50, train loss: 0.10886377841234207, train acc: 0.9622872071464689, val loss: 0.11528486758470535, val acc: 0.9541457286432161\n",
      "Epoch 100, train loss: 0.10886377841234207, train acc: 0.9622872071464689, val loss: 0.11527952551841736, val acc: 0.9541457286432161\n",
      "Epoch 150, train loss: 0.10886377841234207, train acc: 0.9622872071464689, val loss: 0.11527952551841736, val acc: 0.9541457286432161\n",
      "Epoch 200, train loss: 0.10886377841234207, train acc: 0.9622872071464689, val loss: 0.11527952551841736, val acc: 0.9541457286432161\n",
      "Value 's_m_37_val3: 0.9541457286432161' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12232273817062378, train acc: 0.9580716346969954, val loss: 0.13490182161331177, val acc: 0.9481012658227848\n",
      "Epoch 50, train loss: 0.12232273817062378, train acc: 0.9580716346969954, val loss: 0.1335720270872116, val acc: 0.9487341772151898\n",
      "Epoch 100, train loss: 0.12232273817062378, train acc: 0.9580716346969954, val loss: 0.1335657238960266, val acc: 0.9487341772151898\n",
      "Epoch 150, train loss: 0.12232273817062378, train acc: 0.9580716346969954, val loss: 0.1335657238960266, val acc: 0.9487341772151898\n",
      "Epoch 200, train loss: 0.12232273817062378, train acc: 0.9580716346969954, val loss: 0.1335657238960266, val acc: 0.9487341772151898\n",
      "Value 's_m_40_val3: 0.9487341772151898' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13536188006401062, train acc: 0.9537100359035733, val loss: 0.15456049144268036, val acc: 0.9426020408163265\n",
      "Epoch 50, train loss: 0.13536188006401062, train acc: 0.9537100359035733, val loss: 0.15303069353103638, val acc: 0.9426020408163265\n",
      "Epoch 100, train loss: 0.13536188006401062, train acc: 0.9537100359035733, val loss: 0.15302343666553497, val acc: 0.9426020408163265\n",
      "Epoch 150, train loss: 0.13536188006401062, train acc: 0.9537100359035733, val loss: 0.15302343666553497, val acc: 0.9426020408163265\n",
      "Epoch 200, train loss: 0.13536188006401062, train acc: 0.9537100359035733, val loss: 0.15302343666553497, val acc: 0.9426020408163265\n",
      "Value 's_m_43_val3: 0.9426020408163265' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14859440922737122, train acc: 0.9492422937833649, val loss: 0.1748059093952179, val acc: 0.9363753213367609\n",
      "Epoch 50, train loss: 0.14859440922737122, train acc: 0.9492422937833649, val loss: 0.17304627597332, val acc: 0.9363753213367609\n",
      "Epoch 100, train loss: 0.14859440922737122, train acc: 0.9492422937833649, val loss: 0.17303796112537384, val acc: 0.9363753213367609\n",
      "Epoch 150, train loss: 0.14859440922737122, train acc: 0.9492422937833649, val loss: 0.17303796112537384, val acc: 0.9363753213367609\n",
      "Epoch 200, train loss: 0.14859440922737122, train acc: 0.9492422937833649, val loss: 0.17303796112537384, val acc: 0.9363753213367609\n",
      "Value 's_m_46_val3: 0.9363753213367609' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1633477658033371, train acc: 0.9449262792714658, val loss: 0.19607876241207123, val acc: 0.9300518134715026\n",
      "Epoch 50, train loss: 0.1633477658033371, train acc: 0.9449262792714658, val loss: 0.1940794289112091, val acc: 0.9300518134715026\n",
      "Epoch 100, train loss: 0.1633477658033371, train acc: 0.9449262792714658, val loss: 0.19406992197036743, val acc: 0.9300518134715026\n",
      "Epoch 150, train loss: 0.1633477658033371, train acc: 0.9449262792714658, val loss: 0.19406992197036743, val acc: 0.9300518134715026\n",
      "Epoch 200, train loss: 0.1633477658033371, train acc: 0.9449262792714658, val loss: 0.19406992197036743, val acc: 0.9300518134715026\n",
      "Value 's_m_49_val3: 0.9300518134715026' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17930036783218384, train acc: 0.9408963830159007, val loss: 0.2186189442873001, val acc: 0.9236292428198434\n",
      "Epoch 50, train loss: 0.17930036783218384, train acc: 0.9408963830159007, val loss: 0.21638186275959015, val acc: 0.9242819843342036\n",
      "Epoch 100, train loss: 0.17930036783218384, train acc: 0.9408963830159007, val loss: 0.21637114882469177, val acc: 0.9242819843342036\n",
      "Epoch 150, train loss: 0.17930036783218384, train acc: 0.9408963830159007, val loss: 0.21637114882469177, val acc: 0.9242819843342036\n",
      "Epoch 200, train loss: 0.17930036783218384, train acc: 0.9408963830159007, val loss: 0.21637114882469177, val acc: 0.9242819843342036\n",
      "Value 's_m_52_val3: 0.9242819843342036' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09251265972852707, train acc: 0.980679521069818, val loss: 0.08566112816333771, val acc: 0.9784090909090909\n",
      "Epoch 50, train loss: 0.09251265972852707, train acc: 0.980679521069818, val loss: 0.0766153410077095, val acc: 0.9801136363636364\n",
      "Epoch 100, train loss: 0.09251265972852707, train acc: 0.980679521069818, val loss: 0.07658365368843079, val acc: 0.9801136363636364\n",
      "Epoch 150, train loss: 0.09251265972852707, train acc: 0.980679521069818, val loss: 0.07658357918262482, val acc: 0.9801136363636364\n",
      "Epoch 200, train loss: 0.09251265972852707, train acc: 0.980679521069818, val loss: 0.07658357918262482, val acc: 0.9801136363636364\n",
      "Value 's_m_1_val4: 0.9801136363636364' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0798366367816925, train acc: 0.9833698544373142, val loss: 0.05784793570637703, val acc: 0.9851258581235698\n",
      "Epoch 50, train loss: 0.0798366367816925, train acc: 0.9833698544373142, val loss: 0.058361757546663284, val acc: 0.9851258581235698\n",
      "Epoch 100, train loss: 0.0798366367816925, train acc: 0.9833698544373142, val loss: 0.058364275842905045, val acc: 0.9851258581235698\n",
      "Epoch 150, train loss: 0.0798366367816925, train acc: 0.9833698544373142, val loss: 0.058364275842905045, val acc: 0.9851258581235698\n",
      "Epoch 200, train loss: 0.0798366367816925, train acc: 0.9833698544373142, val loss: 0.058364275842905045, val acc: 0.9851258581235698\n",
      "Value 's_m_4_val4: 0.9851258581235698' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06902804225683212, train acc: 0.9855443516622027, val loss: 0.04248914122581482, val acc: 0.9902073732718893\n",
      "Epoch 50, train loss: 0.06902804225683212, train acc: 0.9855443516622027, val loss: 0.04286518692970276, val acc: 0.9902073732718893\n",
      "Epoch 100, train loss: 0.06902804225683212, train acc: 0.9855443516622027, val loss: 0.04286700114607811, val acc: 0.9902073732718893\n",
      "Epoch 150, train loss: 0.06902804225683212, train acc: 0.9855443516622027, val loss: 0.04286700114607811, val acc: 0.9902073732718893\n",
      "Epoch 200, train loss: 0.06902804225683212, train acc: 0.9855443516622027, val loss: 0.04286700114607811, val acc: 0.9902073732718893\n",
      "Value 's_m_7_val4: 0.9902073732718893' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06142769753932953, train acc: 0.9868754956383823, val loss: 0.03375406935811043, val acc: 0.9953596287703016\n",
      "Epoch 50, train loss: 0.06142769753932953, train acc: 0.9868754956383823, val loss: 0.0340406708419323, val acc: 0.9953596287703016\n",
      "Epoch 100, train loss: 0.06142769753932953, train acc: 0.9868754956383823, val loss: 0.03404202684760094, val acc: 0.9953596287703016\n",
      "Epoch 150, train loss: 0.06142769753932953, train acc: 0.9868754956383823, val loss: 0.03404202684760094, val acc: 0.9953596287703016\n",
      "Epoch 200, train loss: 0.06142769753932953, train acc: 0.9868754956383823, val loss: 0.03404202684760094, val acc: 0.9953596287703016\n",
      "Value 's_m_10_val4: 0.9953596287703016' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05688730999827385, train acc: 0.9869072329554527, val loss: 0.028012726455926895, val acc: 0.9970794392523364\n",
      "Epoch 50, train loss: 0.05688730999827385, train acc: 0.9869072329554527, val loss: 0.02821549028158188, val acc: 0.9970794392523364\n",
      "Epoch 100, train loss: 0.05688730999827385, train acc: 0.9869072329554527, val loss: 0.02821645699441433, val acc: 0.9970794392523364\n",
      "Epoch 150, train loss: 0.05688730999827385, train acc: 0.9869072329554527, val loss: 0.02821645699441433, val acc: 0.9970794392523364\n",
      "Epoch 200, train loss: 0.05688730999827385, train acc: 0.9869072329554527, val loss: 0.02821645699441433, val acc: 0.9970794392523364\n",
      "Value 's_m_13_val4: 0.9970794392523364' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05531013011932373, train acc: 0.9858945507153191, val loss: 0.028403664007782936, val acc: 0.9941176470588236\n",
      "Epoch 50, train loss: 0.05531013011932373, train acc: 0.9858945507153191, val loss: 0.02851198799908161, val acc: 0.9947058823529412\n",
      "Epoch 100, train loss: 0.05531013011932373, train acc: 0.9858945507153191, val loss: 0.028512470424175262, val acc: 0.9947058823529412\n",
      "Epoch 150, train loss: 0.05531013011932373, train acc: 0.9858945507153191, val loss: 0.028512470424175262, val acc: 0.9947058823529412\n",
      "Epoch 200, train loss: 0.05531013011932373, train acc: 0.9858945507153191, val loss: 0.028512470424175262, val acc: 0.9947058823529412\n",
      "Value 's_m_16_val4: 0.9947058823529412' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.056617699563503265, train acc: 0.9837756918595242, val loss: 0.03264295682311058, val acc: 0.9893364928909952\n",
      "Epoch 50, train loss: 0.056617699563503265, train acc: 0.9837756918595242, val loss: 0.03265852853655815, val acc: 0.9893364928909952\n",
      "Epoch 100, train loss: 0.056617699563503265, train acc: 0.9837756918595242, val loss: 0.032658547163009644, val acc: 0.9893364928909952\n",
      "Epoch 150, train loss: 0.056617699563503265, train acc: 0.9837756918595242, val loss: 0.032658547163009644, val acc: 0.9893364928909952\n",
      "Epoch 200, train loss: 0.056617699563503265, train acc: 0.9837756918595242, val loss: 0.032658547163009644, val acc: 0.9893364928909952\n",
      "Value 's_m_19_val4: 0.9893364928909952' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06046716123819351, train acc: 0.9806094182825484, val loss: 0.040826182812452316, val acc: 0.983890214797136\n",
      "Epoch 50, train loss: 0.06046716123819351, train acc: 0.9806094182825484, val loss: 0.04071984440088272, val acc: 0.983890214797136\n",
      "Epoch 100, train loss: 0.06046716123819351, train acc: 0.9806094182825484, val loss: 0.040719252079725266, val acc: 0.983890214797136\n",
      "Epoch 150, train loss: 0.06046716123819351, train acc: 0.9806094182825484, val loss: 0.040719252079725266, val acc: 0.983890214797136\n",
      "Epoch 200, train loss: 0.06046716123819351, train acc: 0.9806094182825484, val loss: 0.040719252079725266, val acc: 0.983890214797136\n",
      "Value 's_m_22_val4: 0.983890214797136' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06640706956386566, train acc: 0.977194421657096, val loss: 0.05419136583805084, val acc: 0.9783653846153846\n",
      "Epoch 50, train loss: 0.06640706956386566, train acc: 0.977194421657096, val loss: 0.053953878581523895, val acc: 0.9783653846153846\n",
      "Epoch 100, train loss: 0.06640706956386566, train acc: 0.977194421657096, val loss: 0.053952641785144806, val acc: 0.9783653846153846\n",
      "Epoch 150, train loss: 0.06640706956386566, train acc: 0.977194421657096, val loss: 0.053952641785144806, val acc: 0.9783653846153846\n",
      "Epoch 200, train loss: 0.06640706956386566, train acc: 0.977194421657096, val loss: 0.053952641785144806, val acc: 0.9783653846153846\n",
      "Value 's_m_25_val4: 0.9783653846153846' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07459580898284912, train acc: 0.9736081282008922, val loss: 0.07163690775632858, val acc: 0.9727602905569007\n",
      "Epoch 50, train loss: 0.07459580898284912, train acc: 0.9736081282008922, val loss: 0.07126409560441971, val acc: 0.9727602905569007\n",
      "Epoch 100, train loss: 0.07459580898284912, train acc: 0.9736081282008922, val loss: 0.0712621808052063, val acc: 0.9727602905569007\n",
      "Epoch 150, train loss: 0.07459580898284912, train acc: 0.9736081282008922, val loss: 0.0712621808052063, val acc: 0.9727602905569007\n",
      "Epoch 200, train loss: 0.07459580898284912, train acc: 0.9736081282008922, val loss: 0.0712621808052063, val acc: 0.9727602905569007\n",
      "Value 's_m_28_val4: 0.9727602905569007' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08445920050144196, train acc: 0.9696805855930793, val loss: 0.08489559590816498, val acc: 0.9682926829268292\n",
      "Epoch 50, train loss: 0.08445920050144196, train acc: 0.9696805855930793, val loss: 0.08437767624855042, val acc: 0.9682926829268292\n",
      "Epoch 100, train loss: 0.08445920050144196, train acc: 0.9696805855930793, val loss: 0.08437503129243851, val acc: 0.9682926829268292\n",
      "Epoch 150, train loss: 0.08445920050144196, train acc: 0.9696805855930793, val loss: 0.08437503129243851, val acc: 0.9682926829268292\n",
      "Epoch 200, train loss: 0.08445920050144196, train acc: 0.9696805855930793, val loss: 0.08437503129243851, val acc: 0.9682926829268292\n",
      "Value 's_m_31_val4: 0.9682926829268292' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09428195655345917, train acc: 0.9659071871335232, val loss: 0.09855503588914871, val acc: 0.9643734643734644\n",
      "Epoch 50, train loss: 0.09428195655345917, train acc: 0.9659071871335232, val loss: 0.09792668372392654, val acc: 0.964987714987715\n",
      "Epoch 100, train loss: 0.09428195655345917, train acc: 0.9659071871335232, val loss: 0.09792350232601166, val acc: 0.964987714987715\n",
      "Epoch 150, train loss: 0.09428195655345917, train acc: 0.9659071871335232, val loss: 0.09792350232601166, val acc: 0.964987714987715\n",
      "Epoch 200, train loss: 0.09428195655345917, train acc: 0.9659071871335232, val loss: 0.09792350232601166, val acc: 0.964987714987715\n",
      "Value 's_m_34_val4: 0.964987714987715' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10551265627145767, train acc: 0.9619115910241268, val loss: 0.11379692703485489, val acc: 0.9610148514851485\n",
      "Epoch 50, train loss: 0.10551265627145767, train acc: 0.9619115910241268, val loss: 0.11311029642820358, val acc: 0.9610148514851485\n",
      "Epoch 100, train loss: 0.10551265627145767, train acc: 0.9619115910241268, val loss: 0.11310684680938721, val acc: 0.9610148514851485\n",
      "Epoch 150, train loss: 0.10551265627145767, train acc: 0.9619115910241268, val loss: 0.11310683935880661, val acc: 0.9610148514851485\n",
      "Epoch 200, train loss: 0.10551265627145767, train acc: 0.9619115910241268, val loss: 0.11310683935880661, val acc: 0.9610148514851485\n",
      "Value 's_m_37_val4: 0.9610148514851485' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11833039671182632, train acc: 0.9576040781648258, val loss: 0.13029465079307556, val acc: 0.9569825436408977\n",
      "Epoch 50, train loss: 0.11833039671182632, train acc: 0.9576040781648258, val loss: 0.12951700389385223, val acc: 0.9569825436408977\n",
      "Epoch 100, train loss: 0.11833039671182632, train acc: 0.9576040781648258, val loss: 0.12951314449310303, val acc: 0.9569825436408977\n",
      "Epoch 150, train loss: 0.11833039671182632, train acc: 0.9576040781648258, val loss: 0.12951314449310303, val acc: 0.9569825436408977\n",
      "Epoch 200, train loss: 0.11833039671182632, train acc: 0.9576040781648258, val loss: 0.12951314449310303, val acc: 0.9569825436408977\n",
      "Value 's_m_40_val4: 0.9569825436408977' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13104601204395294, train acc: 0.9534057846996405, val loss: 0.1479441225528717, val acc: 0.9528894472361809\n",
      "Epoch 50, train loss: 0.13104601204395294, train acc: 0.9534057846996405, val loss: 0.1470509171485901, val acc: 0.9528894472361809\n",
      "Epoch 100, train loss: 0.13104601204395294, train acc: 0.9534057846996405, val loss: 0.14704647660255432, val acc: 0.9528894472361809\n",
      "Epoch 150, train loss: 0.13104601204395294, train acc: 0.9534057846996405, val loss: 0.14704647660255432, val acc: 0.9528894472361809\n",
      "Epoch 200, train loss: 0.13104601204395294, train acc: 0.9534057846996405, val loss: 0.14704647660255432, val acc: 0.9528894472361809\n",
      "Value 's_m_43_val4: 0.9528894472361809' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14401903748512268, train acc: 0.9492759868988105, val loss: 0.16676875948905945, val acc: 0.9487341772151898\n",
      "Epoch 50, train loss: 0.14401903748512268, train acc: 0.9492759868988105, val loss: 0.16581368446350098, val acc: 0.9487341772151898\n",
      "Epoch 100, train loss: 0.14401903748512268, train acc: 0.9492759868988105, val loss: 0.16580896079540253, val acc: 0.9487341772151898\n",
      "Epoch 150, train loss: 0.14401903748512268, train acc: 0.9492759868988105, val loss: 0.16580896079540253, val acc: 0.9487341772151898\n",
      "Epoch 200, train loss: 0.14401903748512268, train acc: 0.9492759868988105, val loss: 0.16580896079540253, val acc: 0.9487341772151898\n",
      "Value 's_m_46_val4: 0.9487341772151898' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15851709246635437, train acc: 0.9448689008508422, val loss: 0.1870899647474289, val acc: 0.9445153061224489\n",
      "Epoch 50, train loss: 0.15851709246635437, train acc: 0.9448689008508422, val loss: 0.1860640048980713, val acc: 0.9445153061224489\n",
      "Epoch 100, train loss: 0.15851709246635437, train acc: 0.9448689008508422, val loss: 0.1860589236021042, val acc: 0.9445153061224489\n",
      "Epoch 150, train loss: 0.15851709246635437, train acc: 0.9448689008508422, val loss: 0.1860589236021042, val acc: 0.9445153061224489\n",
      "Epoch 200, train loss: 0.15851709246635437, train acc: 0.9448689008508422, val loss: 0.1860589236021042, val acc: 0.9445153061224489\n",
      "Value 's_m_49_val4: 0.9445153061224489' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17434494197368622, train acc: 0.9403096029386042, val loss: 0.20890098810195923, val acc: 0.9402313624678663\n",
      "Epoch 50, train loss: 0.17434494197368622, train acc: 0.9403096029386042, val loss: 0.20781303942203522, val acc: 0.9402313624678663\n",
      "Epoch 100, train loss: 0.17434494197368622, train acc: 0.9403096029386042, val loss: 0.20780768990516663, val acc: 0.9402313624678663\n",
      "Epoch 150, train loss: 0.17434494197368622, train acc: 0.9403096029386042, val loss: 0.20780768990516663, val acc: 0.9402313624678663\n",
      "Epoch 200, train loss: 0.17434494197368622, train acc: 0.9403096029386042, val loss: 0.20780768990516663, val acc: 0.9402313624678663\n",
      "Value 's_m_52_val4: 0.9402313624678663' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10021279752254486, train acc: 0.9796193497224425, val loss: 0.1094076856970787, val acc: 0.9765901060070671\n",
      "Epoch 50, train loss: 0.10021279752254486, train acc: 0.9796193497224425, val loss: 0.09768453240394592, val acc: 0.9796819787985865\n",
      "Epoch 100, train loss: 0.10021279752254486, train acc: 0.9796193497224425, val loss: 0.09764012694358826, val acc: 0.9796819787985865\n",
      "Epoch 150, train loss: 0.10021279752254486, train acc: 0.9796193497224425, val loss: 0.09763998538255692, val acc: 0.9796819787985865\n",
      "Epoch 200, train loss: 0.10021279752254486, train acc: 0.9796193497224425, val loss: 0.09763998538255692, val acc: 0.9796819787985865\n",
      "Value 's_m_1_val5: 0.9796819787985865' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08674702048301697, train acc: 0.9824365320134121, val loss: 0.07783380150794983, val acc: 0.9849023090586145\n",
      "Epoch 50, train loss: 0.08674702048301697, train acc: 0.9824365320134121, val loss: 0.07861429452896118, val acc: 0.9849023090586145\n",
      "Epoch 100, train loss: 0.08674702048301697, train acc: 0.9824365320134121, val loss: 0.07861816883087158, val acc: 0.9849023090586145\n",
      "Epoch 150, train loss: 0.08674702048301697, train acc: 0.9824365320134121, val loss: 0.07861816883087158, val acc: 0.9849023090586145\n",
      "Epoch 200, train loss: 0.08674702048301697, train acc: 0.9824365320134121, val loss: 0.07861816883087158, val acc: 0.9849023090586145\n",
      "Value 's_m_4_val5: 0.9849023090586145' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07515081763267517, train acc: 0.9846889567593634, val loss: 0.061129264533519745, val acc: 0.9901785714285715\n",
      "Epoch 50, train loss: 0.07515081763267517, train acc: 0.9846889567593634, val loss: 0.06173655763268471, val acc: 0.9901785714285715\n",
      "Epoch 100, train loss: 0.07515081763267517, train acc: 0.9846889567593634, val loss: 0.061739541590213776, val acc: 0.9901785714285715\n",
      "Epoch 150, train loss: 0.07515081763267517, train acc: 0.9846889567593634, val loss: 0.061739541590213776, val acc: 0.9901785714285715\n",
      "Epoch 200, train loss: 0.07515081763267517, train acc: 0.9846889567593634, val loss: 0.061739541590213776, val acc: 0.9901785714285715\n",
      "Value 's_m_7_val5: 0.9901785714285715' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06700449436903, train acc: 0.9860414306522091, val loss: 0.047872647643089294, val acc: 0.994165170556553\n",
      "Epoch 50, train loss: 0.06700449436903, train acc: 0.9860414306522091, val loss: 0.04834311828017235, val acc: 0.994165170556553\n",
      "Epoch 100, train loss: 0.06700449436903, train acc: 0.9860414306522091, val loss: 0.04834539070725441, val acc: 0.994165170556553\n",
      "Epoch 150, train loss: 0.06700449436903, train acc: 0.9860414306522091, val loss: 0.04834539070725441, val acc: 0.994165170556553\n",
      "Epoch 200, train loss: 0.06700449436903, train acc: 0.9860414306522091, val loss: 0.04834539070725441, val acc: 0.994165170556553\n",
      "Value 's_m_10_val5: 0.994165170556553' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.061790935695171356, train acc: 0.9864754766172397, val loss: 0.03853665664792061, val acc: 0.9972924187725631\n",
      "Epoch 50, train loss: 0.061790935695171356, train acc: 0.9864754766172397, val loss: 0.03891834244132042, val acc: 0.9972924187725631\n",
      "Epoch 100, train loss: 0.061790935695171356, train acc: 0.9864754766172397, val loss: 0.03892020881175995, val acc: 0.9972924187725631\n",
      "Epoch 150, train loss: 0.061790935695171356, train acc: 0.9864754766172397, val loss: 0.03892020881175995, val acc: 0.9972924187725631\n",
      "Epoch 200, train loss: 0.061790935695171356, train acc: 0.9864754766172397, val loss: 0.03892020881175995, val acc: 0.9972924187725631\n",
      "Value 's_m_13_val5: 0.9972924187725631' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.059684306383132935, train acc: 0.9855619360131255, val loss: 0.033423490822315216, val acc: 0.9968239564428312\n",
      "Epoch 50, train loss: 0.059684306383132935, train acc: 0.9855619360131255, val loss: 0.03372238948941231, val acc: 0.9963702359346642\n",
      "Epoch 100, train loss: 0.059684306383132935, train acc: 0.9855619360131255, val loss: 0.03372382000088692, val acc: 0.9963702359346642\n",
      "Epoch 150, train loss: 0.059684306383132935, train acc: 0.9855619360131255, val loss: 0.03372382000088692, val acc: 0.9963702359346642\n",
      "Epoch 200, train loss: 0.059684306383132935, train acc: 0.9855619360131255, val loss: 0.03372382000088692, val acc: 0.9963702359346642\n",
      "Value 's_m_16_val5: 0.9968239564428312' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.060386184602975845, train acc: 0.9837683793160417, val loss: 0.033303264528512955, val acc: 0.9931569343065694\n",
      "Epoch 50, train loss: 0.060386184602975845, train acc: 0.9837683793160417, val loss: 0.0335114449262619, val acc: 0.9931569343065694\n",
      "Epoch 100, train loss: 0.060386184602975845, train acc: 0.9837683793160417, val loss: 0.03351239860057831, val acc: 0.9931569343065694\n",
      "Epoch 150, train loss: 0.060386184602975845, train acc: 0.9837683793160417, val loss: 0.03351239860057831, val acc: 0.9931569343065694\n",
      "Epoch 200, train loss: 0.060386184602975845, train acc: 0.9837683793160417, val loss: 0.03351239860057831, val acc: 0.9931569343065694\n",
      "Value 's_m_19_val5: 0.9931569343065694' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06383859366178513, train acc: 0.9808268175012477, val loss: 0.034893061965703964, val acc: 0.9899082568807339\n",
      "Epoch 50, train loss: 0.06383859366178513, train acc: 0.9808268175012477, val loss: 0.035013943910598755, val acc: 0.9899082568807339\n",
      "Epoch 100, train loss: 0.06383859366178513, train acc: 0.9808268175012477, val loss: 0.03501446172595024, val acc: 0.9899082568807339\n",
      "Epoch 150, train loss: 0.06383859366178513, train acc: 0.9808268175012477, val loss: 0.03501446172595024, val acc: 0.9899082568807339\n",
      "Epoch 200, train loss: 0.06383859366178513, train acc: 0.9808268175012477, val loss: 0.03501446172595024, val acc: 0.9899082568807339\n",
      "Value 's_m_22_val5: 0.9899082568807339' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06973008066415787, train acc: 0.9773412631931647, val loss: 0.03715372085571289, val acc: 0.9857011070110702\n",
      "Epoch 50, train loss: 0.06973008066415787, train acc: 0.9773412631931647, val loss: 0.037177618592977524, val acc: 0.9857011070110702\n",
      "Epoch 100, train loss: 0.06973008066415787, train acc: 0.9773412631931647, val loss: 0.037177674472332, val acc: 0.9857011070110702\n",
      "Epoch 150, train loss: 0.06973008066415787, train acc: 0.9773412631931647, val loss: 0.037177674472332, val acc: 0.9857011070110702\n",
      "Epoch 200, train loss: 0.06973008066415787, train acc: 0.9773412631931647, val loss: 0.037177674472332, val acc: 0.9857011070110702\n",
      "Value 's_m_25_val5: 0.9857011070110702' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07767606526613235, train acc: 0.973764130251392, val loss: 0.0447457991540432, val acc: 0.9814471243042672\n",
      "Epoch 50, train loss: 0.07767606526613235, train acc: 0.973764130251392, val loss: 0.044687915593385696, val acc: 0.9814471243042672\n",
      "Epoch 100, train loss: 0.07767606526613235, train acc: 0.973764130251392, val loss: 0.04468758404254913, val acc: 0.9814471243042672\n",
      "Epoch 150, train loss: 0.07767606526613235, train acc: 0.973764130251392, val loss: 0.04468758404254913, val acc: 0.9814471243042672\n",
      "Epoch 200, train loss: 0.07767606526613235, train acc: 0.973764130251392, val loss: 0.04468758404254913, val acc: 0.9814471243042672\n",
      "Value 's_m_28_val5: 0.9814471243042672' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0867740735411644, train acc: 0.9698810535259134, val loss: 0.05550601705908775, val acc: 0.9771455223880597\n",
      "Epoch 50, train loss: 0.0867740735411644, train acc: 0.9698810535259134, val loss: 0.05536342412233353, val acc: 0.9780783582089553\n",
      "Epoch 100, train loss: 0.0867740735411644, train acc: 0.9698810535259134, val loss: 0.05536267161369324, val acc: 0.9780783582089553\n",
      "Epoch 150, train loss: 0.0867740735411644, train acc: 0.9698810535259134, val loss: 0.05536267161369324, val acc: 0.9780783582089553\n",
      "Epoch 200, train loss: 0.0867740735411644, train acc: 0.9698810535259134, val loss: 0.05536267161369324, val acc: 0.9780783582089553\n",
      "Value 's_m_31_val5: 0.9780783582089553' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09668506681919098, train acc: 0.966113297963375, val loss: 0.05793449655175209, val acc: 0.975140712945591\n",
      "Epoch 50, train loss: 0.09668506681919098, train acc: 0.966113297963375, val loss: 0.057735852897167206, val acc: 0.975140712945591\n",
      "Epoch 100, train loss: 0.09668506681919098, train acc: 0.966113297963375, val loss: 0.057734835892915726, val acc: 0.975140712945591\n",
      "Epoch 150, train loss: 0.09668506681919098, train acc: 0.966113297963375, val loss: 0.057734835892915726, val acc: 0.975140712945591\n",
      "Epoch 200, train loss: 0.09668506681919098, train acc: 0.966113297963375, val loss: 0.057734835892915726, val acc: 0.975140712945591\n",
      "Value 's_m_34_val5: 0.975140712945591' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10800085216760635, train acc: 0.9623340803309774, val loss: 0.062365807592868805, val acc: 0.9721698113207548\n",
      "Epoch 50, train loss: 0.10800085216760635, train acc: 0.9623340803309774, val loss: 0.06209762021899223, val acc: 0.9726415094339622\n",
      "Epoch 100, train loss: 0.10800085216760635, train acc: 0.9623340803309774, val loss: 0.06209626793861389, val acc: 0.9726415094339622\n",
      "Epoch 150, train loss: 0.10800085216760635, train acc: 0.9623340803309774, val loss: 0.06209626793861389, val acc: 0.9726415094339622\n",
      "Epoch 200, train loss: 0.10800085216760635, train acc: 0.9623340803309774, val loss: 0.06209626793861389, val acc: 0.9726415094339622\n",
      "Value 's_m_37_val5: 0.9726415094339622' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12097431719303131, train acc: 0.9579788157666261, val loss: 0.06724943220615387, val acc: 0.969639468690702\n",
      "Epoch 50, train loss: 0.12097431719303131, train acc: 0.9579788157666261, val loss: 0.06690526008605957, val acc: 0.9705882352941176\n",
      "Epoch 100, train loss: 0.12097431719303131, train acc: 0.9579788157666261, val loss: 0.06690356135368347, val acc: 0.9705882352941176\n",
      "Epoch 150, train loss: 0.12097431719303131, train acc: 0.9579788157666261, val loss: 0.06690356135368347, val acc: 0.9705882352941176\n",
      "Epoch 200, train loss: 0.12097431719303131, train acc: 0.9579788157666261, val loss: 0.06690356135368347, val acc: 0.9705882352941176\n",
      "Value 's_m_40_val5: 0.9705882352941176' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13394685089588165, train acc: 0.9536470176666083, val loss: 0.07217152416706085, val acc: 0.9675572519083969\n",
      "Epoch 50, train loss: 0.13394685089588165, train acc: 0.9536470176666083, val loss: 0.0717506855726242, val acc: 0.9680343511450382\n",
      "Epoch 100, train loss: 0.13394685089588165, train acc: 0.9536470176666083, val loss: 0.07174860686063766, val acc: 0.9680343511450382\n",
      "Epoch 150, train loss: 0.13394685089588165, train acc: 0.9536470176666083, val loss: 0.07174860686063766, val acc: 0.9680343511450382\n",
      "Epoch 200, train loss: 0.13394685089588165, train acc: 0.9536470176666083, val loss: 0.07174860686063766, val acc: 0.9680343511450382\n",
      "Value 's_m_43_val5: 0.9680343511450382' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14722348749637604, train acc: 0.9497797356828194, val loss: 0.07774126529693604, val acc: 0.9649712092130518\n",
      "Epoch 50, train loss: 0.14722348749637604, train acc: 0.9497797356828194, val loss: 0.0772344172000885, val acc: 0.9654510556621881\n",
      "Epoch 100, train loss: 0.14722348749637604, train acc: 0.9497797356828194, val loss: 0.07723195850849152, val acc: 0.9654510556621881\n",
      "Epoch 150, train loss: 0.14722348749637604, train acc: 0.9497797356828194, val loss: 0.07723195850849152, val acc: 0.9654510556621881\n",
      "Epoch 200, train loss: 0.14722348749637604, train acc: 0.9497797356828194, val loss: 0.07723195850849152, val acc: 0.9654510556621881\n",
      "Value 's_m_46_val5: 0.9654510556621881' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16207902133464813, train acc: 0.945499733712054, val loss: 0.08453724533319473, val acc: 0.9623552123552124\n",
      "Epoch 50, train loss: 0.16207902133464813, train acc: 0.945499733712054, val loss: 0.08394137024879456, val acc: 0.9623552123552124\n",
      "Epoch 100, train loss: 0.16207902133464813, train acc: 0.945499733712054, val loss: 0.08393847942352295, val acc: 0.9623552123552124\n",
      "Epoch 150, train loss: 0.16207902133464813, train acc: 0.945499733712054, val loss: 0.08393847942352295, val acc: 0.9623552123552124\n",
      "Epoch 200, train loss: 0.16207902133464813, train acc: 0.945499733712054, val loss: 0.08393847942352295, val acc: 0.9623552123552124\n",
      "Value 's_m_49_val5: 0.9623552123552124' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17843031883239746, train acc: 0.9409318547665891, val loss: 0.09201344847679138, val acc: 0.9592233009708738\n",
      "Epoch 50, train loss: 0.17843031883239746, train acc: 0.9409318547665891, val loss: 0.09132508188486099, val acc: 0.9597087378640776\n",
      "Epoch 100, train loss: 0.17843031883239746, train acc: 0.9409318547665891, val loss: 0.09132175147533417, val acc: 0.9597087378640776\n",
      "Epoch 150, train loss: 0.17843031883239746, train acc: 0.9409318547665891, val loss: 0.09132175147533417, val acc: 0.9597087378640776\n",
      "Epoch 200, train loss: 0.17843031883239746, train acc: 0.9409318547665891, val loss: 0.09132175147533417, val acc: 0.9597087378640776\n",
      "Value 's_m_52_val5: 0.9597087378640776' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08961635082960129, train acc: 0.9798684632659609, val loss: 0.057243309915065765, val acc: 0.9917582417582418\n",
      "Epoch 50, train loss: 0.08961635082960129, train acc: 0.9798684632659609, val loss: 0.04660036414861679, val acc: 0.9913657770800628\n",
      "Epoch 100, train loss: 0.08961635082960129, train acc: 0.9798684632659609, val loss: 0.04656846821308136, val acc: 0.9913657770800628\n",
      "Epoch 150, train loss: 0.08961635082960129, train acc: 0.9798684632659609, val loss: 0.04656831920146942, val acc: 0.9913657770800628\n",
      "Epoch 200, train loss: 0.08961635082960129, train acc: 0.9798684632659609, val loss: 0.04656831920146942, val acc: 0.9913657770800628\n",
      "Value 's_m_1_val6: 0.9921507064364207' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07662054151296616, train acc: 0.9830830103359173, val loss: 0.04057585448026657, val acc: 0.9925078864353313\n",
      "Epoch 50, train loss: 0.07662054151296616, train acc: 0.9830830103359173, val loss: 0.04133548587560654, val acc: 0.9925078864353313\n",
      "Epoch 100, train loss: 0.07662054151296616, train acc: 0.9830830103359173, val loss: 0.04133917763829231, val acc: 0.9925078864353313\n",
      "Epoch 150, train loss: 0.07662054151296616, train acc: 0.9830830103359173, val loss: 0.04133917763829231, val acc: 0.9925078864353313\n",
      "Epoch 200, train loss: 0.07662054151296616, train acc: 0.9830830103359173, val loss: 0.04133917763829231, val acc: 0.9925078864353313\n",
      "Value 's_m_4_val6: 0.9925078864353313' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06570962816476822, train acc: 0.9858536585365854, val loss: 0.036819666624069214, val acc: 0.993660855784469\n",
      "Epoch 50, train loss: 0.06570962816476822, train acc: 0.9858536585365854, val loss: 0.037448856979608536, val acc: 0.993660855784469\n",
      "Epoch 100, train loss: 0.06570962816476822, train acc: 0.9858536585365854, val loss: 0.0374518558382988, val acc: 0.993660855784469\n",
      "Epoch 150, train loss: 0.06570962816476822, train acc: 0.9858536585365854, val loss: 0.0374518521130085, val acc: 0.993660855784469\n",
      "Epoch 200, train loss: 0.06570962816476822, train acc: 0.9858536585365854, val loss: 0.0374518521130085, val acc: 0.993660855784469\n",
      "Value 's_m_7_val6: 0.993660855784469' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05837886780500412, train acc: 0.9878847413228553, val loss: 0.034677423536777496, val acc: 0.9948248407643312\n",
      "Epoch 50, train loss: 0.05837886780500412, train acc: 0.9878847413228553, val loss: 0.03519892320036888, val acc: 0.9948248407643312\n",
      "Epoch 100, train loss: 0.05837886780500412, train acc: 0.9878847413228553, val loss: 0.03520142659544945, val acc: 0.9948248407643312\n",
      "Epoch 150, train loss: 0.05837886780500412, train acc: 0.9878847413228553, val loss: 0.03520142659544945, val acc: 0.9948248407643312\n",
      "Epoch 200, train loss: 0.05837886780500412, train acc: 0.9878847413228553, val loss: 0.03520142659544945, val acc: 0.9948248407643312\n",
      "Value 's_m_10_val6: 0.9948248407643312' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.054306890815496445, train acc: 0.9889136168809759, val loss: 0.034950848668813705, val acc: 0.9936\n",
      "Epoch 50, train loss: 0.054306890815496445, train acc: 0.9889136168809759, val loss: 0.03536999598145485, val acc: 0.994\n",
      "Epoch 100, train loss: 0.054306890815496445, train acc: 0.9889136168809759, val loss: 0.03537202626466751, val acc: 0.994\n",
      "Epoch 150, train loss: 0.054306890815496445, train acc: 0.9889136168809759, val loss: 0.03537202626466751, val acc: 0.994\n",
      "Epoch 200, train loss: 0.054306890815496445, train acc: 0.9889136168809759, val loss: 0.03537202626466751, val acc: 0.994\n",
      "Value 's_m_13_val6: 0.994' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05339157581329346, train acc: 0.9875498007968128, val loss: 0.03822147101163864, val acc: 0.9903536977491961\n",
      "Epoch 50, train loss: 0.05339157581329346, train acc: 0.9875498007968128, val loss: 0.03854390233755112, val acc: 0.9903536977491961\n",
      "Epoch 100, train loss: 0.05339157581329346, train acc: 0.9875498007968128, val loss: 0.038545478135347366, val acc: 0.9903536977491961\n",
      "Epoch 150, train loss: 0.05339157581329346, train acc: 0.9875498007968128, val loss: 0.038545478135347366, val acc: 0.9903536977491961\n",
      "Epoch 200, train loss: 0.05339157581329346, train acc: 0.9875498007968128, val loss: 0.038545478135347366, val acc: 0.9903536977491961\n",
      "Value 's_m_16_val6: 0.9903536977491961' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.055514074862003326, train acc: 0.9852056168505516, val loss: 0.042682960629463196, val acc: 0.9866720516962844\n",
      "Epoch 50, train loss: 0.055514074862003326, train acc: 0.9852056168505516, val loss: 0.04290264472365379, val acc: 0.9866720516962844\n",
      "Epoch 100, train loss: 0.055514074862003326, train acc: 0.9852056168505516, val loss: 0.04290374368429184, val acc: 0.9866720516962844\n",
      "Epoch 150, train loss: 0.055514074862003326, train acc: 0.9852056168505516, val loss: 0.04290374368429184, val acc: 0.9866720516962844\n",
      "Epoch 200, train loss: 0.055514074862003326, train acc: 0.9852056168505516, val loss: 0.04290374368429184, val acc: 0.9866720516962844\n",
      "Value 's_m_19_val6: 0.9866720516962844' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.060474567115306854, train acc: 0.9820286195286195, val loss: 0.04701928049325943, val acc: 0.9829545454545454\n",
      "Epoch 50, train loss: 0.060474567115306854, train acc: 0.9820286195286195, val loss: 0.047125011682510376, val acc: 0.9833603896103896\n",
      "Epoch 100, train loss: 0.060474567115306854, train acc: 0.9820286195286195, val loss: 0.047125570476055145, val acc: 0.9833603896103896\n",
      "Epoch 150, train loss: 0.060474567115306854, train acc: 0.9820286195286195, val loss: 0.047125570476055145, val acc: 0.9833603896103896\n",
      "Epoch 200, train loss: 0.060474567115306854, train acc: 0.9820286195286195, val loss: 0.047125570476055145, val acc: 0.9833603896103896\n",
      "Value 's_m_22_val6: 0.9833603896103896' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06782224029302597, train acc: 0.9781705662936588, val loss: 0.05174022540450096, val acc: 0.9800163132137031\n",
      "Epoch 50, train loss: 0.06782224029302597, train acc: 0.9781705662936588, val loss: 0.051716066896915436, val acc: 0.9800163132137031\n",
      "Epoch 100, train loss: 0.06782224029302597, train acc: 0.9781705662936588, val loss: 0.05171600729227066, val acc: 0.9800163132137031\n",
      "Epoch 150, train loss: 0.06782224029302597, train acc: 0.9781705662936588, val loss: 0.05171600729227066, val acc: 0.9800163132137031\n",
      "Epoch 200, train loss: 0.06782224029302597, train acc: 0.9781705662936588, val loss: 0.05171600729227066, val acc: 0.9800163132137031\n",
      "Value 's_m_25_val6: 0.9800163132137031' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07774987816810608, train acc: 0.9741717896174863, val loss: 0.056716907769441605, val acc: 0.9762295081967213\n",
      "Epoch 50, train loss: 0.07774987816810608, train acc: 0.9741717896174863, val loss: 0.05655999109148979, val acc: 0.976639344262295\n",
      "Epoch 100, train loss: 0.07774987816810608, train acc: 0.9741717896174863, val loss: 0.05655929073691368, val acc: 0.976639344262295\n",
      "Epoch 150, train loss: 0.07774987816810608, train acc: 0.9741717896174863, val loss: 0.05655929073691368, val acc: 0.976639344262295\n",
      "Epoch 200, train loss: 0.07774987816810608, train acc: 0.9741717896174863, val loss: 0.05655929073691368, val acc: 0.976639344262295\n",
      "Value 's_m_28_val6: 0.976639344262295' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08886166661977768, train acc: 0.9701582387340901, val loss: 0.06263019889593124, val acc: 0.9728171334431631\n",
      "Epoch 50, train loss: 0.08886166661977768, train acc: 0.9701582387340901, val loss: 0.06232869625091553, val acc: 0.9736408566721582\n",
      "Epoch 100, train loss: 0.08886166661977768, train acc: 0.9701582387340901, val loss: 0.0623273029923439, val acc: 0.9736408566721582\n",
      "Epoch 150, train loss: 0.08886166661977768, train acc: 0.9701582387340901, val loss: 0.0623273029923439, val acc: 0.9736408566721582\n",
      "Epoch 200, train loss: 0.08886166661977768, train acc: 0.9701582387340901, val loss: 0.0623273029923439, val acc: 0.9736408566721582\n",
      "Value 's_m_31_val6: 0.9736408566721582' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09952584654092789, train acc: 0.9664760914760915, val loss: 0.06888461112976074, val acc: 0.9697847682119205\n",
      "Epoch 50, train loss: 0.09952584654092789, train acc: 0.9664760914760915, val loss: 0.06843569129705429, val acc: 0.9701986754966887\n",
      "Epoch 100, train loss: 0.09952584654092789, train acc: 0.9664760914760915, val loss: 0.06843357533216476, val acc: 0.9701986754966887\n",
      "Epoch 150, train loss: 0.09952584654092789, train acc: 0.9664760914760915, val loss: 0.06843357533216476, val acc: 0.9701986754966887\n",
      "Epoch 200, train loss: 0.09952584654092789, train acc: 0.9664760914760915, val loss: 0.06843357533216476, val acc: 0.9701986754966887\n",
      "Value 's_m_34_val6: 0.9701986754966887' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11161063611507416, train acc: 0.962434554973822, val loss: 0.07567637413740158, val acc: 0.96630615640599\n",
      "Epoch 50, train loss: 0.11161063611507416, train acc: 0.962434554973822, val loss: 0.07507379353046417, val acc: 0.9671381031613977\n",
      "Epoch 100, train loss: 0.11161063611507416, train acc: 0.962434554973822, val loss: 0.07507092505693436, val acc: 0.9671381031613977\n",
      "Epoch 150, train loss: 0.11161063611507416, train acc: 0.962434554973822, val loss: 0.07507092505693436, val acc: 0.9671381031613977\n",
      "Epoch 200, train loss: 0.11161063611507416, train acc: 0.962434554973822, val loss: 0.07507092505693436, val acc: 0.9671381031613977\n",
      "Value 's_m_37_val6: 0.9671381031613977' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12531578540802002, train acc: 0.9580696202531646, val loss: 0.0829271450638771, val acc: 0.9632107023411371\n",
      "Epoch 50, train loss: 0.12531578540802002, train acc: 0.9580696202531646, val loss: 0.08215609192848206, val acc: 0.9632107023411371\n",
      "Epoch 100, train loss: 0.12531578540802002, train acc: 0.9580696202531646, val loss: 0.08215238153934479, val acc: 0.9632107023411371\n",
      "Epoch 150, train loss: 0.12531578540802002, train acc: 0.9580696202531646, val loss: 0.08215238153934479, val acc: 0.9632107023411371\n",
      "Epoch 200, train loss: 0.12531578540802002, train acc: 0.9580696202531646, val loss: 0.08215238153934479, val acc: 0.9632107023411371\n",
      "Value 's_m_40_val6: 0.9632107023411371' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13850343227386475, train acc: 0.954215373715905, val loss: 0.09072327613830566, val acc: 0.9592436974789916\n",
      "Epoch 50, train loss: 0.13850343227386475, train acc: 0.954215373715905, val loss: 0.0897795632481575, val acc: 0.9596638655462185\n",
      "Epoch 100, train loss: 0.13850343227386475, train acc: 0.954215373715905, val loss: 0.08977501839399338, val acc: 0.9596638655462185\n",
      "Epoch 150, train loss: 0.13850343227386475, train acc: 0.954215373715905, val loss: 0.08977501094341278, val acc: 0.9596638655462185\n",
      "Epoch 200, train loss: 0.13850343227386475, train acc: 0.954215373715905, val loss: 0.08977501094341278, val acc: 0.9596638655462185\n",
      "Value 's_m_43_val6: 0.9596638655462185' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15180228650569916, train acc: 0.9500356887937188, val loss: 0.09902005642652512, val acc: 0.9556587837837838\n",
      "Epoch 50, train loss: 0.15180228650569916, train acc: 0.9500356887937188, val loss: 0.09789051860570908, val acc: 0.9556587837837838\n",
      "Epoch 100, train loss: 0.15180228650569916, train acc: 0.9500356887937188, val loss: 0.09788507223129272, val acc: 0.9556587837837838\n",
      "Epoch 150, train loss: 0.15180228650569916, train acc: 0.9500356887937188, val loss: 0.09788507223129272, val acc: 0.9556587837837838\n",
      "Epoch 200, train loss: 0.15180228650569916, train acc: 0.9500356887937188, val loss: 0.09788507223129272, val acc: 0.9556587837837838\n",
      "Value 's_m_46_val6: 0.9556587837837838' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16664955019950867, train acc: 0.9458827759798634, val loss: 0.10753723978996277, val acc: 0.9516129032258065\n",
      "Epoch 50, train loss: 0.16664955019950867, train acc: 0.9458827759798634, val loss: 0.10621120780706406, val acc: 0.952037351443124\n",
      "Epoch 100, train loss: 0.16664955019950867, train acc: 0.9458827759798634, val loss: 0.1062048152089119, val acc: 0.952037351443124\n",
      "Epoch 150, train loss: 0.16664955019950867, train acc: 0.9458827759798634, val loss: 0.1062048003077507, val acc: 0.952037351443124\n",
      "Epoch 200, train loss: 0.16664955019950867, train acc: 0.9458827759798634, val loss: 0.1062048003077507, val acc: 0.952037351443124\n",
      "Value 's_m_49_val6: 0.952037351443124' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18275012075901031, train acc: 0.9416213768115942, val loss: 0.11638517677783966, val acc: 0.9479522184300341\n",
      "Epoch 50, train loss: 0.18275012075901031, train acc: 0.9416213768115942, val loss: 0.11483973264694214, val acc: 0.9496587030716723\n",
      "Epoch 100, train loss: 0.18275012075901031, train acc: 0.9416213768115942, val loss: 0.11483220756053925, val acc: 0.9496587030716723\n",
      "Epoch 150, train loss: 0.18275012075901031, train acc: 0.9416213768115942, val loss: 0.11483221501111984, val acc: 0.9496587030716723\n",
      "Epoch 200, train loss: 0.18275012075901031, train acc: 0.9416213768115942, val loss: 0.11483221501111984, val acc: 0.9496587030716723\n",
      "Value 's_m_52_val6: 0.9496587030716723' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09493933618068695, train acc: 0.9806055252168326, val loss: 0.07552868127822876, val acc: 0.982170542635659\n",
      "Epoch 50, train loss: 0.09493933618068695, train acc: 0.9806055252168326, val loss: 0.06774649024009705, val acc: 0.982170542635659\n",
      "Epoch 100, train loss: 0.09493933618068695, train acc: 0.9806055252168326, val loss: 0.0677402913570404, val acc: 0.982170542635659\n",
      "Epoch 150, train loss: 0.09493933618068695, train acc: 0.9806055252168326, val loss: 0.06774028390645981, val acc: 0.982170542635659\n",
      "Epoch 200, train loss: 0.09493933618068695, train acc: 0.9806055252168326, val loss: 0.06774028390645981, val acc: 0.982170542635659\n",
      "Value 's_m_1_val7: 0.9829457364341085' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08189516514539719, train acc: 0.9832632600258733, val loss: 0.05563780665397644, val acc: 0.9855919003115264\n",
      "Epoch 50, train loss: 0.08189516514539719, train acc: 0.9832632600258733, val loss: 0.05624648556113243, val acc: 0.985202492211838\n",
      "Epoch 100, train loss: 0.08189516514539719, train acc: 0.9832632600258733, val loss: 0.05624955892562866, val acc: 0.985202492211838\n",
      "Epoch 150, train loss: 0.08189516514539719, train acc: 0.9832632600258733, val loss: 0.05624957010149956, val acc: 0.985202492211838\n",
      "Epoch 200, train loss: 0.08189516514539719, train acc: 0.9832632600258733, val loss: 0.05624957010149956, val acc: 0.985202492211838\n",
      "Value 's_m_4_val7: 0.9855919003115264' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07060287892818451, train acc: 0.9853060892217519, val loss: 0.04739305004477501, val acc: 0.9886541471048513\n",
      "Epoch 50, train loss: 0.07060287892818451, train acc: 0.9853060892217519, val loss: 0.04784506931900978, val acc: 0.9886541471048513\n",
      "Epoch 100, train loss: 0.07060287892818451, train acc: 0.9853060892217519, val loss: 0.04784733057022095, val acc: 0.9886541471048513\n",
      "Epoch 150, train loss: 0.07060287892818451, train acc: 0.9853060892217519, val loss: 0.047847334295511246, val acc: 0.9886541471048513\n",
      "Epoch 200, train loss: 0.07060287892818451, train acc: 0.9853060892217519, val loss: 0.047847334295511246, val acc: 0.9886541471048513\n",
      "Value 's_m_7_val7: 0.9886541471048513' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0628964751958847, train acc: 0.9865573770491803, val loss: 0.040499527007341385, val acc: 0.9921383647798742\n",
      "Epoch 50, train loss: 0.0628964751958847, train acc: 0.9865573770491803, val loss: 0.04090550169348717, val acc: 0.9921383647798742\n",
      "Epoch 100, train loss: 0.0628964751958847, train acc: 0.9865573770491803, val loss: 0.040907539427280426, val acc: 0.9921383647798742\n",
      "Epoch 150, train loss: 0.0628964751958847, train acc: 0.9865573770491803, val loss: 0.040907539427280426, val acc: 0.9921383647798742\n",
      "Epoch 200, train loss: 0.0628964751958847, train acc: 0.9865573770491803, val loss: 0.040907539427280426, val acc: 0.9921383647798742\n",
      "Value 's_m_10_val7: 0.9921383647798742' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.058039844036102295, train acc: 0.9869593925387917, val loss: 0.03806096687912941, val acc: 0.990916271721959\n",
      "Epoch 50, train loss: 0.058039844036102295, train acc: 0.9869593925387917, val loss: 0.0384347178041935, val acc: 0.990916271721959\n",
      "Epoch 100, train loss: 0.058039844036102295, train acc: 0.9869593925387917, val loss: 0.03843658044934273, val acc: 0.990916271721959\n",
      "Epoch 150, train loss: 0.058039844036102295, train acc: 0.9869593925387917, val loss: 0.03843658044934273, val acc: 0.990916271721959\n",
      "Epoch 200, train loss: 0.058039844036102295, train acc: 0.9869593925387917, val loss: 0.03843658044934273, val acc: 0.990916271721959\n",
      "Value 's_m_13_val7: 0.990916271721959' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05647630989551544, train acc: 0.9856632313829787, val loss: 0.03759121894836426, val acc: 0.9896825396825397\n",
      "Epoch 50, train loss: 0.05647630989551544, train acc: 0.9856632313829787, val loss: 0.037933822721242905, val acc: 0.9900793650793651\n",
      "Epoch 100, train loss: 0.05647630989551544, train acc: 0.9856632313829787, val loss: 0.03793557360768318, val acc: 0.9900793650793651\n",
      "Epoch 150, train loss: 0.05647630989551544, train acc: 0.9856632313829787, val loss: 0.03793557360768318, val acc: 0.9900793650793651\n",
      "Epoch 200, train loss: 0.05647630989551544, train acc: 0.9856632313829787, val loss: 0.03793557360768318, val acc: 0.9900793650793651\n",
      "Value 's_m_16_val7: 0.9900793650793651' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05825148895382881, train acc: 0.9835118848342819, val loss: 0.03651369363069534, val acc: 0.988835725677831\n",
      "Epoch 50, train loss: 0.05825148895382881, train acc: 0.9835118848342819, val loss: 0.03679034113883972, val acc: 0.988835725677831\n",
      "Epoch 100, train loss: 0.05825148895382881, train acc: 0.9835118848342819, val loss: 0.03679174929857254, val acc: 0.988835725677831\n",
      "Epoch 150, train loss: 0.05825148895382881, train acc: 0.9835118848342819, val loss: 0.03679174929857254, val acc: 0.988835725677831\n",
      "Epoch 200, train loss: 0.05825148895382881, train acc: 0.9835118848342819, val loss: 0.03679174929857254, val acc: 0.988835725677831\n",
      "Value 's_m_19_val7: 0.988835725677831' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06291206926107407, train acc: 0.9801500337154416, val loss: 0.035694997757673264, val acc: 0.9875801282051282\n",
      "Epoch 50, train loss: 0.06291206926107407, train acc: 0.9801500337154416, val loss: 0.03589565306901932, val acc: 0.9875801282051282\n",
      "Epoch 100, train loss: 0.06291206926107407, train acc: 0.9801500337154416, val loss: 0.03589668497443199, val acc: 0.9875801282051282\n",
      "Epoch 150, train loss: 0.06291206926107407, train acc: 0.9801500337154416, val loss: 0.03589668497443199, val acc: 0.9875801282051282\n",
      "Epoch 200, train loss: 0.06291206926107407, train acc: 0.9801500337154416, val loss: 0.03589668497443199, val acc: 0.9875801282051282\n",
      "Value 's_m_22_val7: 0.9875801282051282' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06988418847322464, train acc: 0.9762733446519525, val loss: 0.036973532289266586, val acc: 0.9863123993558777\n",
      "Epoch 50, train loss: 0.06988418847322464, train acc: 0.9762733446519525, val loss: 0.037097908556461334, val acc: 0.9863123993558777\n",
      "Epoch 100, train loss: 0.06988418847322464, train acc: 0.9762733446519525, val loss: 0.03709854185581207, val acc: 0.9863123993558777\n",
      "Epoch 150, train loss: 0.06988418847322464, train acc: 0.9762733446519525, val loss: 0.03709854185581207, val acc: 0.9863123993558777\n",
      "Epoch 200, train loss: 0.06988418847322464, train acc: 0.9762733446519525, val loss: 0.03709854185581207, val acc: 0.9863123993558777\n",
      "Value 's_m_25_val7: 0.9863123993558777' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07940200716257095, train acc: 0.9724264705882353, val loss: 0.03876584395766258, val acc: 0.985032362459547\n",
      "Epoch 50, train loss: 0.07940200716257095, train acc: 0.9724264705882353, val loss: 0.03880830109119415, val acc: 0.985032362459547\n",
      "Epoch 100, train loss: 0.07940200716257095, train acc: 0.9724264705882353, val loss: 0.038808535784482956, val acc: 0.985032362459547\n",
      "Epoch 150, train loss: 0.07940200716257095, train acc: 0.9724264705882353, val loss: 0.038808535784482956, val acc: 0.985032362459547\n",
      "Epoch 200, train loss: 0.07940200716257095, train acc: 0.9724264705882353, val loss: 0.038808535784482956, val acc: 0.985032362459547\n",
      "Value 's_m_28_val7: 0.985032362459547' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09015276283025742, train acc: 0.968222528418877, val loss: 0.04263520985841751, val acc: 0.9829268292682927\n",
      "Epoch 50, train loss: 0.09015276283025742, train acc: 0.968222528418877, val loss: 0.0426001213490963, val acc: 0.9829268292682927\n",
      "Epoch 100, train loss: 0.09015276283025742, train acc: 0.968222528418877, val loss: 0.04259997978806496, val acc: 0.9829268292682927\n",
      "Epoch 150, train loss: 0.09015276283025742, train acc: 0.968222528418877, val loss: 0.04259997978806496, val acc: 0.9829268292682927\n",
      "Epoch 200, train loss: 0.09015276283025742, train acc: 0.968222528418877, val loss: 0.04259997978806496, val acc: 0.9829268292682927\n",
      "Value 's_m_31_val7: 0.9829268292682927' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1007649227976799, train acc: 0.9644344205412908, val loss: 0.0483369454741478, val acc: 0.9791666666666666\n",
      "Epoch 50, train loss: 0.1007649227976799, train acc: 0.9644344205412908, val loss: 0.04821661859750748, val acc: 0.9791666666666666\n",
      "Epoch 100, train loss: 0.1007649227976799, train acc: 0.9644344205412908, val loss: 0.04821605980396271, val acc: 0.9791666666666666\n",
      "Epoch 150, train loss: 0.1007649227976799, train acc: 0.9644344205412908, val loss: 0.04821605980396271, val acc: 0.9791666666666666\n",
      "Epoch 200, train loss: 0.1007649227976799, train acc: 0.9644344205412908, val loss: 0.04821605980396271, val acc: 0.9791666666666666\n",
      "Value 's_m_34_val7: 0.9791666666666666' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11271154135465622, train acc: 0.9604596295001747, val loss: 0.056354254484176636, val acc: 0.9753694581280788\n",
      "Epoch 50, train loss: 0.11271154135465622, train acc: 0.9604596295001747, val loss: 0.056132037192583084, val acc: 0.9753694581280788\n",
      "Epoch 100, train loss: 0.11271154135465622, train acc: 0.9604596295001747, val loss: 0.05613096058368683, val acc: 0.9753694581280788\n",
      "Epoch 150, train loss: 0.11271154135465622, train acc: 0.9604596295001747, val loss: 0.05613096058368683, val acc: 0.9753694581280788\n",
      "Epoch 200, train loss: 0.11271154135465622, train acc: 0.9604596295001747, val loss: 0.05613096058368683, val acc: 0.9753694581280788\n",
      "Value 's_m_37_val7: 0.9753694581280788' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1261521875858307, train acc: 0.9560739436619718, val loss: 0.06647379696369171, val acc: 0.9715346534653465\n",
      "Epoch 50, train loss: 0.1261521875858307, train acc: 0.9560739436619718, val loss: 0.06615853309631348, val acc: 0.9715346534653465\n",
      "Epoch 100, train loss: 0.1261521875858307, train acc: 0.9560739436619718, val loss: 0.06615700572729111, val acc: 0.9715346534653465\n",
      "Epoch 150, train loss: 0.1261521875858307, train acc: 0.9560739436619718, val loss: 0.06615700572729111, val acc: 0.9715346534653465\n",
      "Epoch 200, train loss: 0.1261521875858307, train acc: 0.9560739436619718, val loss: 0.06615700572729111, val acc: 0.9715346534653465\n",
      "Value 's_m_40_val7: 0.9715346534653465' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13968217372894287, train acc: 0.9517559418233416, val loss: 0.0760672464966774, val acc: 0.9680762852404643\n",
      "Epoch 50, train loss: 0.13968217372894287, train acc: 0.9517559418233416, val loss: 0.07567407935857773, val acc: 0.9680762852404643\n",
      "Epoch 100, train loss: 0.13968217372894287, train acc: 0.9517559418233416, val loss: 0.07567217946052551, val acc: 0.9680762852404643\n",
      "Epoch 150, train loss: 0.13968217372894287, train acc: 0.9517559418233416, val loss: 0.07567217946052551, val acc: 0.9680762852404643\n",
      "Epoch 200, train loss: 0.13968217372894287, train acc: 0.9517559418233416, val loss: 0.07567217946052551, val acc: 0.9680762852404643\n",
      "Value 's_m_43_val7: 0.9680762852404643' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15372146666049957, train acc: 0.9476411722659042, val loss: 0.08433172851800919, val acc: 0.9654166666666667\n",
      "Epoch 50, train loss: 0.15372146666049957, train acc: 0.9476411722659042, val loss: 0.08385760337114334, val acc: 0.9654166666666667\n",
      "Epoch 100, train loss: 0.15372146666049957, train acc: 0.9476411722659042, val loss: 0.08385533094406128, val acc: 0.9654166666666667\n",
      "Epoch 150, train loss: 0.15372146666049957, train acc: 0.9476411722659042, val loss: 0.08385533094406128, val acc: 0.9654166666666667\n",
      "Epoch 200, train loss: 0.15372146666049957, train acc: 0.9476411722659042, val loss: 0.08385533094406128, val acc: 0.9654166666666667\n",
      "Value 's_m_46_val7: 0.9654166666666667' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1693689525127411, train acc: 0.943014043932301, val loss: 0.09324603527784348, val acc: 0.9631490787269682\n",
      "Epoch 50, train loss: 0.1693689525127411, train acc: 0.943014043932301, val loss: 0.09269578754901886, val acc: 0.9635678391959799\n",
      "Epoch 100, train loss: 0.1693689525127411, train acc: 0.943014043932301, val loss: 0.09269313514232635, val acc: 0.9635678391959799\n",
      "Epoch 150, train loss: 0.1693689525127411, train acc: 0.943014043932301, val loss: 0.09269314259290695, val acc: 0.9635678391959799\n",
      "Epoch 200, train loss: 0.1693689525127411, train acc: 0.943014043932301, val loss: 0.09269314259290695, val acc: 0.9635678391959799\n",
      "Value 's_m_49_val7: 0.9635678391959799' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18651455640792847, train acc: 0.9384071117561683, val loss: 0.10233666002750397, val acc: 0.9608585858585859\n",
      "Epoch 50, train loss: 0.18651455640792847, train acc: 0.9384071117561683, val loss: 0.10170924663543701, val acc: 0.9608585858585859\n",
      "Epoch 100, train loss: 0.18651455640792847, train acc: 0.9384071117561683, val loss: 0.10170620679855347, val acc: 0.9608585858585859\n",
      "Epoch 150, train loss: 0.18651455640792847, train acc: 0.9384071117561683, val loss: 0.10170620679855347, val acc: 0.9608585858585859\n",
      "Epoch 200, train loss: 0.18651455640792847, train acc: 0.9384071117561683, val loss: 0.10170620679855347, val acc: 0.9608585858585859\n",
      "Value 's_m_52_val7: 0.9608585858585859' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08277115225791931, train acc: 0.9813887176325524, val loss: 0.12943917512893677, val acc: 0.9751958224543081\n",
      "Epoch 50, train loss: 0.08277115225791931, train acc: 0.9813887176325524, val loss: 0.11173300445079803, val acc: 0.9797650130548303\n",
      "Epoch 100, train loss: 0.08277115225791931, train acc: 0.9813887176325524, val loss: 0.11167038977146149, val acc: 0.9797650130548303\n",
      "Epoch 150, train loss: 0.08277115225791931, train acc: 0.9813887176325524, val loss: 0.11167017370462418, val acc: 0.9797650130548303\n",
      "Epoch 200, train loss: 0.08277115225791931, train acc: 0.9813887176325524, val loss: 0.11167017370462418, val acc: 0.9797650130548303\n",
      "Value 's_m_1_val8: 0.9797650130548303' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07014890760183334, train acc: 0.984641638225256, val loss: 0.10552741587162018, val acc: 0.9776315789473684\n",
      "Epoch 50, train loss: 0.07014890760183334, train acc: 0.984641638225256, val loss: 0.10603540390729904, val acc: 0.9776315789473684\n",
      "Epoch 100, train loss: 0.07014890760183334, train acc: 0.984641638225256, val loss: 0.10603772848844528, val acc: 0.9776315789473684\n",
      "Epoch 150, train loss: 0.07014890760183334, train acc: 0.984641638225256, val loss: 0.10603772103786469, val acc: 0.9776315789473684\n",
      "Epoch 200, train loss: 0.07014890760183334, train acc: 0.984641638225256, val loss: 0.10603772103786469, val acc: 0.9776315789473684\n",
      "Value 's_m_4_val8: 0.9776315789473684' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05953196436166763, train acc: 0.9877029981261711, val loss: 0.10136411339044571, val acc: 0.9754641909814323\n",
      "Epoch 50, train loss: 0.05953196436166763, train acc: 0.9877029981261711, val loss: 0.10176470130681992, val acc: 0.9754641909814323\n",
      "Epoch 100, train loss: 0.05953196436166763, train acc: 0.9877029981261711, val loss: 0.10176659375429153, val acc: 0.9754641909814323\n",
      "Epoch 150, train loss: 0.05953196436166763, train acc: 0.9877029981261711, val loss: 0.10176660120487213, val acc: 0.9754641909814323\n",
      "Epoch 200, train loss: 0.05953196436166763, train acc: 0.9877029981261711, val loss: 0.10176660120487213, val acc: 0.9754641909814323\n",
      "Value 's_m_7_val8: 0.9754641909814323' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.052450671792030334, train acc: 0.9895866079849104, val loss: 0.09808177500963211, val acc: 0.9732620320855615\n",
      "Epoch 50, train loss: 0.052450671792030334, train acc: 0.9895866079849104, val loss: 0.09842059016227722, val acc: 0.9725935828877005\n",
      "Epoch 100, train loss: 0.052450671792030334, train acc: 0.9895866079849104, val loss: 0.09842220693826675, val acc: 0.9725935828877005\n",
      "Epoch 150, train loss: 0.052450671792030334, train acc: 0.9895866079849104, val loss: 0.09842220693826675, val acc: 0.9725935828877005\n",
      "Epoch 200, train loss: 0.052450671792030334, train acc: 0.9895866079849104, val loss: 0.09842220693826675, val acc: 0.9725935828877005\n",
      "Value 's_m_10_val8: 0.9732620320855615' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.048574332147836685, train acc: 0.9899525316455696, val loss: 0.09612365812063217, val acc: 0.9703504043126685\n",
      "Epoch 50, train loss: 0.048574332147836685, train acc: 0.9899525316455696, val loss: 0.09632137417793274, val acc: 0.9703504043126685\n",
      "Epoch 100, train loss: 0.048574332147836685, train acc: 0.9899525316455696, val loss: 0.0963224321603775, val acc: 0.9703504043126685\n",
      "Epoch 150, train loss: 0.048574332147836685, train acc: 0.9899525316455696, val loss: 0.0963224321603775, val acc: 0.9703504043126685\n",
      "Epoch 200, train loss: 0.048574332147836685, train acc: 0.9899525316455696, val loss: 0.0963224321603775, val acc: 0.9703504043126685\n",
      "Value 's_m_13_val8: 0.9703504043126685' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.047785867005586624, train acc: 0.9884119146224912, val loss: 0.09832032769918442, val acc: 0.9680706521739131\n",
      "Epoch 50, train loss: 0.047785867005586624, train acc: 0.9884119146224912, val loss: 0.0984029471874237, val acc: 0.9680706521739131\n",
      "Epoch 100, train loss: 0.047785867005586624, train acc: 0.9884119146224912, val loss: 0.09840349853038788, val acc: 0.9680706521739131\n",
      "Epoch 150, train loss: 0.047785867005586624, train acc: 0.9884119146224912, val loss: 0.09840349853038788, val acc: 0.9680706521739131\n",
      "Epoch 200, train loss: 0.047785867005586624, train acc: 0.9884119146224912, val loss: 0.09840349853038788, val acc: 0.9680706521739131\n",
      "Value 's_m_16_val8: 0.9680706521739131' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.049897924065589905, train acc: 0.9856478511866581, val loss: 0.10357902944087982, val acc: 0.9657534246575342\n",
      "Epoch 50, train loss: 0.049897924065589905, train acc: 0.9856478511866581, val loss: 0.10350853949785233, val acc: 0.9657534246575342\n",
      "Epoch 100, train loss: 0.049897924065589905, train acc: 0.9856478511866581, val loss: 0.10350843518972397, val acc: 0.9657534246575342\n",
      "Epoch 150, train loss: 0.049897924065589905, train acc: 0.9856478511866581, val loss: 0.10350843518972397, val acc: 0.9657534246575342\n",
      "Epoch 200, train loss: 0.049897924065589905, train acc: 0.9856478511866581, val loss: 0.10350843518972397, val acc: 0.9657534246575342\n",
      "Value 's_m_19_val8: 0.9657534246575342' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05460819602012634, train acc: 0.9822812399095899, val loss: 0.11209012567996979, val acc: 0.9633977900552486\n",
      "Epoch 50, train loss: 0.05460819602012634, train acc: 0.9822812399095899, val loss: 0.11184071749448776, val acc: 0.9640883977900553\n",
      "Epoch 100, train loss: 0.05460819602012634, train acc: 0.9822812399095899, val loss: 0.11183977872133255, val acc: 0.9640883977900553\n",
      "Epoch 150, train loss: 0.05460819602012634, train acc: 0.9822812399095899, val loss: 0.11183977872133255, val acc: 0.9640883977900553\n",
      "Epoch 200, train loss: 0.05460819602012634, train acc: 0.9822812399095899, val loss: 0.11183977872133255, val acc: 0.9640883977900553\n",
      "Value 's_m_22_val8: 0.9640883977900553' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.061571087688207626, train acc: 0.9785029258777633, val loss: 0.12373202294111252, val acc: 0.9616991643454039\n",
      "Epoch 50, train loss: 0.061571087688207626, train acc: 0.9785029258777633, val loss: 0.12330842018127441, val acc: 0.9616991643454039\n",
      "Epoch 100, train loss: 0.061571087688207626, train acc: 0.9785029258777633, val loss: 0.12330663204193115, val acc: 0.9616991643454039\n",
      "Epoch 150, train loss: 0.061571087688207626, train acc: 0.9785029258777633, val loss: 0.12330663204193115, val acc: 0.9616991643454039\n",
      "Epoch 200, train loss: 0.061571087688207626, train acc: 0.9785029258777633, val loss: 0.12330663204193115, val acc: 0.9616991643454039\n",
      "Value 's_m_25_val8: 0.9616991643454039' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07090961933135986, train acc: 0.9745908346972176, val loss: 0.1381126493215561, val acc: 0.9592696629213483\n",
      "Epoch 50, train loss: 0.07090961933135986, train acc: 0.9745908346972176, val loss: 0.13749311864376068, val acc: 0.9592696629213483\n",
      "Epoch 100, train loss: 0.07090961933135986, train acc: 0.9745908346972176, val loss: 0.1374904215335846, val acc: 0.9592696629213483\n",
      "Epoch 150, train loss: 0.07090961933135986, train acc: 0.9745908346972176, val loss: 0.1374904215335846, val acc: 0.9592696629213483\n",
      "Epoch 200, train loss: 0.07090961933135986, train acc: 0.9745908346972176, val loss: 0.1374904215335846, val acc: 0.9592696629213483\n",
      "Value 's_m_28_val8: 0.9592696629213483' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08134161680936813, train acc: 0.9706657877389585, val loss: 0.15580077469348907, val acc: 0.9539660056657224\n",
      "Epoch 50, train loss: 0.08134161680936813, train acc: 0.9706657877389585, val loss: 0.1549849659204483, val acc: 0.9539660056657224\n",
      "Epoch 100, train loss: 0.08134161680936813, train acc: 0.9706657877389585, val loss: 0.15498137474060059, val acc: 0.9539660056657224\n",
      "Epoch 150, train loss: 0.08134161680936813, train acc: 0.9706657877389585, val loss: 0.15498137474060059, val acc: 0.9539660056657224\n",
      "Epoch 200, train loss: 0.08134161680936813, train acc: 0.9706657877389585, val loss: 0.15498137474060059, val acc: 0.9539660056657224\n",
      "Value 's_m_31_val8: 0.9539660056657224' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09151554107666016, train acc: 0.9669349485562562, val loss: 0.17341652512550354, val acc: 0.9485714285714286\n",
      "Epoch 50, train loss: 0.09151554107666016, train acc: 0.9669349485562562, val loss: 0.17240427434444427, val acc: 0.9485714285714286\n",
      "Epoch 100, train loss: 0.09151554107666016, train acc: 0.9669349485562562, val loss: 0.17239972949028015, val acc: 0.9485714285714286\n",
      "Epoch 150, train loss: 0.09151554107666016, train acc: 0.9669349485562562, val loss: 0.17239972949028015, val acc: 0.9485714285714286\n",
      "Epoch 200, train loss: 0.09151554107666016, train acc: 0.9669349485562562, val loss: 0.17239972949028015, val acc: 0.9485714285714286\n",
      "Value 's_m_34_val8: 0.9485714285714286' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10306308418512344, train acc: 0.9629428475935828, val loss: 0.19305479526519775, val acc: 0.94164265129683\n",
      "Epoch 50, train loss: 0.10306308418512344, train acc: 0.9629428475935828, val loss: 0.1918001025915146, val acc: 0.94164265129683\n",
      "Epoch 100, train loss: 0.10306308418512344, train acc: 0.9629428475935828, val loss: 0.19179438054561615, val acc: 0.94164265129683\n",
      "Epoch 150, train loss: 0.10306308418512344, train acc: 0.9629428475935828, val loss: 0.19179438054561615, val acc: 0.94164265129683\n",
      "Epoch 200, train loss: 0.10306308418512344, train acc: 0.9629428475935828, val loss: 0.19179438054561615, val acc: 0.94164265129683\n",
      "Value 's_m_37_val8: 0.9423631123919308' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11609752476215363, train acc: 0.9585577246718276, val loss: 0.21546390652656555, val acc: 0.934593023255814\n",
      "Epoch 50, train loss: 0.11609752476215363, train acc: 0.9585577246718276, val loss: 0.21395541727542877, val acc: 0.934593023255814\n",
      "Epoch 100, train loss: 0.11609752476215363, train acc: 0.9585577246718276, val loss: 0.21394851803779602, val acc: 0.934593023255814\n",
      "Epoch 150, train loss: 0.11609752476215363, train acc: 0.9585577246718276, val loss: 0.21394851803779602, val acc: 0.934593023255814\n",
      "Epoch 200, train loss: 0.11609752476215363, train acc: 0.9585577246718276, val loss: 0.21394851803779602, val acc: 0.934593023255814\n",
      "Value 's_m_40_val8: 0.934593023255814' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12869660556316376, train acc: 0.9544915254237288, val loss: 0.2402799278497696, val acc: 0.9274193548387096\n",
      "Epoch 50, train loss: 0.12869660556316376, train acc: 0.9544915254237288, val loss: 0.2385375201702118, val acc: 0.9281524926686217\n",
      "Epoch 100, train loss: 0.12869660556316376, train acc: 0.9544915254237288, val loss: 0.2385295033454895, val acc: 0.9281524926686217\n",
      "Epoch 150, train loss: 0.12869660556316376, train acc: 0.9544915254237288, val loss: 0.2385295033454895, val acc: 0.9281524926686217\n",
      "Epoch 200, train loss: 0.12869660556316376, train acc: 0.9544915254237288, val loss: 0.2385295033454895, val acc: 0.9281524926686217\n",
      "Value 's_m_43_val8: 0.9281524926686217' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1414792686700821, train acc: 0.9501963127347217, val loss: 0.2662278115749359, val acc: 0.9208579881656804\n",
      "Epoch 50, train loss: 0.1414792686700821, train acc: 0.9501963127347217, val loss: 0.26418665051460266, val acc: 0.9208579881656804\n",
      "Epoch 100, train loss: 0.1414792686700821, train acc: 0.9501963127347217, val loss: 0.264177143573761, val acc: 0.9208579881656804\n",
      "Epoch 150, train loss: 0.1414792686700821, train acc: 0.9501963127347217, val loss: 0.264177143573761, val acc: 0.9208579881656804\n",
      "Epoch 200, train loss: 0.1414792686700821, train acc: 0.9501963127347217, val loss: 0.264177143573761, val acc: 0.9208579881656804\n",
      "Value 's_m_46_val8: 0.9208579881656804' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15581129491329193, train acc: 0.9459250343878954, val loss: 0.2925237715244293, val acc: 0.9134328358208955\n",
      "Epoch 50, train loss: 0.15581129491329193, train acc: 0.9459250343878954, val loss: 0.2901674211025238, val acc: 0.914179104477612\n",
      "Epoch 100, train loss: 0.15581129491329193, train acc: 0.9459250343878954, val loss: 0.2901563346385956, val acc: 0.914179104477612\n",
      "Epoch 150, train loss: 0.15581129491329193, train acc: 0.9459250343878954, val loss: 0.2901563048362732, val acc: 0.914179104477612\n",
      "Epoch 200, train loss: 0.15581129491329193, train acc: 0.9459250343878954, val loss: 0.2901563048362732, val acc: 0.914179104477612\n",
      "Value 's_m_49_val8: 0.914179104477612' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17134466767311096, train acc: 0.9418514028403187, val loss: 0.32048550248146057, val acc: 0.9066265060240963\n",
      "Epoch 50, train loss: 0.17134466767311096, train acc: 0.9418514028403187, val loss: 0.31780609488487244, val acc: 0.9066265060240963\n",
      "Epoch 100, train loss: 0.17134466767311096, train acc: 0.9418514028403187, val loss: 0.3177935481071472, val acc: 0.9066265060240963\n",
      "Epoch 150, train loss: 0.17134466767311096, train acc: 0.9418514028403187, val loss: 0.3177935481071472, val acc: 0.9066265060240963\n",
      "Epoch 200, train loss: 0.17134466767311096, train acc: 0.9418514028403187, val loss: 0.3177935481071472, val acc: 0.9066265060240963\n",
      "Value 's_m_52_val8: 0.9066265060240963' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09466622024774551, train acc: 0.9808183123305152, val loss: 0.0896153450012207, val acc: 0.9721760797342193\n",
      "Epoch 50, train loss: 0.09466622024774551, train acc: 0.9808183123305152, val loss: 0.09322482347488403, val acc: 0.973421926910299\n",
      "Epoch 100, train loss: 0.09466622024774551, train acc: 0.9808183123305152, val loss: 0.09326090663671494, val acc: 0.973421926910299\n",
      "Epoch 150, train loss: 0.09466622024774551, train acc: 0.9808183123305152, val loss: 0.0932609811425209, val acc: 0.973421926910299\n",
      "Epoch 200, train loss: 0.09466622024774551, train acc: 0.9808183123305152, val loss: 0.0932609811425209, val acc: 0.973421926910299\n",
      "Value 's_m_1_val9: 0.973421926910299' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08128242939710617, train acc: 0.9835394250843102, val loss: 0.07890770584344864, val acc: 0.9770450751252087\n",
      "Epoch 50, train loss: 0.08128242939710617, train acc: 0.9835394250843102, val loss: 0.07945501804351807, val acc: 0.9770450751252087\n",
      "Epoch 100, train loss: 0.08128242939710617, train acc: 0.9835394250843102, val loss: 0.07945794612169266, val acc: 0.9770450751252087\n",
      "Epoch 150, train loss: 0.08128242939710617, train acc: 0.9835394250843102, val loss: 0.07945793867111206, val acc: 0.9770450751252087\n",
      "Epoch 200, train loss: 0.08128242939710617, train acc: 0.9835394250843102, val loss: 0.07945793867111206, val acc: 0.9770450751252087\n",
      "Value 's_m_4_val9: 0.9770450751252087' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06993135064840317, train acc: 0.98589329021827, val loss: 0.06650183349847794, val acc: 0.9807046979865772\n",
      "Epoch 50, train loss: 0.06993135064840317, train acc: 0.98589329021827, val loss: 0.0669260025024414, val acc: 0.9807046979865772\n",
      "Epoch 100, train loss: 0.06993135064840317, train acc: 0.98589329021827, val loss: 0.06692826002836227, val acc: 0.9807046979865772\n",
      "Epoch 150, train loss: 0.06993135064840317, train acc: 0.98589329021827, val loss: 0.06692826002836227, val acc: 0.9807046979865772\n",
      "Epoch 200, train loss: 0.06993135064840317, train acc: 0.98589329021827, val loss: 0.06692826002836227, val acc: 0.9807046979865772\n",
      "Value 's_m_7_val9: 0.9807046979865772' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06220601126551628, train acc: 0.987221227413316, val loss: 0.05589042231440544, val acc: 0.9844013490725126\n",
      "Epoch 50, train loss: 0.06220601126551628, train acc: 0.987221227413316, val loss: 0.056249652057886124, val acc: 0.9844013490725126\n",
      "Epoch 100, train loss: 0.06220601126551628, train acc: 0.987221227413316, val loss: 0.056251537054777145, val acc: 0.9844013490725126\n",
      "Epoch 150, train loss: 0.06220601126551628, train acc: 0.987221227413316, val loss: 0.056251537054777145, val acc: 0.9844013490725126\n",
      "Epoch 200, train loss: 0.06220601126551628, train acc: 0.987221227413316, val loss: 0.056251537054777145, val acc: 0.9844013490725126\n",
      "Value 's_m_10_val9: 0.9844013490725126' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.057681385427713394, train acc: 0.9872152106212096, val loss: 0.04769495502114296, val acc: 0.988135593220339\n",
      "Epoch 50, train loss: 0.057681385427713394, train acc: 0.9872152106212096, val loss: 0.04799174517393112, val acc: 0.9877118644067797\n",
      "Epoch 100, train loss: 0.057681385427713394, train acc: 0.9872152106212096, val loss: 0.04799334704875946, val acc: 0.9877118644067797\n",
      "Epoch 150, train loss: 0.057681385427713394, train acc: 0.9872152106212096, val loss: 0.04799334704875946, val acc: 0.9877118644067797\n",
      "Epoch 200, train loss: 0.057681385427713394, train acc: 0.9872152106212096, val loss: 0.04799334704875946, val acc: 0.9877118644067797\n",
      "Value 's_m_13_val9: 0.988135593220339' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05643587186932564, train acc: 0.9856824558508005, val loss: 0.04175609350204468, val acc: 0.989778534923339\n",
      "Epoch 50, train loss: 0.05643587186932564, train acc: 0.9856824558508005, val loss: 0.04199223965406418, val acc: 0.989778534923339\n",
      "Epoch 100, train loss: 0.05643587186932564, train acc: 0.9856824558508005, val loss: 0.04199352115392685, val acc: 0.989778534923339\n",
      "Epoch 150, train loss: 0.05643587186932564, train acc: 0.9856824558508005, val loss: 0.04199352115392685, val acc: 0.989778534923339\n",
      "Epoch 200, train loss: 0.05643587186932564, train acc: 0.9856824558508005, val loss: 0.04199352115392685, val acc: 0.989778534923339\n",
      "Value 's_m_16_val9: 0.989778534923339' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05832284316420555, train acc: 0.9830064816353664, val loss: 0.037254709750413895, val acc: 0.9910102739726028\n",
      "Epoch 50, train loss: 0.05832284316420555, train acc: 0.9830064816353664, val loss: 0.037428900599479675, val acc: 0.9910102739726028\n",
      "Epoch 100, train loss: 0.05832284316420555, train acc: 0.9830064816353664, val loss: 0.037429869174957275, val acc: 0.9910102739726028\n",
      "Epoch 150, train loss: 0.05832284316420555, train acc: 0.9830064816353664, val loss: 0.037429869174957275, val acc: 0.9910102739726028\n",
      "Epoch 200, train loss: 0.05832284316420555, train acc: 0.9830064816353664, val loss: 0.037429869174957275, val acc: 0.9910102739726028\n",
      "Value 's_m_19_val9: 0.9910102739726028' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06306994706392288, train acc: 0.9793305439330544, val loss: 0.033421874046325684, val acc: 0.9922547332185886\n",
      "Epoch 50, train loss: 0.06306994706392288, train acc: 0.9793305439330544, val loss: 0.033525679260492325, val acc: 0.9918244406196214\n",
      "Epoch 100, train loss: 0.06306994706392288, train acc: 0.9793305439330544, val loss: 0.03352629765868187, val acc: 0.9918244406196214\n",
      "Epoch 150, train loss: 0.06306994706392288, train acc: 0.9793305439330544, val loss: 0.03352629765868187, val acc: 0.9918244406196214\n",
      "Epoch 200, train loss: 0.06306994706392288, train acc: 0.9793305439330544, val loss: 0.03352629765868187, val acc: 0.9918244406196214\n",
      "Value 's_m_22_val9: 0.9922547332185886' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07000651955604553, train acc: 0.9756868363391201, val loss: 0.03291896730661392, val acc: 0.990484429065744\n",
      "Epoch 50, train loss: 0.07000651955604553, train acc: 0.9756868363391201, val loss: 0.0329585038125515, val acc: 0.9900519031141869\n",
      "Epoch 100, train loss: 0.07000651955604553, train acc: 0.9756868363391201, val loss: 0.03295876830816269, val acc: 0.9900519031141869\n",
      "Epoch 150, train loss: 0.07000651955604553, train acc: 0.9756868363391201, val loss: 0.03295876830816269, val acc: 0.9900519031141869\n",
      "Epoch 200, train loss: 0.07000651955604553, train acc: 0.9756868363391201, val loss: 0.03295876830816269, val acc: 0.9900519031141869\n",
      "Value 's_m_25_val9: 0.990484429065744' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07942820340394974, train acc: 0.9719487353590223, val loss: 0.03400076553225517, val acc: 0.9891304347826086\n",
      "Epoch 50, train loss: 0.07942820340394974, train acc: 0.9719487353590223, val loss: 0.03395640850067139, val acc: 0.9886956521739131\n",
      "Epoch 100, train loss: 0.07942820340394974, train acc: 0.9719487353590223, val loss: 0.033956240862607956, val acc: 0.9886956521739131\n",
      "Epoch 150, train loss: 0.07942820340394974, train acc: 0.9719487353590223, val loss: 0.033956240862607956, val acc: 0.9886956521739131\n",
      "Epoch 200, train loss: 0.07942820340394974, train acc: 0.9719487353590223, val loss: 0.033956240862607956, val acc: 0.9886956521739131\n",
      "Value 's_m_28_val9: 0.9891304347826086' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0899696946144104, train acc: 0.9681569499059668, val loss: 0.03858628123998642, val acc: 0.986451048951049\n",
      "Epoch 50, train loss: 0.0899696946144104, train acc: 0.9681569499059668, val loss: 0.038442306220531464, val acc: 0.986013986013986\n",
      "Epoch 100, train loss: 0.0899696946144104, train acc: 0.9681569499059668, val loss: 0.03844163566827774, val acc: 0.986013986013986\n",
      "Epoch 150, train loss: 0.0899696946144104, train acc: 0.9681569499059668, val loss: 0.03844163566827774, val acc: 0.986013986013986\n",
      "Epoch 200, train loss: 0.0899696946144104, train acc: 0.9681569499059668, val loss: 0.03844163566827774, val acc: 0.986013986013986\n",
      "Value 's_m_31_val9: 0.986451048951049' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10052557289600372, train acc: 0.9643103151369037, val loss: 0.04365568980574608, val acc: 0.9846221441124781\n",
      "Epoch 50, train loss: 0.10052557289600372, train acc: 0.9643103151369037, val loss: 0.04339895769953728, val acc: 0.9846221441124781\n",
      "Epoch 100, train loss: 0.10052557289600372, train acc: 0.9643103151369037, val loss: 0.0433976985514164, val acc: 0.9846221441124781\n",
      "Epoch 150, train loss: 0.10052557289600372, train acc: 0.9643103151369037, val loss: 0.0433976985514164, val acc: 0.9846221441124781\n",
      "Epoch 200, train loss: 0.10052557289600372, train acc: 0.9643103151369037, val loss: 0.0433976985514164, val acc: 0.9846221441124781\n",
      "Value 's_m_34_val9: 0.9846221441124781' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11235576868057251, train acc: 0.96040763226366, val loss: 0.051639944314956665, val acc: 0.9823321554770318\n",
      "Epoch 50, train loss: 0.11235576868057251, train acc: 0.96040763226366, val loss: 0.05129876360297203, val acc: 0.9823321554770318\n",
      "Epoch 100, train loss: 0.11235576868057251, train acc: 0.96040763226366, val loss: 0.051297083497047424, val acc: 0.9823321554770318\n",
      "Epoch 150, train loss: 0.11235576868057251, train acc: 0.96040763226366, val loss: 0.051297083497047424, val acc: 0.9823321554770318\n",
      "Epoch 200, train loss: 0.11235576868057251, train acc: 0.96040763226366, val loss: 0.051297083497047424, val acc: 0.9823321554770318\n",
      "Value 's_m_37_val9: 0.9823321554770318' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12572699785232544, train acc: 0.9563166171588328, val loss: 0.06171058863401413, val acc: 0.9791296625222025\n",
      "Epoch 50, train loss: 0.12572699785232544, train acc: 0.9563166171588328, val loss: 0.061271537095308304, val acc: 0.9795737122557726\n",
      "Epoch 100, train loss: 0.12572699785232544, train acc: 0.9563166171588328, val loss: 0.06126935034990311, val acc: 0.9795737122557726\n",
      "Epoch 150, train loss: 0.12572699785232544, train acc: 0.9563166171588328, val loss: 0.06126935034990311, val acc: 0.9795737122557726\n",
      "Epoch 200, train loss: 0.12572699785232544, train acc: 0.9563166171588328, val loss: 0.06126935034990311, val acc: 0.9795737122557726\n",
      "Value 's_m_40_val9: 0.9795737122557726' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1386929452419281, train acc: 0.9521211054391833, val loss: 0.07667740434408188, val acc: 0.9754464285714286\n",
      "Epoch 50, train loss: 0.1386929452419281, train acc: 0.9521211054391833, val loss: 0.07613802701234818, val acc: 0.9754464285714286\n",
      "Epoch 100, train loss: 0.1386929452419281, train acc: 0.9521211054391833, val loss: 0.0761353150010109, val acc: 0.9754464285714286\n",
      "Epoch 150, train loss: 0.1386929452419281, train acc: 0.9521211054391833, val loss: 0.0761353150010109, val acc: 0.9754464285714286\n",
      "Epoch 200, train loss: 0.1386929452419281, train acc: 0.9521211054391833, val loss: 0.0761353150010109, val acc: 0.9754464285714286\n",
      "Value 's_m_43_val9: 0.9754464285714286' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15208666026592255, train acc: 0.9479074303954602, val loss: 0.09110886603593826, val acc: 0.9712746858168761\n",
      "Epoch 50, train loss: 0.15208666026592255, train acc: 0.9479074303954602, val loss: 0.09047561138868332, val acc: 0.9712746858168761\n",
      "Epoch 100, train loss: 0.15208666026592255, train acc: 0.9479074303954602, val loss: 0.09047247469425201, val acc: 0.9712746858168761\n",
      "Epoch 150, train loss: 0.15208666026592255, train acc: 0.9479074303954602, val loss: 0.09047247469425201, val acc: 0.9712746858168761\n",
      "Epoch 200, train loss: 0.15208666026592255, train acc: 0.9479074303954602, val loss: 0.09047247469425201, val acc: 0.9712746858168761\n",
      "Value 's_m_46_val9: 0.9712746858168761' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16701331734657288, train acc: 0.9435858495622655, val loss: 0.10733882337808609, val acc: 0.967057761732852\n",
      "Epoch 50, train loss: 0.16701331734657288, train acc: 0.9435858495622655, val loss: 0.10660751163959503, val acc: 0.967057761732852\n",
      "Epoch 100, train loss: 0.16701331734657288, train acc: 0.9435858495622655, val loss: 0.10660389810800552, val acc: 0.967057761732852\n",
      "Epoch 150, train loss: 0.16701331734657288, train acc: 0.9435858495622655, val loss: 0.10660389810800552, val acc: 0.967057761732852\n",
      "Epoch 200, train loss: 0.16701331734657288, train acc: 0.9435858495622655, val loss: 0.10660389810800552, val acc: 0.967057761732852\n",
      "Value 's_m_49_val9: 0.967057761732852' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18324345350265503, train acc: 0.9393789378937893, val loss: 0.125760018825531, val acc: 0.9627949183303085\n",
      "Epoch 50, train loss: 0.18324345350265503, train acc: 0.9393789378937893, val loss: 0.12493143230676651, val acc: 0.9627949183303085\n",
      "Epoch 100, train loss: 0.18324345350265503, train acc: 0.9393789378937893, val loss: 0.12492728978395462, val acc: 0.9627949183303085\n",
      "Epoch 150, train loss: 0.18324345350265503, train acc: 0.9393789378937893, val loss: 0.12492728978395462, val acc: 0.9627949183303085\n",
      "Epoch 200, train loss: 0.18324345350265503, train acc: 0.9393789378937893, val loss: 0.12492728978395462, val acc: 0.9627949183303085\n",
      "Value 's_m_52_val9: 0.9627949183303085' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09129972755908966, train acc: 0.9805021367521367, val loss: 0.1086450070142746, val acc: 0.9780564263322884\n",
      "Epoch 50, train loss: 0.09129972755908966, train acc: 0.9805021367521367, val loss: 0.09444192796945572, val acc: 0.9835423197492164\n",
      "Epoch 100, train loss: 0.09129972755908966, train acc: 0.9805021367521367, val loss: 0.09438464790582657, val acc: 0.9835423197492164\n",
      "Epoch 150, train loss: 0.09129972755908966, train acc: 0.9805021367521367, val loss: 0.09438440948724747, val acc: 0.9835423197492164\n",
      "Epoch 200, train loss: 0.09129972755908966, train acc: 0.9805021367521367, val loss: 0.09438440948724747, val acc: 0.9835423197492164\n",
      "Value 's_m_1_val10: 0.9835423197492164' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07847920805215836, train acc: 0.9834869431643625, val loss: 0.07393964380025864, val acc: 0.9857594936708861\n",
      "Epoch 50, train loss: 0.07847920805215836, train acc: 0.9834869431643625, val loss: 0.07461126893758774, val acc: 0.9849683544303798\n",
      "Epoch 100, train loss: 0.07847920805215836, train acc: 0.9834869431643625, val loss: 0.07461467385292053, val acc: 0.9849683544303798\n",
      "Epoch 150, train loss: 0.07847920805215836, train acc: 0.9834869431643625, val loss: 0.07461468130350113, val acc: 0.9849683544303798\n",
      "Epoch 200, train loss: 0.07847920805215836, train acc: 0.9834869431643625, val loss: 0.07461468130350113, val acc: 0.9849683544303798\n",
      "Value 's_m_4_val10: 0.9857594936708861' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06759773939847946, train acc: 0.9862012987012987, val loss: 0.05626413971185684, val acc: 0.987220447284345\n",
      "Epoch 50, train loss: 0.06759773939847946, train acc: 0.9862012987012987, val loss: 0.05667560175061226, val acc: 0.987220447284345\n",
      "Epoch 100, train loss: 0.06759773939847946, train acc: 0.9862012987012987, val loss: 0.05667765811085701, val acc: 0.987220447284345\n",
      "Epoch 150, train loss: 0.06759773939847946, train acc: 0.9862012987012987, val loss: 0.05667765811085701, val acc: 0.987220447284345\n",
      "Epoch 200, train loss: 0.06759773939847946, train acc: 0.9862012987012987, val loss: 0.05667765811085701, val acc: 0.987220447284345\n",
      "Value 's_m_7_val10: 0.987220447284345' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05980425328016281, train acc: 0.9877840024898848, val loss: 0.04952492564916611, val acc: 0.989516129032258\n",
      "Epoch 50, train loss: 0.05980425328016281, train acc: 0.9877840024898848, val loss: 0.04968639463186264, val acc: 0.989516129032258\n",
      "Epoch 100, train loss: 0.05980425328016281, train acc: 0.9877840024898848, val loss: 0.04968719929456711, val acc: 0.989516129032258\n",
      "Epoch 150, train loss: 0.05980425328016281, train acc: 0.9877840024898848, val loss: 0.04968719929456711, val acc: 0.989516129032258\n",
      "Epoch 200, train loss: 0.05980425328016281, train acc: 0.9877840024898848, val loss: 0.04968719929456711, val acc: 0.989516129032258\n",
      "Value 's_m_10_val10: 0.989516129032258' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05472688004374504, train acc: 0.9882518796992481, val loss: 0.053193774074316025, val acc: 0.9869706840390879\n",
      "Epoch 50, train loss: 0.05472688004374504, train acc: 0.9882518796992481, val loss: 0.053187042474746704, val acc: 0.9869706840390879\n",
      "Epoch 100, train loss: 0.05472688004374504, train acc: 0.9882518796992481, val loss: 0.05318703129887581, val acc: 0.9869706840390879\n",
      "Epoch 150, train loss: 0.05472688004374504, train acc: 0.9882518796992481, val loss: 0.05318703129887581, val acc: 0.9869706840390879\n",
      "Epoch 200, train loss: 0.05472688004374504, train acc: 0.9882518796992481, val loss: 0.05318703129887581, val acc: 0.9869706840390879\n",
      "Value 's_m_13_val10: 0.9869706840390879' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05274272337555885, train acc: 0.9873462630085147, val loss: 0.062001027166843414, val acc: 0.9794407894736842\n",
      "Epoch 50, train loss: 0.05274272337555885, train acc: 0.9873462630085147, val loss: 0.0618099682033062, val acc: 0.9794407894736842\n",
      "Epoch 100, train loss: 0.05274272337555885, train acc: 0.9873462630085147, val loss: 0.06180896982550621, val acc: 0.9794407894736842\n",
      "Epoch 150, train loss: 0.05274272337555885, train acc: 0.9873462630085147, val loss: 0.06180896982550621, val acc: 0.9794407894736842\n",
      "Epoch 200, train loss: 0.05274272337555885, train acc: 0.9873462630085147, val loss: 0.06180896982550621, val acc: 0.9794407894736842\n",
      "Value 's_m_16_val10: 0.9794407894736842' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05379878357052803, train acc: 0.9854365079365079, val loss: 0.0722021758556366, val acc: 0.9717607973421927\n",
      "Epoch 50, train loss: 0.05379878357052803, train acc: 0.9854365079365079, val loss: 0.07179360091686249, val acc: 0.9717607973421927\n",
      "Epoch 100, train loss: 0.05379878357052803, train acc: 0.9854365079365079, val loss: 0.07179155200719833, val acc: 0.9717607973421927\n",
      "Epoch 150, train loss: 0.05379878357052803, train acc: 0.9854365079365079, val loss: 0.07179155200719833, val acc: 0.9717607973421927\n",
      "Epoch 200, train loss: 0.05379878357052803, train acc: 0.9854365079365079, val loss: 0.07179155200719833, val acc: 0.9717607973421927\n",
      "Value 's_m_19_val10: 0.9717607973421927' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.057356178760528564, train acc: 0.9821828060083093, val loss: 0.08786322176456451, val acc: 0.9639261744966443\n",
      "Epoch 50, train loss: 0.057356178760528564, train acc: 0.9821828060083093, val loss: 0.08723820745944977, val acc: 0.9639261744966443\n",
      "Epoch 100, train loss: 0.057356178760528564, train acc: 0.9821828060083093, val loss: 0.08723509311676025, val acc: 0.9639261744966443\n",
      "Epoch 150, train loss: 0.057356178760528564, train acc: 0.9821828060083093, val loss: 0.08723509311676025, val acc: 0.9639261744966443\n",
      "Epoch 200, train loss: 0.057356178760528564, train acc: 0.9821828060083093, val loss: 0.08723509311676025, val acc: 0.9639261744966443\n",
      "Value 's_m_22_val10: 0.9639261744966443' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0630548968911171, train acc: 0.9787242599742599, val loss: 0.1088651493191719, val acc: 0.9559322033898305\n",
      "Epoch 50, train loss: 0.0630548968911171, train acc: 0.9787242599742599, val loss: 0.10796751827001572, val acc: 0.9559322033898305\n",
      "Epoch 100, train loss: 0.0630548968911171, train acc: 0.9787242599742599, val loss: 0.10796303302049637, val acc: 0.9559322033898305\n",
      "Epoch 150, train loss: 0.0630548968911171, train acc: 0.9787242599742599, val loss: 0.10796303302049637, val acc: 0.9559322033898305\n",
      "Epoch 200, train loss: 0.0630548968911171, train acc: 0.9787242599742599, val loss: 0.10796303302049637, val acc: 0.9559322033898305\n",
      "Value 's_m_25_val10: 0.9559322033898305' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07069478183984756, train acc: 0.9750566893424036, val loss: 0.14110067486763, val acc: 0.9477739726027398\n",
      "Epoch 50, train loss: 0.07069478183984756, train acc: 0.9750566893424036, val loss: 0.1399611532688141, val acc: 0.9486301369863014\n",
      "Epoch 100, train loss: 0.07069478183984756, train acc: 0.9750566893424036, val loss: 0.1399555206298828, val acc: 0.9486301369863014\n",
      "Epoch 150, train loss: 0.07069478183984756, train acc: 0.9750566893424036, val loss: 0.1399555206298828, val acc: 0.9486301369863014\n",
      "Epoch 200, train loss: 0.07069478183984756, train acc: 0.9750566893424036, val loss: 0.1399555206298828, val acc: 0.9486301369863014\n",
      "Value 's_m_28_val10: 0.9486301369863014' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07951901853084564, train acc: 0.9712573385518591, val loss: 0.17716346681118011, val acc: 0.9403114186851211\n",
      "Epoch 50, train loss: 0.07951901853084564, train acc: 0.9712573385518591, val loss: 0.17576314508914948, val acc: 0.9403114186851211\n",
      "Epoch 100, train loss: 0.07951901853084564, train acc: 0.9712573385518591, val loss: 0.17575623095035553, val acc: 0.9403114186851211\n",
      "Epoch 150, train loss: 0.07951901853084564, train acc: 0.9712573385518591, val loss: 0.17575623095035553, val acc: 0.9403114186851211\n",
      "Epoch 200, train loss: 0.07951901853084564, train acc: 0.9712573385518591, val loss: 0.17575623095035553, val acc: 0.9403114186851211\n",
      "Value 's_m_31_val10: 0.9403114186851211' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08821995556354523, train acc: 0.9676929392446634, val loss: 0.21518461406230927, val acc: 0.9318181818181818\n",
      "Epoch 50, train loss: 0.08821995556354523, train acc: 0.9676929392446634, val loss: 0.21355417370796204, val acc: 0.9326923076923077\n",
      "Epoch 100, train loss: 0.08821995556354523, train acc: 0.9676929392446634, val loss: 0.21354617178440094, val acc: 0.9326923076923077\n",
      "Epoch 150, train loss: 0.08821995556354523, train acc: 0.9676929392446634, val loss: 0.21354617178440094, val acc: 0.9326923076923077\n",
      "Epoch 200, train loss: 0.08821995556354523, train acc: 0.9676929392446634, val loss: 0.21354617178440094, val acc: 0.9326923076923077\n",
      "Value 's_m_34_val10: 0.9326923076923077' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09847190231084824, train acc: 0.9636243386243386, val loss: 0.2524237334728241, val acc: 0.9240282685512368\n",
      "Epoch 50, train loss: 0.09847190231084824, train acc: 0.9636243386243386, val loss: 0.2505551874637604, val acc: 0.9240282685512368\n",
      "Epoch 100, train loss: 0.09847190231084824, train acc: 0.9636243386243386, val loss: 0.25054606795310974, val acc: 0.9240282685512368\n",
      "Epoch 150, train loss: 0.09847190231084824, train acc: 0.9636243386243386, val loss: 0.25054606795310974, val acc: 0.9240282685512368\n",
      "Epoch 200, train loss: 0.09847190231084824, train acc: 0.9636243386243386, val loss: 0.25054606795310974, val acc: 0.9240282685512368\n",
      "Value 's_m_37_val10: 0.9240282685512368' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11040700972080231, train acc: 0.9593323343323343, val loss: 0.2886676490306854, val acc: 0.9151785714285714\n",
      "Epoch 50, train loss: 0.11040700972080231, train acc: 0.9593323343323343, val loss: 0.2865123152732849, val acc: 0.9151785714285714\n",
      "Epoch 100, train loss: 0.11040700972080231, train acc: 0.9593323343323343, val loss: 0.2865017354488373, val acc: 0.9151785714285714\n",
      "Epoch 150, train loss: 0.11040700972080231, train acc: 0.9593323343323343, val loss: 0.2865017354488373, val acc: 0.9151785714285714\n",
      "Epoch 200, train loss: 0.11040700972080231, train acc: 0.9593323343323343, val loss: 0.2865017354488373, val acc: 0.9151785714285714\n",
      "Value 's_m_40_val10: 0.9151785714285714' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12302407622337341, train acc: 0.9551475519785378, val loss: 0.3082927465438843, val acc: 0.9088447653429603\n",
      "Epoch 50, train loss: 0.12302407622337341, train acc: 0.9551475519785378, val loss: 0.30588340759277344, val acc: 0.9097472924187726\n",
      "Epoch 100, train loss: 0.12302407622337341, train acc: 0.9551475519785378, val loss: 0.3058716058731079, val acc: 0.9097472924187726\n",
      "Epoch 150, train loss: 0.12302407622337341, train acc: 0.9551475519785378, val loss: 0.3058716058731079, val acc: 0.9097472924187726\n",
      "Epoch 200, train loss: 0.12302407622337341, train acc: 0.9551475519785378, val loss: 0.3058716058731079, val acc: 0.9097472924187726\n",
      "Value 's_m_43_val10: 0.9097472924187726' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13594509661197662, train acc: 0.9511144883485309, val loss: 0.3282390534877777, val acc: 0.9032846715328468\n",
      "Epoch 50, train loss: 0.13594509661197662, train acc: 0.9511144883485309, val loss: 0.32559508085250854, val acc: 0.9032846715328468\n",
      "Epoch 100, train loss: 0.13594509661197662, train acc: 0.9511144883485309, val loss: 0.32558220624923706, val acc: 0.9032846715328468\n",
      "Epoch 150, train loss: 0.13594509661197662, train acc: 0.9511144883485309, val loss: 0.32558220624923706, val acc: 0.9032846715328468\n",
      "Epoch 200, train loss: 0.13594509661197662, train acc: 0.9511144883485309, val loss: 0.32558220624923706, val acc: 0.9032846715328468\n",
      "Value 's_m_46_val10: 0.9032846715328468' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15058507025241852, train acc: 0.9465561224489796, val loss: 0.345534086227417, val acc: 0.8966789667896679\n",
      "Epoch 50, train loss: 0.15058507025241852, train acc: 0.9465561224489796, val loss: 0.3426087498664856, val acc: 0.8976014760147601\n",
      "Epoch 100, train loss: 0.15058507025241852, train acc: 0.9465561224489796, val loss: 0.3425944447517395, val acc: 0.8976014760147601\n",
      "Epoch 150, train loss: 0.15058507025241852, train acc: 0.9465561224489796, val loss: 0.3425944447517395, val acc: 0.8976014760147601\n",
      "Epoch 200, train loss: 0.15058507025241852, train acc: 0.9465561224489796, val loss: 0.3425944447517395, val acc: 0.8976014760147601\n",
      "Value 's_m_49_val10: 0.8976014760147601' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1666116714477539, train acc: 0.9421462829736211, val loss: 0.36332300305366516, val acc: 0.8908582089552238\n",
      "Epoch 50, train loss: 0.1666116714477539, train acc: 0.9421462829736211, val loss: 0.36014029383659363, val acc: 0.8908582089552238\n",
      "Epoch 100, train loss: 0.1666116714477539, train acc: 0.9421462829736211, val loss: 0.360124796628952, val acc: 0.8908582089552238\n",
      "Epoch 150, train loss: 0.1666116714477539, train acc: 0.9421462829736211, val loss: 0.36012476682662964, val acc: 0.8908582089552238\n",
      "Epoch 200, train loss: 0.1666116714477539, train acc: 0.9421462829736211, val loss: 0.36012476682662964, val acc: 0.8908582089552238\n",
      "Value 's_m_52_val10: 0.8908582089552238' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08884025365114212, train acc: 0.9810395427865307, val loss: 0.15384043753147125, val acc: 0.9741813602015114\n",
      "Epoch 50, train loss: 0.08884025365114212, train acc: 0.9810395427865307, val loss: 0.12685123085975647, val acc: 0.97544080604534\n",
      "Epoch 100, train loss: 0.08884025365114212, train acc: 0.9810395427865307, val loss: 0.12673009932041168, val acc: 0.97544080604534\n",
      "Epoch 150, train loss: 0.08884025365114212, train acc: 0.9810395427865307, val loss: 0.12672974169254303, val acc: 0.97544080604534\n",
      "Epoch 200, train loss: 0.08884025365114212, train acc: 0.9810395427865307, val loss: 0.12672974169254303, val acc: 0.97544080604534\n",
      "Value 's_m_1_val11: 0.97544080604534' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07612664252519608, train acc: 0.9839474502487562, val loss: 0.10703670978546143, val acc: 0.9809644670050761\n",
      "Epoch 50, train loss: 0.07612664252519608, train acc: 0.9839474502487562, val loss: 0.10830920934677124, val acc: 0.9803299492385786\n",
      "Epoch 100, train loss: 0.07612664252519608, train acc: 0.9839474502487562, val loss: 0.10831570625305176, val acc: 0.9803299492385786\n",
      "Epoch 150, train loss: 0.07612664252519608, train acc: 0.9839474502487562, val loss: 0.10831571370363235, val acc: 0.9803299492385786\n",
      "Epoch 200, train loss: 0.07612664252519608, train acc: 0.9839474502487562, val loss: 0.10831571370363235, val acc: 0.9803299492385786\n",
      "Value 's_m_4_val11: 0.9809644670050761' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06530127674341202, train acc: 0.9864632237871674, val loss: 0.09159982949495316, val acc: 0.9833759590792839\n",
      "Epoch 50, train loss: 0.06530127674341202, train acc: 0.9864632237871674, val loss: 0.09256839007139206, val acc: 0.9833759590792839\n",
      "Epoch 100, train loss: 0.06530127674341202, train acc: 0.9864632237871674, val loss: 0.09257324039936066, val acc: 0.9833759590792839\n",
      "Epoch 150, train loss: 0.06530127674341202, train acc: 0.9864632237871674, val loss: 0.09257324784994125, val acc: 0.9833759590792839\n",
      "Epoch 200, train loss: 0.06530127674341202, train acc: 0.9864632237871674, val loss: 0.09257324784994125, val acc: 0.9833759590792839\n",
      "Value 's_m_7_val11: 0.9833759590792839' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05803661793470383, train acc: 0.9879095778197857, val loss: 0.07765783369541168, val acc: 0.9851804123711341\n",
      "Epoch 50, train loss: 0.05803661793470383, train acc: 0.9879095778197857, val loss: 0.07846885919570923, val acc: 0.9851804123711341\n",
      "Epoch 100, train loss: 0.05803661793470383, train acc: 0.9879095778197857, val loss: 0.07847291231155396, val acc: 0.9851804123711341\n",
      "Epoch 150, train loss: 0.05803661793470383, train acc: 0.9879095778197857, val loss: 0.07847291231155396, val acc: 0.9851804123711341\n",
      "Epoch 200, train loss: 0.05803661793470383, train acc: 0.9879095778197857, val loss: 0.07847291231155396, val acc: 0.9851804123711341\n",
      "Value 's_m_10_val11: 0.9851804123711341' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05398736149072647, train acc: 0.9881462099587695, val loss: 0.06408096104860306, val acc: 0.987012987012987\n",
      "Epoch 50, train loss: 0.05398736149072647, train acc: 0.9881462099587695, val loss: 0.06469783931970596, val acc: 0.987012987012987\n",
      "Epoch 100, train loss: 0.05398736149072647, train acc: 0.9881462099587695, val loss: 0.06470092386007309, val acc: 0.987012987012987\n",
      "Epoch 150, train loss: 0.05398736149072647, train acc: 0.9881462099587695, val loss: 0.06470092386007309, val acc: 0.987012987012987\n",
      "Epoch 200, train loss: 0.05398736149072647, train acc: 0.9881462099587695, val loss: 0.06470092386007309, val acc: 0.987012987012987\n",
      "Value 's_m_13_val11: 0.987012987012987' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05326930060982704, train acc: 0.9868694125159643, val loss: 0.05094519257545471, val acc: 0.9888743455497382\n",
      "Epoch 50, train loss: 0.05326930060982704, train acc: 0.9868694125159643, val loss: 0.0514177605509758, val acc: 0.9888743455497382\n",
      "Epoch 100, train loss: 0.05326930060982704, train acc: 0.9868694125159643, val loss: 0.05142010748386383, val acc: 0.9888743455497382\n",
      "Epoch 150, train loss: 0.05326930060982704, train acc: 0.9868694125159643, val loss: 0.05142010748386383, val acc: 0.9888743455497382\n",
      "Epoch 200, train loss: 0.05326930060982704, train acc: 0.9868694125159643, val loss: 0.05142010748386383, val acc: 0.9888743455497382\n",
      "Value 's_m_16_val11: 0.9888743455497382' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05509713292121887, train acc: 0.98424943747991, val loss: 0.04752920940518379, val acc: 0.9907651715039578\n",
      "Epoch 50, train loss: 0.05509713292121887, train acc: 0.98424943747991, val loss: 0.04784151166677475, val acc: 0.9907651715039578\n",
      "Epoch 100, train loss: 0.05509713292121887, train acc: 0.98424943747991, val loss: 0.04784306883811951, val acc: 0.9907651715039578\n",
      "Epoch 150, train loss: 0.05509713292121887, train acc: 0.98424943747991, val loss: 0.04784306883811951, val acc: 0.9907651715039578\n",
      "Epoch 200, train loss: 0.05509713292121887, train acc: 0.98424943747991, val loss: 0.04784306883811951, val acc: 0.9907651715039578\n",
      "Value 's_m_19_val11: 0.9907651715039578' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05923677235841751, train acc: 0.9810679611650486, val loss: 0.05204896256327629, val acc: 0.9847074468085106\n",
      "Epoch 50, train loss: 0.05923677235841751, train acc: 0.9810679611650486, val loss: 0.05223388224840164, val acc: 0.9847074468085106\n",
      "Epoch 100, train loss: 0.05923677235841751, train acc: 0.9810679611650486, val loss: 0.05223480612039566, val acc: 0.9847074468085106\n",
      "Epoch 150, train loss: 0.05923677235841751, train acc: 0.9810679611650486, val loss: 0.05223480612039566, val acc: 0.9847074468085106\n",
      "Epoch 200, train loss: 0.05923677235841751, train acc: 0.9810679611650486, val loss: 0.05223480612039566, val acc: 0.9847074468085106\n",
      "Value 's_m_22_val11: 0.9847074468085106' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06572934985160828, train acc: 0.9774356467904854, val loss: 0.05774698406457901, val acc: 0.9785522788203753\n",
      "Epoch 50, train loss: 0.06572934985160828, train acc: 0.9774356467904854, val loss: 0.05778168886899948, val acc: 0.9785522788203753\n",
      "Epoch 100, train loss: 0.06572934985160828, train acc: 0.9774356467904854, val loss: 0.057781882584095, val acc: 0.9785522788203753\n",
      "Epoch 150, train loss: 0.06572934985160828, train acc: 0.9774356467904854, val loss: 0.057781882584095, val acc: 0.9785522788203753\n",
      "Epoch 200, train loss: 0.06572934985160828, train acc: 0.9774356467904854, val loss: 0.057781882584095, val acc: 0.9785522788203753\n",
      "Value 's_m_25_val11: 0.9785522788203753' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07459843158721924, train acc: 0.9739173228346457, val loss: 0.06542377173900604, val acc: 0.9722972972972973\n",
      "Epoch 50, train loss: 0.07459843158721924, train acc: 0.9739173228346457, val loss: 0.06530957669019699, val acc: 0.9722972972972973\n",
      "Epoch 100, train loss: 0.07459843158721924, train acc: 0.9739173228346457, val loss: 0.06530900299549103, val acc: 0.9722972972972973\n",
      "Epoch 150, train loss: 0.07459843158721924, train acc: 0.9739173228346457, val loss: 0.06530900299549103, val acc: 0.9722972972972973\n",
      "Epoch 200, train loss: 0.07459843158721924, train acc: 0.9739173228346457, val loss: 0.06530900299549103, val acc: 0.9722972972972973\n",
      "Value 's_m_28_val11: 0.9722972972972973' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08472368121147156, train acc: 0.9700611166171127, val loss: 0.07503314316272736, val acc: 0.9659400544959128\n",
      "Epoch 50, train loss: 0.08472368121147156, train acc: 0.9700611166171127, val loss: 0.07476142793893814, val acc: 0.9659400544959128\n",
      "Epoch 100, train loss: 0.08472368121147156, train acc: 0.9700611166171127, val loss: 0.0747600793838501, val acc: 0.9659400544959128\n",
      "Epoch 150, train loss: 0.08472368121147156, train acc: 0.9700611166171127, val loss: 0.0747600793838501, val acc: 0.9659400544959128\n",
      "Epoch 200, train loss: 0.08472368121147156, train acc: 0.9700611166171127, val loss: 0.0747600793838501, val acc: 0.9659400544959128\n",
      "Value 's_m_31_val11: 0.9659400544959128' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09463890641927719, train acc: 0.9664421157684631, val loss: 0.08761215209960938, val acc: 0.959478021978022\n",
      "Epoch 50, train loss: 0.09463890641927719, train acc: 0.9664421157684631, val loss: 0.08716841787099838, val acc: 0.959478021978022\n",
      "Epoch 100, train loss: 0.09463890641927719, train acc: 0.9664421157684631, val loss: 0.08716624230146408, val acc: 0.959478021978022\n",
      "Epoch 150, train loss: 0.09463890641927719, train acc: 0.9664421157684631, val loss: 0.08716624230146408, val acc: 0.959478021978022\n",
      "Epoch 200, train loss: 0.09463890641927719, train acc: 0.9664421157684631, val loss: 0.08716624230146408, val acc: 0.959478021978022\n",
      "Value 's_m_34_val11: 0.959478021978022' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10603276640176773, train acc: 0.962395309882747, val loss: 0.10092397779226303, val acc: 0.9529085872576177\n",
      "Epoch 50, train loss: 0.10603276640176773, train acc: 0.962395309882747, val loss: 0.10030706226825714, val acc: 0.9529085872576177\n",
      "Epoch 100, train loss: 0.10603276640176773, train acc: 0.962395309882747, val loss: 0.1003040000796318, val acc: 0.9529085872576177\n",
      "Epoch 150, train loss: 0.10603276640176773, train acc: 0.962395309882747, val loss: 0.1003040000796318, val acc: 0.9529085872576177\n",
      "Epoch 200, train loss: 0.10603276640176773, train acc: 0.962395309882747, val loss: 0.1003040000796318, val acc: 0.9529085872576177\n",
      "Value 's_m_37_val11: 0.9529085872576177' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11913075298070908, train acc: 0.9579537786774629, val loss: 0.11326774954795837, val acc: 0.9462290502793296\n",
      "Epoch 50, train loss: 0.11913075298070908, train acc: 0.9579537786774629, val loss: 0.11244315654039383, val acc: 0.9462290502793296\n",
      "Epoch 100, train loss: 0.11913075298070908, train acc: 0.9579537786774629, val loss: 0.1124390959739685, val acc: 0.9462290502793296\n",
      "Epoch 150, train loss: 0.11913075298070908, train acc: 0.9579537786774629, val loss: 0.1124390959739685, val acc: 0.9462290502793296\n",
      "Epoch 200, train loss: 0.11913075298070908, train acc: 0.9579537786774629, val loss: 0.1124390959739685, val acc: 0.9462290502793296\n",
      "Value 's_m_40_val11: 0.9462290502793296' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1320245862007141, train acc: 0.9537461773700305, val loss: 0.1275009661912918, val acc: 0.9394366197183098\n",
      "Epoch 50, train loss: 0.1320245862007141, train acc: 0.9537461773700305, val loss: 0.12643155455589294, val acc: 0.9394366197183098\n",
      "Epoch 100, train loss: 0.1320245862007141, train acc: 0.9537461773700305, val loss: 0.12642624974250793, val acc: 0.9394366197183098\n",
      "Epoch 150, train loss: 0.1320245862007141, train acc: 0.9537461773700305, val loss: 0.12642624974250793, val acc: 0.9394366197183098\n",
      "Epoch 200, train loss: 0.1320245862007141, train acc: 0.9537461773700305, val loss: 0.12642624974250793, val acc: 0.9394366197183098\n",
      "Value 's_m_43_val11: 0.9394366197183098' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1450059562921524, train acc: 0.9497347707049966, val loss: 0.1455390304327011, val acc: 0.9325284090909091\n",
      "Epoch 50, train loss: 0.1450059562921524, train acc: 0.9497347707049966, val loss: 0.1442626565694809, val acc: 0.9332386363636364\n",
      "Epoch 100, train loss: 0.1450059562921524, train acc: 0.9497347707049966, val loss: 0.1442563682794571, val acc: 0.9332386363636364\n",
      "Epoch 150, train loss: 0.1450059562921524, train acc: 0.9497347707049966, val loss: 0.1442563682794571, val acc: 0.9332386363636364\n",
      "Epoch 200, train loss: 0.1450059562921524, train acc: 0.9497347707049966, val loss: 0.1442563682794571, val acc: 0.9332386363636364\n",
      "Value 's_m_46_val11: 0.9332386363636364' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15942327678203583, train acc: 0.9454067562909342, val loss: 0.166585773229599, val acc: 0.9262177650429799\n",
      "Epoch 50, train loss: 0.15942327678203583, train acc: 0.9454067562909342, val loss: 0.1650894731283188, val acc: 0.9262177650429799\n",
      "Epoch 100, train loss: 0.15942327678203583, train acc: 0.9454067562909342, val loss: 0.16508206725120544, val acc: 0.9262177650429799\n",
      "Epoch 150, train loss: 0.15942327678203583, train acc: 0.9454067562909342, val loss: 0.16508206725120544, val acc: 0.9262177650429799\n",
      "Epoch 200, train loss: 0.15942327678203583, train acc: 0.9454067562909342, val loss: 0.16508206725120544, val acc: 0.9262177650429799\n",
      "Value 's_m_49_val11: 0.9262177650429799' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17512327432632446, train acc: 0.9408854166666667, val loss: 0.19003181159496307, val acc: 0.9190751445086706\n",
      "Epoch 50, train loss: 0.17512327432632446, train acc: 0.9408854166666667, val loss: 0.18833044171333313, val acc: 0.9190751445086706\n",
      "Epoch 100, train loss: 0.17512327432632446, train acc: 0.9408854166666667, val loss: 0.1883220374584198, val acc: 0.9190751445086706\n",
      "Epoch 150, train loss: 0.17512327432632446, train acc: 0.9408854166666667, val loss: 0.1883220374584198, val acc: 0.9190751445086706\n",
      "Epoch 200, train loss: 0.17512327432632446, train acc: 0.9408854166666667, val loss: 0.1883220374584198, val acc: 0.9190751445086706\n",
      "Value 's_m_52_val11: 0.9190751445086706' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09321587532758713, train acc: 0.9801441275682305, val loss: 0.08150076866149902, val acc: 0.9806590257879656\n",
      "Epoch 50, train loss: 0.09321587532758713, train acc: 0.9801441275682305, val loss: 0.06532344967126846, val acc: 0.9856733524355301\n",
      "Epoch 100, train loss: 0.09321587532758713, train acc: 0.9801441275682305, val loss: 0.06525470316410065, val acc: 0.9856733524355301\n",
      "Epoch 150, train loss: 0.09321587532758713, train acc: 0.9801441275682305, val loss: 0.06525454670190811, val acc: 0.9856733524355301\n",
      "Epoch 200, train loss: 0.09321587532758713, train acc: 0.9801441275682305, val loss: 0.06525454670190811, val acc: 0.9856733524355301\n",
      "Value 's_m_1_val12: 0.9856733524355301' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07981365919113159, train acc: 0.9831018518518518, val loss: 0.05410029739141464, val acc: 0.9877167630057804\n",
      "Epoch 50, train loss: 0.07981365919113159, train acc: 0.9831018518518518, val loss: 0.054728735238313675, val acc: 0.9869942196531792\n",
      "Epoch 100, train loss: 0.07981365919113159, train acc: 0.9831018518518518, val loss: 0.05473191291093826, val acc: 0.9869942196531792\n",
      "Epoch 150, train loss: 0.07981365919113159, train acc: 0.9831018518518518, val loss: 0.05473191291093826, val acc: 0.9869942196531792\n",
      "Epoch 200, train loss: 0.07981365919113159, train acc: 0.9831018518518518, val loss: 0.05473191291093826, val acc: 0.9869942196531792\n",
      "Value 's_m_4_val12: 0.9877167630057804' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06851587444543839, train acc: 0.9859428393911153, val loss: 0.04243332892656326, val acc: 0.989067055393586\n",
      "Epoch 50, train loss: 0.06851587444543839, train acc: 0.9859428393911153, val loss: 0.0427982397377491, val acc: 0.989067055393586\n",
      "Epoch 100, train loss: 0.06851587444543839, train acc: 0.9859428393911153, val loss: 0.04280007630586624, val acc: 0.989067055393586\n",
      "Epoch 150, train loss: 0.06851587444543839, train acc: 0.9859428393911153, val loss: 0.04280007630586624, val acc: 0.989067055393586\n",
      "Epoch 200, train loss: 0.06851587444543839, train acc: 0.9859428393911153, val loss: 0.04280007630586624, val acc: 0.989067055393586\n",
      "Value 's_m_7_val12: 0.989067055393586' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06010110676288605, train acc: 0.9877267041901189, val loss: 0.045507874339818954, val acc: 0.986764705882353\n",
      "Epoch 50, train loss: 0.06010110676288605, train acc: 0.9877267041901189, val loss: 0.045723240822553635, val acc: 0.986764705882353\n",
      "Epoch 100, train loss: 0.06010110676288605, train acc: 0.9877267041901189, val loss: 0.04572436958551407, val acc: 0.986764705882353\n",
      "Epoch 150, train loss: 0.06010110676288605, train acc: 0.9877267041901189, val loss: 0.04572436958551407, val acc: 0.986764705882353\n",
      "Epoch 200, train loss: 0.06010110676288605, train acc: 0.9877267041901189, val loss: 0.04572436958551407, val acc: 0.986764705882353\n",
      "Value 's_m_10_val12: 0.986764705882353' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05491442233324051, train acc: 0.9882357570034623, val loss: 0.04869776964187622, val acc: 0.9859050445103857\n",
      "Epoch 50, train loss: 0.05491442233324051, train acc: 0.9882357570034623, val loss: 0.048758428543806076, val acc: 0.9859050445103857\n",
      "Epoch 100, train loss: 0.05491442233324051, train acc: 0.9882357570034623, val loss: 0.048758793622255325, val acc: 0.9859050445103857\n",
      "Epoch 150, train loss: 0.05491442233324051, train acc: 0.9882357570034623, val loss: 0.048758793622255325, val acc: 0.9859050445103857\n",
      "Epoch 200, train loss: 0.05491442233324051, train acc: 0.9882357570034623, val loss: 0.048758793622255325, val acc: 0.9859050445103857\n",
      "Value 's_m_13_val12: 0.9859050445103857' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.052772488445043564, train acc: 0.9872465145754119, val loss: 0.05717778950929642, val acc: 0.9805389221556886\n",
      "Epoch 50, train loss: 0.052772488445043564, train acc: 0.9872465145754119, val loss: 0.05707325041294098, val acc: 0.9812874251497006\n",
      "Epoch 100, train loss: 0.052772488445043564, train acc: 0.9872465145754119, val loss: 0.05707279220223427, val acc: 0.9812874251497006\n",
      "Epoch 150, train loss: 0.052772488445043564, train acc: 0.9872465145754119, val loss: 0.05707279220223427, val acc: 0.9812874251497006\n",
      "Epoch 200, train loss: 0.052772488445043564, train acc: 0.9872465145754119, val loss: 0.05707279220223427, val acc: 0.9812874251497006\n",
      "Value 's_m_16_val12: 0.9812874251497006' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05356628820300102, train acc: 0.9854864433811802, val loss: 0.06931435316801071, val acc: 0.9750755287009063\n",
      "Epoch 50, train loss: 0.05356628820300102, train acc: 0.9854864433811802, val loss: 0.06898227334022522, val acc: 0.9750755287009063\n",
      "Epoch 100, train loss: 0.05356628820300102, train acc: 0.9854864433811802, val loss: 0.06898072361946106, val acc: 0.9750755287009063\n",
      "Epoch 150, train loss: 0.05356628820300102, train acc: 0.9854864433811802, val loss: 0.06898072361946106, val acc: 0.9750755287009063\n",
      "Epoch 200, train loss: 0.05356628820300102, train acc: 0.9854864433811802, val loss: 0.06898072361946106, val acc: 0.9750755287009063\n",
      "Value 's_m_19_val12: 0.9750755287009063' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0569869801402092, train acc: 0.9821371226718048, val loss: 0.08494515717029572, val acc: 0.96875\n",
      "Epoch 50, train loss: 0.0569869801402092, train acc: 0.9821371226718048, val loss: 0.08434171229600906, val acc: 0.96875\n",
      "Epoch 100, train loss: 0.0569869801402092, train acc: 0.9821371226718048, val loss: 0.08433881402015686, val acc: 0.96875\n",
      "Epoch 150, train loss: 0.0569869801402092, train acc: 0.9821371226718048, val loss: 0.08433881402015686, val acc: 0.96875\n",
      "Epoch 200, train loss: 0.0569869801402092, train acc: 0.9821371226718048, val loss: 0.08433881402015686, val acc: 0.96875\n",
      "Value 's_m_22_val12: 0.96875' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06250418722629547, train acc: 0.9786614936954413, val loss: 0.10661745816469193, val acc: 0.9615384615384616\n",
      "Epoch 50, train loss: 0.06250418722629547, train acc: 0.9786614936954413, val loss: 0.10574162751436234, val acc: 0.9623076923076923\n",
      "Epoch 100, train loss: 0.06250418722629547, train acc: 0.9786614936954413, val loss: 0.10573739558458328, val acc: 0.9623076923076923\n",
      "Epoch 150, train loss: 0.06250418722629547, train acc: 0.9786614936954413, val loss: 0.10573739558458328, val acc: 0.9623076923076923\n",
      "Epoch 200, train loss: 0.06250418722629547, train acc: 0.9786614936954413, val loss: 0.10573739558458328, val acc: 0.9623076923076923\n",
      "Value 's_m_25_val12: 0.9623076923076923' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07029719650745392, train acc: 0.9750569661458334, val loss: 0.13245108723640442, val acc: 0.9549689440993789\n",
      "Epoch 50, train loss: 0.07029719650745392, train acc: 0.9750569661458334, val loss: 0.13129650056362152, val acc: 0.9557453416149069\n",
      "Epoch 100, train loss: 0.07029719650745392, train acc: 0.9750569661458334, val loss: 0.13129086792469025, val acc: 0.9557453416149069\n",
      "Epoch 150, train loss: 0.07029719650745392, train acc: 0.9750569661458334, val loss: 0.13129086792469025, val acc: 0.9557453416149069\n",
      "Epoch 200, train loss: 0.07029719650745392, train acc: 0.9750569661458334, val loss: 0.13129086792469025, val acc: 0.9557453416149069\n",
      "Value 's_m_28_val12: 0.9557453416149069' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0795145258307457, train acc: 0.9711569977056703, val loss: 0.15658687055110931, val acc: 0.9482758620689655\n",
      "Epoch 50, train loss: 0.0795145258307457, train acc: 0.9711569977056703, val loss: 0.15513795614242554, val acc: 0.9482758620689655\n",
      "Epoch 100, train loss: 0.0795145258307457, train acc: 0.9711569977056703, val loss: 0.15513092279434204, val acc: 0.9482758620689655\n",
      "Epoch 150, train loss: 0.0795145258307457, train acc: 0.9711569977056703, val loss: 0.15513092279434204, val acc: 0.9482758620689655\n",
      "Epoch 200, train loss: 0.0795145258307457, train acc: 0.9711569977056703, val loss: 0.15513092279434204, val acc: 0.9482758620689655\n",
      "Value 's_m_31_val12: 0.9482758620689655' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08882103115320206, train acc: 0.967450495049505, val loss: 0.1786525994539261, val acc: 0.9406645569620253\n",
      "Epoch 50, train loss: 0.08882103115320206, train acc: 0.967450495049505, val loss: 0.17696025967597961, val acc: 0.9406645569620253\n",
      "Epoch 100, train loss: 0.08882103115320206, train acc: 0.967450495049505, val loss: 0.17695201933383942, val acc: 0.9406645569620253\n",
      "Epoch 150, train loss: 0.08882103115320206, train acc: 0.967450495049505, val loss: 0.17695201933383942, val acc: 0.9406645569620253\n",
      "Epoch 200, train loss: 0.08882103115320206, train acc: 0.967450495049505, val loss: 0.17695201933383942, val acc: 0.9406645569620253\n",
      "Value 's_m_34_val12: 0.9406645569620253' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09955743700265884, train acc: 0.9634430043203722, val loss: 0.20251896977424622, val acc: 0.9329073482428115\n",
      "Epoch 50, train loss: 0.09955743700265884, train acc: 0.9634430043203722, val loss: 0.20065288245677948, val acc: 0.9329073482428115\n",
      "Epoch 100, train loss: 0.09955743700265884, train acc: 0.9634430043203722, val loss: 0.20064374804496765, val acc: 0.9329073482428115\n",
      "Epoch 150, train loss: 0.09955743700265884, train acc: 0.9634430043203722, val loss: 0.20064374804496765, val acc: 0.9329073482428115\n",
      "Epoch 200, train loss: 0.09955743700265884, train acc: 0.9634430043203722, val loss: 0.20064374804496765, val acc: 0.9329073482428115\n",
      "Value 's_m_37_val12: 0.9329073482428115' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11131906509399414, train acc: 0.9593373493975904, val loss: 0.23819796741008759, val acc: 0.925\n",
      "Epoch 50, train loss: 0.11131906509399414, train acc: 0.9593373493975904, val loss: 0.23608122766017914, val acc: 0.9274193548387096\n",
      "Epoch 100, train loss: 0.11131906509399414, train acc: 0.9593373493975904, val loss: 0.23607094585895538, val acc: 0.9274193548387096\n",
      "Epoch 150, train loss: 0.11131906509399414, train acc: 0.9593373493975904, val loss: 0.236070916056633, val acc: 0.9274193548387096\n",
      "Epoch 200, train loss: 0.11131906509399414, train acc: 0.9593373493975904, val loss: 0.236070916056633, val acc: 0.9274193548387096\n",
      "Value 's_m_40_val12: 0.9274193548387096' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12361303716897964, train acc: 0.9552578361981799, val loss: 0.26223692297935486, val acc: 0.9210097719869706\n",
      "Epoch 50, train loss: 0.12361303716897964, train acc: 0.9552578361981799, val loss: 0.25986048579216003, val acc: 0.9218241042345277\n",
      "Epoch 100, train loss: 0.12361303716897964, train acc: 0.9552578361981799, val loss: 0.25984877347946167, val acc: 0.9218241042345277\n",
      "Epoch 150, train loss: 0.12361303716897964, train acc: 0.9552578361981799, val loss: 0.25984877347946167, val acc: 0.9218241042345277\n",
      "Epoch 200, train loss: 0.12361303716897964, train acc: 0.9552578361981799, val loss: 0.25984877347946167, val acc: 0.9218241042345277\n",
      "Value 's_m_43_val12: 0.9218241042345277' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1364491581916809, train acc: 0.9511625933469111, val loss: 0.282118558883667, val acc: 0.9161184210526315\n",
      "Epoch 50, train loss: 0.1364491581916809, train acc: 0.9511625933469111, val loss: 0.27956557273864746, val acc: 0.9161184210526315\n",
      "Epoch 100, train loss: 0.1364491581916809, train acc: 0.9511625933469111, val loss: 0.27955320477485657, val acc: 0.9161184210526315\n",
      "Epoch 150, train loss: 0.1364491581916809, train acc: 0.9511625933469111, val loss: 0.2795531749725342, val acc: 0.9161184210526315\n",
      "Epoch 200, train loss: 0.1364491581916809, train acc: 0.9511625933469111, val loss: 0.2795531749725342, val acc: 0.9161184210526315\n",
      "Value 's_m_46_val12: 0.9161184210526315' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1508779376745224, train acc: 0.9467948717948718, val loss: 0.3025936186313629, val acc: 0.9102990033222591\n",
      "Epoch 50, train loss: 0.1508779376745224, train acc: 0.9467948717948718, val loss: 0.2998088300228119, val acc: 0.9102990033222591\n",
      "Epoch 100, train loss: 0.1508779376745224, train acc: 0.9467948717948718, val loss: 0.2997952699661255, val acc: 0.9102990033222591\n",
      "Epoch 150, train loss: 0.1508779376745224, train acc: 0.9467948717948718, val loss: 0.2997952699661255, val acc: 0.9102990033222591\n",
      "Epoch 200, train loss: 0.1508779376745224, train acc: 0.9467948717948718, val loss: 0.2997952699661255, val acc: 0.9102990033222591\n",
      "Value 's_m_49_val12: 0.9102990033222591' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16669632494449615, train acc: 0.9424500688705234, val loss: 0.3241124749183655, val acc: 0.9043624161073825\n",
      "Epoch 50, train loss: 0.16669632494449615, train acc: 0.9424500688705234, val loss: 0.3211205303668976, val acc: 0.9060402684563759\n",
      "Epoch 100, train loss: 0.16669632494449615, train acc: 0.9424500688705234, val loss: 0.3211059868335724, val acc: 0.9060402684563759\n",
      "Epoch 150, train loss: 0.16669632494449615, train acc: 0.9424500688705234, val loss: 0.3211059868335724, val acc: 0.9060402684563759\n",
      "Epoch 200, train loss: 0.16669632494449615, train acc: 0.9424500688705234, val loss: 0.3211059868335724, val acc: 0.9060402684563759\n",
      "Value 's_m_52_val12: 0.9060402684563759' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09021123498678207, train acc: 0.9809993783027665, val loss: 0.11474594473838806, val acc: 0.9748283752860412\n",
      "Epoch 50, train loss: 0.09021123498678207, train acc: 0.9809993783027665, val loss: 0.09486234933137894, val acc: 0.9776887871853547\n",
      "Epoch 100, train loss: 0.09021123498678207, train acc: 0.9809993783027665, val loss: 0.09478653222322464, val acc: 0.9776887871853547\n",
      "Epoch 150, train loss: 0.09021123498678207, train acc: 0.9809993783027665, val loss: 0.09478622674942017, val acc: 0.9776887871853547\n",
      "Epoch 200, train loss: 0.09021123498678207, train acc: 0.9809993783027665, val loss: 0.09478622674942017, val acc: 0.9776887871853547\n",
      "Value 's_m_1_val13: 0.9776887871853547' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07705726474523544, train acc: 0.9840816645807259, val loss: 0.08527049422264099, val acc: 0.9792626728110599\n",
      "Epoch 50, train loss: 0.07705726474523544, train acc: 0.9840816645807259, val loss: 0.08579548448324203, val acc: 0.9792626728110599\n",
      "Epoch 100, train loss: 0.07705726474523544, train acc: 0.9840816645807259, val loss: 0.08579840511083603, val acc: 0.9792626728110599\n",
      "Epoch 150, train loss: 0.07705726474523544, train acc: 0.9840816645807259, val loss: 0.08579840511083603, val acc: 0.9792626728110599\n",
      "Epoch 200, train loss: 0.07705726474523544, train acc: 0.9840816645807259, val loss: 0.08579840511083603, val acc: 0.9792626728110599\n",
      "Value 's_m_4_val13: 0.9792626728110599' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0658595934510231, train acc: 0.9866535433070867, val loss: 0.07784967869520187, val acc: 0.9808584686774942\n",
      "Epoch 50, train loss: 0.0658595934510231, train acc: 0.9866535433070867, val loss: 0.07821126282215118, val acc: 0.9808584686774942\n",
      "Epoch 100, train loss: 0.0658595934510231, train acc: 0.9866535433070867, val loss: 0.07821326702833176, val acc: 0.9808584686774942\n",
      "Epoch 150, train loss: 0.0658595934510231, train acc: 0.9866535433070867, val loss: 0.07821326702833176, val acc: 0.9808584686774942\n",
      "Epoch 200, train loss: 0.0658595934510231, train acc: 0.9866535433070867, val loss: 0.07821326702833176, val acc: 0.9808584686774942\n",
      "Value 's_m_7_val13: 0.9808584686774942' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.058162711560726166, train acc: 0.9881103360811667, val loss: 0.07240888476371765, val acc: 0.9824766355140186\n",
      "Epoch 50, train loss: 0.058162711560726166, train acc: 0.9881103360811667, val loss: 0.07277403771877289, val acc: 0.9813084112149533\n",
      "Epoch 100, train loss: 0.058162711560726166, train acc: 0.9881103360811667, val loss: 0.07277610152959824, val acc: 0.9813084112149533\n",
      "Epoch 150, train loss: 0.058162711560726166, train acc: 0.9881103360811667, val loss: 0.07277610152959824, val acc: 0.9813084112149533\n",
      "Epoch 200, train loss: 0.058162711560726166, train acc: 0.9881103360811667, val loss: 0.07277610152959824, val acc: 0.9813084112149533\n",
      "Value 's_m_10_val13: 0.9824766355140186' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.053557608276605606, train acc: 0.9882700287264603, val loss: 0.06926120817661285, val acc: 0.9829411764705882\n",
      "Epoch 50, train loss: 0.053557608276605606, train acc: 0.9882700287264603, val loss: 0.06955699622631073, val acc: 0.9829411764705882\n",
      "Epoch 100, train loss: 0.053557608276605606, train acc: 0.9882700287264603, val loss: 0.06955867260694504, val acc: 0.9829411764705882\n",
      "Epoch 150, train loss: 0.053557608276605606, train acc: 0.9882700287264603, val loss: 0.06955867260694504, val acc: 0.9829411764705882\n",
      "Epoch 200, train loss: 0.053557608276605606, train acc: 0.9882700287264603, val loss: 0.06955867260694504, val acc: 0.9829411764705882\n",
      "Value 's_m_13_val13: 0.9829411764705882' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0521022230386734, train acc: 0.9868251928020566, val loss: 0.06969227641820908, val acc: 0.9834123222748815\n",
      "Epoch 50, train loss: 0.0521022230386734, train acc: 0.9868251928020566, val loss: 0.06989746540784836, val acc: 0.9834123222748815\n",
      "Epoch 100, train loss: 0.0521022230386734, train acc: 0.9868251928020566, val loss: 0.06989879161119461, val acc: 0.9834123222748815\n",
      "Epoch 150, train loss: 0.0521022230386734, train acc: 0.9868251928020566, val loss: 0.06989879161119461, val acc: 0.9834123222748815\n",
      "Epoch 200, train loss: 0.0521022230386734, train acc: 0.9868251928020566, val loss: 0.06989879161119461, val acc: 0.9834123222748815\n",
      "Value 's_m_16_val13: 0.9834123222748815' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05348610505461693, train acc: 0.9846732449045617, val loss: 0.0747988373041153, val acc: 0.9803102625298329\n",
      "Epoch 50, train loss: 0.05348610505461693, train acc: 0.9846732449045617, val loss: 0.07492909580469131, val acc: 0.9803102625298329\n",
      "Epoch 100, train loss: 0.05348610505461693, train acc: 0.9846732449045617, val loss: 0.0749301090836525, val acc: 0.9803102625298329\n",
      "Epoch 150, train loss: 0.05348610505461693, train acc: 0.9846732449045617, val loss: 0.0749301090836525, val acc: 0.9803102625298329\n",
      "Epoch 200, train loss: 0.05348610505461693, train acc: 0.9846732449045617, val loss: 0.0749301090836525, val acc: 0.9803102625298329\n",
      "Value 's_m_19_val13: 0.9803102625298329' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05753691866993904, train acc: 0.9815553745928339, val loss: 0.08197490870952606, val acc: 0.9747596153846154\n",
      "Epoch 50, train loss: 0.05753691866993904, train acc: 0.9815553745928339, val loss: 0.08203000575304031, val acc: 0.9747596153846154\n",
      "Epoch 100, train loss: 0.05753691866993904, train acc: 0.9815553745928339, val loss: 0.08203072845935822, val acc: 0.9747596153846154\n",
      "Epoch 150, train loss: 0.05753691866993904, train acc: 0.9815553745928339, val loss: 0.08203072845935822, val acc: 0.9747596153846154\n",
      "Epoch 200, train loss: 0.05753691866993904, train acc: 0.9815553745928339, val loss: 0.08203072845935822, val acc: 0.9747596153846154\n",
      "Value 's_m_22_val13: 0.9747596153846154' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06391999125480652, train acc: 0.9780255821580847, val loss: 0.09068413078784943, val acc: 0.9691283292978208\n",
      "Epoch 50, train loss: 0.06391999125480652, train acc: 0.9780255821580847, val loss: 0.09070343524217606, val acc: 0.9691283292978208\n",
      "Epoch 100, train loss: 0.06391999125480652, train acc: 0.9780255821580847, val loss: 0.09070385992527008, val acc: 0.9691283292978208\n",
      "Epoch 150, train loss: 0.06391999125480652, train acc: 0.9780255821580847, val loss: 0.09070385992527008, val acc: 0.9691283292978208\n",
      "Epoch 200, train loss: 0.06391999125480652, train acc: 0.9780255821580847, val loss: 0.09070385992527008, val acc: 0.9691283292978208\n",
      "Value 's_m_25_val13: 0.9691283292978208' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07283950597047806, train acc: 0.9744055482166446, val loss: 0.09901202470064163, val acc: 0.9634146341463414\n",
      "Epoch 50, train loss: 0.07283950597047806, train acc: 0.9744055482166446, val loss: 0.09896091371774673, val acc: 0.9628048780487805\n",
      "Epoch 100, train loss: 0.07283950597047806, train acc: 0.9744055482166446, val loss: 0.09896092861890793, val acc: 0.9628048780487805\n",
      "Epoch 150, train loss: 0.07283950597047806, train acc: 0.9744055482166446, val loss: 0.09896092861890793, val acc: 0.9628048780487805\n",
      "Epoch 200, train loss: 0.07283950597047806, train acc: 0.9744055482166446, val loss: 0.09896092861890793, val acc: 0.9628048780487805\n",
      "Value 's_m_28_val13: 0.9634146341463414' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.083225779235363, train acc: 0.9704023944130362, val loss: 0.10588336735963821, val acc: 0.9594594594594594\n",
      "Epoch 50, train loss: 0.083225779235363, train acc: 0.9704023944130362, val loss: 0.10576353222131729, val acc: 0.9594594594594594\n",
      "Epoch 100, train loss: 0.083225779235363, train acc: 0.9704023944130362, val loss: 0.10576330870389938, val acc: 0.9594594594594594\n",
      "Epoch 150, train loss: 0.083225779235363, train acc: 0.9704023944130362, val loss: 0.10576330870389938, val acc: 0.9594594594594594\n",
      "Epoch 200, train loss: 0.083225779235363, train acc: 0.9704023944130362, val loss: 0.10576330870389938, val acc: 0.9594594594594594\n",
      "Value 's_m_31_val13: 0.9594594594594594' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.093687042593956, train acc: 0.9664266577361018, val loss: 0.11104993522167206, val acc: 0.9573019801980198\n",
      "Epoch 50, train loss: 0.093687042593956, train acc: 0.9664266577361018, val loss: 0.11080300062894821, val acc: 0.9573019801980198\n",
      "Epoch 100, train loss: 0.093687042593956, train acc: 0.9664266577361018, val loss: 0.11080198734998703, val acc: 0.9573019801980198\n",
      "Epoch 150, train loss: 0.093687042593956, train acc: 0.9664266577361018, val loss: 0.11080198734998703, val acc: 0.9573019801980198\n",
      "Epoch 200, train loss: 0.093687042593956, train acc: 0.9664266577361018, val loss: 0.11080198734998703, val acc: 0.9573019801980198\n",
      "Value 's_m_34_val13: 0.9573019801980198' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10545552521944046, train acc: 0.9622681281618887, val loss: 0.11954315751791, val acc: 0.9526184538653366\n",
      "Epoch 50, train loss: 0.10545552521944046, train acc: 0.9622681281618887, val loss: 0.11916512250900269, val acc: 0.9532418952618454\n",
      "Epoch 100, train loss: 0.10545552521944046, train acc: 0.9622681281618887, val loss: 0.11916352063417435, val acc: 0.9532418952618454\n",
      "Epoch 150, train loss: 0.10545552521944046, train acc: 0.9622681281618887, val loss: 0.11916352063417435, val acc: 0.9532418952618454\n",
      "Epoch 200, train loss: 0.10545552521944046, train acc: 0.9622681281618887, val loss: 0.11916352063417435, val acc: 0.9532418952618454\n",
      "Value 's_m_37_val13: 0.9532418952618454' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11872362345457077, train acc: 0.9578804347826086, val loss: 0.13034778833389282, val acc: 0.9472361809045227\n",
      "Epoch 50, train loss: 0.11872362345457077, train acc: 0.9578804347826086, val loss: 0.1298459768295288, val acc: 0.9472361809045227\n",
      "Epoch 100, train loss: 0.11872362345457077, train acc: 0.9578804347826086, val loss: 0.12984374165534973, val acc: 0.9472361809045227\n",
      "Epoch 150, train loss: 0.11872362345457077, train acc: 0.9578804347826086, val loss: 0.12984374165534973, val acc: 0.9472361809045227\n",
      "Epoch 200, train loss: 0.11872362345457077, train acc: 0.9578804347826086, val loss: 0.12984374165534973, val acc: 0.9472361809045227\n",
      "Value 's_m_40_val13: 0.9472361809045227' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13176856935024261, train acc: 0.9536435169346562, val loss: 0.14290034770965576, val acc: 0.9411392405063291\n",
      "Epoch 50, train loss: 0.13176856935024261, train acc: 0.9536435169346562, val loss: 0.14229588210582733, val acc: 0.9411392405063291\n",
      "Epoch 100, train loss: 0.13176856935024261, train acc: 0.9536435169346562, val loss: 0.14229318499565125, val acc: 0.9411392405063291\n",
      "Epoch 150, train loss: 0.13176856935024261, train acc: 0.9536435169346562, val loss: 0.14229318499565125, val acc: 0.9411392405063291\n",
      "Epoch 200, train loss: 0.13176856935024261, train acc: 0.9536435169346562, val loss: 0.14229318499565125, val acc: 0.9411392405063291\n",
      "Value 's_m_43_val13: 0.9411392405063291' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14518693089485168, train acc: 0.9493022053756031, val loss: 0.15465189516544342, val acc: 0.9387755102040817\n",
      "Epoch 50, train loss: 0.14518693089485168, train acc: 0.9493022053756031, val loss: 0.15392044186592102, val acc: 0.9387755102040817\n",
      "Epoch 100, train loss: 0.14518693089485168, train acc: 0.9493022053756031, val loss: 0.1539170742034912, val acc: 0.9387755102040817\n",
      "Epoch 150, train loss: 0.14518693089485168, train acc: 0.9493022053756031, val loss: 0.1539170742034912, val acc: 0.9387755102040817\n",
      "Epoch 200, train loss: 0.14518693089485168, train acc: 0.9493022053756031, val loss: 0.1539170742034912, val acc: 0.9387755102040817\n",
      "Value 's_m_46_val13: 0.9387755102040817' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15964435040950775, train acc: 0.9448976049982645, val loss: 0.17508509755134583, val acc: 0.9338046272493573\n",
      "Epoch 50, train loss: 0.15964435040950775, train acc: 0.9448976049982645, val loss: 0.1742577701807022, val acc: 0.9338046272493573\n",
      "Epoch 100, train loss: 0.15964435040950775, train acc: 0.9448976049982645, val loss: 0.1742539405822754, val acc: 0.9338046272493573\n",
      "Epoch 150, train loss: 0.15964435040950775, train acc: 0.9448976049982645, val loss: 0.1742539405822754, val acc: 0.9338046272493573\n",
      "Epoch 200, train loss: 0.15964435040950775, train acc: 0.9448976049982645, val loss: 0.1742539405822754, val acc: 0.9338046272493573\n",
      "Value 's_m_49_val13: 0.9338046272493573' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17526641488075256, train acc: 0.9405594405594405, val loss: 0.19957581162452698, val acc: 0.927461139896373\n",
      "Epoch 50, train loss: 0.17526641488075256, train acc: 0.9405594405594405, val loss: 0.1986304521560669, val acc: 0.927461139896373\n",
      "Epoch 100, train loss: 0.17526641488075256, train acc: 0.9405594405594405, val loss: 0.19862604141235352, val acc: 0.927461139896373\n",
      "Epoch 150, train loss: 0.17526641488075256, train acc: 0.9405594405594405, val loss: 0.19862604141235352, val acc: 0.927461139896373\n",
      "Epoch 200, train loss: 0.17526641488075256, train acc: 0.9405594405594405, val loss: 0.19862604141235352, val acc: 0.927461139896373\n",
      "Value 's_m_52_val13: 0.927461139896373' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08413500338792801, train acc: 0.9815702609242377, val loss: 0.14894191920757294, val acc: 0.974950884086444\n",
      "Epoch 50, train loss: 0.08413500338792801, train acc: 0.9815702609242377, val loss: 0.12353355437517166, val acc: 0.9774066797642437\n",
      "Epoch 100, train loss: 0.08413500338792801, train acc: 0.9815702609242377, val loss: 0.12342115491628647, val acc: 0.9774066797642437\n",
      "Epoch 150, train loss: 0.08413500338792801, train acc: 0.9815702609242377, val loss: 0.12342078983783722, val acc: 0.9774066797642437\n",
      "Epoch 200, train loss: 0.08413500338792801, train acc: 0.9815702609242377, val loss: 0.12342078983783722, val acc: 0.9774066797642437\n",
      "Value 's_m_1_val14: 0.9774066797642437' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07240547984838486, train acc: 0.9844541139240506, val loss: 0.09884228557348251, val acc: 0.9817193675889329\n",
      "Epoch 50, train loss: 0.07240547984838486, train acc: 0.9844541139240506, val loss: 0.10025312006473541, val acc: 0.9817193675889329\n",
      "Epoch 100, train loss: 0.07240547984838486, train acc: 0.9844541139240506, val loss: 0.10026021301746368, val acc: 0.9817193675889329\n",
      "Epoch 150, train loss: 0.07240547984838486, train acc: 0.9844541139240506, val loss: 0.10026022791862488, val acc: 0.9817193675889329\n",
      "Epoch 200, train loss: 0.07240547984838486, train acc: 0.9844541139240506, val loss: 0.10026022791862488, val acc: 0.9817193675889329\n",
      "Value 's_m_4_val14: 0.9817193675889329' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06265490502119064, train acc: 0.9867792290538387, val loss: 0.07763355225324631, val acc: 0.9860834990059643\n",
      "Epoch 50, train loss: 0.06265490502119064, train acc: 0.9867792290538387, val loss: 0.07864493876695633, val acc: 0.9860834990059643\n",
      "Epoch 100, train loss: 0.06265490502119064, train acc: 0.9867792290538387, val loss: 0.07864987850189209, val acc: 0.9860834990059643\n",
      "Epoch 150, train loss: 0.06265490502119064, train acc: 0.9867792290538387, val loss: 0.07864987850189209, val acc: 0.9860834990059643\n",
      "Epoch 200, train loss: 0.06265490502119064, train acc: 0.9867792290538387, val loss: 0.07864987850189209, val acc: 0.9860834990059643\n",
      "Value 's_m_7_val14: 0.9860834990059643' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05634579062461853, train acc: 0.988173508659397, val loss: 0.05885130912065506, val acc: 0.9905\n",
      "Epoch 50, train loss: 0.05634579062461853, train acc: 0.988173508659397, val loss: 0.05965117737650871, val acc: 0.9905\n",
      "Epoch 100, train loss: 0.05634579062461853, train acc: 0.988173508659397, val loss: 0.05965505540370941, val acc: 0.9905\n",
      "Epoch 150, train loss: 0.05634579062461853, train acc: 0.988173508659397, val loss: 0.05965505912899971, val acc: 0.9905\n",
      "Epoch 200, train loss: 0.05634579062461853, train acc: 0.988173508659397, val loss: 0.05965505912899971, val acc: 0.9905\n",
      "Value 's_m_10_val14: 0.9905' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05305975303053856, train acc: 0.9881336777526639, val loss: 0.044190168380737305, val acc: 0.9949698189134809\n",
      "Epoch 50, train loss: 0.05305975303053856, train acc: 0.9881336777526639, val loss: 0.0448596216738224, val acc: 0.9949698189134809\n",
      "Epoch 100, train loss: 0.05305975303053856, train acc: 0.9881336777526639, val loss: 0.04486292973160744, val acc: 0.9949698189134809\n",
      "Epoch 150, train loss: 0.05305975303053856, train acc: 0.9881336777526639, val loss: 0.04486292973160744, val acc: 0.9949698189134809\n",
      "Epoch 200, train loss: 0.05305975303053856, train acc: 0.9881336777526639, val loss: 0.04486292973160744, val acc: 0.9949698189134809\n",
      "Value 's_m_13_val14: 0.9949698189134809' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05263420566916466, train acc: 0.9865490897269181, val loss: 0.036985352635383606, val acc: 0.9954453441295547\n",
      "Epoch 50, train loss: 0.05263420566916466, train acc: 0.9865490897269181, val loss: 0.03756500780582428, val acc: 0.9954453441295547\n",
      "Epoch 100, train loss: 0.05263420566916466, train acc: 0.9865490897269181, val loss: 0.03756781294941902, val acc: 0.9954453441295547\n",
      "Epoch 150, train loss: 0.05263420566916466, train acc: 0.9865490897269181, val loss: 0.03756781294941902, val acc: 0.9954453441295547\n",
      "Epoch 200, train loss: 0.05263420566916466, train acc: 0.9865490897269181, val loss: 0.03756781294941902, val acc: 0.9954453441295547\n",
      "Value 's_m_16_val14: 0.9954453441295547' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05493053048849106, train acc: 0.9839607201309329, val loss: 0.035673439502716064, val acc: 0.9938900203665988\n",
      "Epoch 50, train loss: 0.05493053048849106, train acc: 0.9839607201309329, val loss: 0.036167848855257034, val acc: 0.9938900203665988\n",
      "Epoch 100, train loss: 0.05493053048849106, train acc: 0.9839607201309329, val loss: 0.03617018088698387, val acc: 0.9938900203665988\n",
      "Epoch 150, train loss: 0.05493053048849106, train acc: 0.9839607201309329, val loss: 0.03617018088698387, val acc: 0.9938900203665988\n",
      "Epoch 200, train loss: 0.05493053048849106, train acc: 0.9839607201309329, val loss: 0.03617018088698387, val acc: 0.9938900203665988\n",
      "Value 's_m_19_val14: 0.9938900203665988' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05972202867269516, train acc: 0.9807597231377719, val loss: 0.03920670971274376, val acc: 0.9912909836065574\n",
      "Epoch 50, train loss: 0.05972202867269516, train acc: 0.9807597231377719, val loss: 0.0396285243332386, val acc: 0.9912909836065574\n",
      "Epoch 100, train loss: 0.05972202867269516, train acc: 0.9807597231377719, val loss: 0.03963048756122589, val acc: 0.9912909836065574\n",
      "Epoch 150, train loss: 0.05972202867269516, train acc: 0.9807597231377719, val loss: 0.03963048756122589, val acc: 0.9912909836065574\n",
      "Epoch 200, train loss: 0.05972202867269516, train acc: 0.9807597231377719, val loss: 0.03963048756122589, val acc: 0.9912909836065574\n",
      "Value 's_m_22_val14: 0.9912909836065574' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0665724128484726, train acc: 0.9770577497510786, val loss: 0.048222240060567856, val acc: 0.98659793814433\n",
      "Epoch 50, train loss: 0.0665724128484726, train acc: 0.9770577497510786, val loss: 0.04856503754854202, val acc: 0.9871134020618557\n",
      "Epoch 100, train loss: 0.0665724128484726, train acc: 0.9770577497510786, val loss: 0.048566654324531555, val acc: 0.9871134020618557\n",
      "Epoch 150, train loss: 0.0665724128484726, train acc: 0.9770577497510786, val loss: 0.048566654324531555, val acc: 0.9871134020618557\n",
      "Epoch 200, train loss: 0.0665724128484726, train acc: 0.9770577497510786, val loss: 0.048566654324531555, val acc: 0.9871134020618557\n",
      "Value 's_m_25_val14: 0.9871134020618557' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07587405294179916, train acc: 0.9732620320855615, val loss: 0.0589342825114727, val acc: 0.9823651452282157\n",
      "Epoch 50, train loss: 0.07587405294179916, train acc: 0.9732620320855615, val loss: 0.05920124426484108, val acc: 0.9828838174273858\n",
      "Epoch 100, train loss: 0.07587405294179916, train acc: 0.9732620320855615, val loss: 0.059202536940574646, val acc: 0.9828838174273858\n",
      "Epoch 150, train loss: 0.07587405294179916, train acc: 0.9732620320855615, val loss: 0.059202536940574646, val acc: 0.9828838174273858\n",
      "Epoch 200, train loss: 0.07587405294179916, train acc: 0.9732620320855615, val loss: 0.059202536940574646, val acc: 0.9828838174273858\n",
      "Value 's_m_28_val14: 0.9828838174273858' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08618760854005814, train acc: 0.9693705822955234, val loss: 0.07309400290250778, val acc: 0.9786012526096033\n",
      "Epoch 50, train loss: 0.08618760854005814, train acc: 0.9693705822955234, val loss: 0.07327128946781158, val acc: 0.9786012526096033\n",
      "Epoch 100, train loss: 0.08618760854005814, train acc: 0.9693705822955234, val loss: 0.07327214628458023, val acc: 0.9786012526096033\n",
      "Epoch 150, train loss: 0.08618760854005814, train acc: 0.9693705822955234, val loss: 0.07327214628458023, val acc: 0.9786012526096033\n",
      "Epoch 200, train loss: 0.08618760854005814, train acc: 0.9693705822955234, val loss: 0.07327214628458023, val acc: 0.9786012526096033\n",
      "Value 's_m_31_val14: 0.9786012526096033' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09605425596237183, train acc: 0.965635593220339, val loss: 0.08890979737043381, val acc: 0.9737394957983193\n",
      "Epoch 50, train loss: 0.09605425596237183, train acc: 0.965635593220339, val loss: 0.08899187296628952, val acc: 0.9737394957983193\n",
      "Epoch 100, train loss: 0.09605425596237183, train acc: 0.965635593220339, val loss: 0.08899225294589996, val acc: 0.9737394957983193\n",
      "Epoch 150, train loss: 0.09605425596237183, train acc: 0.965635593220339, val loss: 0.08899225294589996, val acc: 0.9737394957983193\n",
      "Epoch 200, train loss: 0.09605425596237183, train acc: 0.965635593220339, val loss: 0.08899225294589996, val acc: 0.9737394957983193\n",
      "Value 's_m_34_val14: 0.9737394957983193' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10725000500679016, train acc: 0.9615483100034141, val loss: 0.10663606226444244, val acc: 0.968816067653277\n",
      "Epoch 50, train loss: 0.10725000500679016, train acc: 0.9615483100034141, val loss: 0.10663218796253204, val acc: 0.9693446088794926\n",
      "Epoch 100, train loss: 0.10725000500679016, train acc: 0.9615483100034141, val loss: 0.10663217306137085, val acc: 0.9693446088794926\n",
      "Epoch 150, train loss: 0.10725000500679016, train acc: 0.9615483100034141, val loss: 0.10663217306137085, val acc: 0.9693446088794926\n",
      "Epoch 200, train loss: 0.10725000500679016, train acc: 0.9615483100034141, val loss: 0.10663217306137085, val acc: 0.9693446088794926\n",
      "Value 's_m_37_val14: 0.9693446088794926' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12001348286867142, train acc: 0.9572730398899587, val loss: 0.125470370054245, val acc: 0.9643617021276596\n",
      "Epoch 50, train loss: 0.12001348286867142, train acc: 0.9572730398899587, val loss: 0.12538301944732666, val acc: 0.9648936170212766\n",
      "Epoch 100, train loss: 0.12001348286867142, train acc: 0.9572730398899587, val loss: 0.12538257241249084, val acc: 0.9648936170212766\n",
      "Epoch 150, train loss: 0.12001348286867142, train acc: 0.9572730398899587, val loss: 0.12538257241249084, val acc: 0.9648936170212766\n",
      "Epoch 200, train loss: 0.12001348286867142, train acc: 0.9572730398899587, val loss: 0.12538257241249084, val acc: 0.9648936170212766\n",
      "Value 's_m_40_val14: 0.9648936170212766' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13241082429885864, train acc: 0.9528922757187391, val loss: 0.1446278691291809, val acc: 0.9598501070663812\n",
      "Epoch 50, train loss: 0.13241082429885864, train acc: 0.9528922757187391, val loss: 0.14444874227046967, val acc: 0.9593147751605996\n",
      "Epoch 100, train loss: 0.13241082429885864, train acc: 0.9528922757187391, val loss: 0.14444787800312042, val acc: 0.9593147751605996\n",
      "Epoch 150, train loss: 0.13241082429885864, train acc: 0.9528922757187391, val loss: 0.14444787800312042, val acc: 0.9593147751605996\n",
      "Epoch 200, train loss: 0.13241082429885864, train acc: 0.9528922757187391, val loss: 0.14444787800312042, val acc: 0.9593147751605996\n",
      "Value 's_m_43_val14: 0.9598501070663812' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14507010579109192, train acc: 0.9486217725052338, val loss: 0.1631043553352356, val acc: 0.9563577586206896\n",
      "Epoch 50, train loss: 0.14507010579109192, train acc: 0.9486217725052338, val loss: 0.16282370686531067, val acc: 0.9574353448275862\n",
      "Epoch 100, train loss: 0.14507010579109192, train acc: 0.9486217725052338, val loss: 0.16282229125499725, val acc: 0.9574353448275862\n",
      "Epoch 150, train loss: 0.14507010579109192, train acc: 0.9486217725052338, val loss: 0.16282229125499725, val acc: 0.9574353448275862\n",
      "Epoch 200, train loss: 0.14507010579109192, train acc: 0.9486217725052338, val loss: 0.16282229125499725, val acc: 0.9574353448275862\n",
      "Value 's_m_46_val14: 0.9574353448275862' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.159205824136734, train acc: 0.9442442882249561, val loss: 0.18275988101959229, val acc: 0.9522776572668112\n",
      "Epoch 50, train loss: 0.159205824136734, train acc: 0.9442442882249561, val loss: 0.18235434591770172, val acc: 0.9539045553145337\n",
      "Epoch 100, train loss: 0.159205824136734, train acc: 0.9442442882249561, val loss: 0.18235237896442413, val acc: 0.9539045553145337\n",
      "Epoch 150, train loss: 0.159205824136734, train acc: 0.9442442882249561, val loss: 0.18235237896442413, val acc: 0.9539045553145337\n",
      "Epoch 200, train loss: 0.159205824136734, train acc: 0.9442442882249561, val loss: 0.18235237896442413, val acc: 0.9539045553145337\n",
      "Value 's_m_49_val14: 0.9539045553145337' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.17443770170211792, train acc: 0.9400672804532578, val loss: 0.20484627783298492, val acc: 0.9486899563318777\n",
      "Epoch 50, train loss: 0.17443770170211792, train acc: 0.9400672804532578, val loss: 0.20433421432971954, val acc: 0.949235807860262\n",
      "Epoch 100, train loss: 0.17443770170211792, train acc: 0.9400672804532578, val loss: 0.20433172583580017, val acc: 0.949235807860262\n",
      "Epoch 150, train loss: 0.17443770170211792, train acc: 0.9400672804532578, val loss: 0.20433172583580017, val acc: 0.949235807860262\n",
      "Epoch 200, train loss: 0.17443770170211792, train acc: 0.9400672804532578, val loss: 0.20433172583580017, val acc: 0.949235807860262\n",
      "Value 's_m_52_val14: 0.949235807860262' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08366039395332336, train acc: 0.9814517362704605, val loss: 0.0709242969751358, val acc: 0.9880239520958084\n",
      "Epoch 50, train loss: 0.08366039395332336, train acc: 0.9814517362704605, val loss: 0.05583139508962631, val acc: 0.9895209580838323\n",
      "Epoch 100, train loss: 0.08366039395332336, train acc: 0.9814517362704605, val loss: 0.0557682029902935, val acc: 0.9895209580838323\n",
      "Epoch 150, train loss: 0.08366039395332336, train acc: 0.9814517362704605, val loss: 0.055767983198165894, val acc: 0.9895209580838323\n",
      "Epoch 200, train loss: 0.08366039395332336, train acc: 0.9814517362704605, val loss: 0.055767983198165894, val acc: 0.9895209580838323\n",
      "Value 's_m_1_val15: 0.9895209580838323' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07129421830177307, train acc: 0.9846420323325635, val loss: 0.050409022718667984, val acc: 0.9894259818731118\n",
      "Epoch 50, train loss: 0.07129421830177307, train acc: 0.9846420323325635, val loss: 0.05102093145251274, val acc: 0.9894259818731118\n",
      "Epoch 100, train loss: 0.07129421830177307, train acc: 0.9846420323325635, val loss: 0.051023904234170914, val acc: 0.9894259818731118\n",
      "Epoch 150, train loss: 0.07129421830177307, train acc: 0.9846420323325635, val loss: 0.051023904234170914, val acc: 0.9894259818731118\n",
      "Epoch 200, train loss: 0.07129421830177307, train acc: 0.9846420323325635, val loss: 0.051023904234170914, val acc: 0.9894259818731118\n",
      "Value 's_m_4_val15: 0.9894259818731118' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06081763654947281, train acc: 0.9870215403688207, val loss: 0.049328576773405075, val acc: 0.989329268292683\n",
      "Epoch 50, train loss: 0.06081763654947281, train acc: 0.9870215403688207, val loss: 0.04984597489237785, val acc: 0.989329268292683\n",
      "Epoch 100, train loss: 0.06081763654947281, train acc: 0.9870215403688207, val loss: 0.04984847456216812, val acc: 0.989329268292683\n",
      "Epoch 150, train loss: 0.06081763654947281, train acc: 0.9870215403688207, val loss: 0.04984847456216812, val acc: 0.989329268292683\n",
      "Epoch 200, train loss: 0.06081763654947281, train acc: 0.9870215403688207, val loss: 0.04984847456216812, val acc: 0.989329268292683\n",
      "Value 's_m_7_val15: 0.989329268292683' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.053873755037784576, train acc: 0.9884963344252067, val loss: 0.04938197135925293, val acc: 0.9892307692307692\n",
      "Epoch 50, train loss: 0.053873755037784576, train acc: 0.9884963344252067, val loss: 0.049888670444488525, val acc: 0.9892307692307692\n",
      "Epoch 100, train loss: 0.053873755037784576, train acc: 0.9884963344252067, val loss: 0.049891162663698196, val acc: 0.9892307692307692\n",
      "Epoch 150, train loss: 0.053873755037784576, train acc: 0.9884963344252067, val loss: 0.049891162663698196, val acc: 0.9892307692307692\n",
      "Epoch 200, train loss: 0.053873755037784576, train acc: 0.9884963344252067, val loss: 0.049891162663698196, val acc: 0.9892307692307692\n",
      "Value 's_m_10_val15: 0.9892307692307692' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05011707916855812, train acc: 0.9885774846914743, val loss: 0.05053729936480522, val acc: 0.9875776397515528\n",
      "Epoch 50, train loss: 0.05011707916855812, train acc: 0.9885774846914743, val loss: 0.0509658008813858, val acc: 0.9875776397515528\n",
      "Epoch 100, train loss: 0.05011707916855812, train acc: 0.9885774846914743, val loss: 0.050967901945114136, val acc: 0.9875776397515528\n",
      "Epoch 150, train loss: 0.05011707916855812, train acc: 0.9885774846914743, val loss: 0.050967901945114136, val acc: 0.9875776397515528\n",
      "Epoch 200, train loss: 0.05011707916855812, train acc: 0.9885774846914743, val loss: 0.050967901945114136, val acc: 0.9875776397515528\n",
      "Value 's_m_13_val15: 0.9875776397515528' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.04971800744533539, train acc: 0.9868025920657499, val loss: 0.05113161727786064, val acc: 0.987460815047022\n",
      "Epoch 50, train loss: 0.04971800744533539, train acc: 0.9868025920657499, val loss: 0.051497071981430054, val acc: 0.987460815047022\n",
      "Epoch 100, train loss: 0.04971800744533539, train acc: 0.9868025920657499, val loss: 0.05149883031845093, val acc: 0.987460815047022\n",
      "Epoch 150, train loss: 0.04971800744533539, train acc: 0.9868025920657499, val loss: 0.05149883031845093, val acc: 0.987460815047022\n",
      "Epoch 200, train loss: 0.04971800744533539, train acc: 0.9868025920657499, val loss: 0.05149883031845093, val acc: 0.987460815047022\n",
      "Value 's_m_16_val15: 0.987460815047022' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05233379453420639, train acc: 0.9843675417661097, val loss: 0.05359261482954025, val acc: 0.9825949367088608\n",
      "Epoch 50, train loss: 0.05233379453420639, train acc: 0.9843675417661097, val loss: 0.053884029388427734, val acc: 0.9825949367088608\n",
      "Epoch 100, train loss: 0.05233379453420639, train acc: 0.9843675417661097, val loss: 0.053885430097579956, val acc: 0.9825949367088608\n",
      "Epoch 150, train loss: 0.05233379453420639, train acc: 0.9843675417661097, val loss: 0.053885430097579956, val acc: 0.9825949367088608\n",
      "Epoch 200, train loss: 0.05233379453420639, train acc: 0.9843675417661097, val loss: 0.053885430097579956, val acc: 0.9825949367088608\n",
      "Value 's_m_19_val15: 0.9825949367088608' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.05762472376227379, train acc: 0.981058785840141, val loss: 0.05763701722025871, val acc: 0.9776357827476039\n",
      "Epoch 50, train loss: 0.05762472376227379, train acc: 0.981058785840141, val loss: 0.05784577131271362, val acc: 0.9776357827476039\n",
      "Epoch 100, train loss: 0.05762472376227379, train acc: 0.981058785840141, val loss: 0.05784675106406212, val acc: 0.9776357827476039\n",
      "Epoch 150, train loss: 0.05762472376227379, train acc: 0.981058785840141, val loss: 0.05784675106406212, val acc: 0.9776357827476039\n",
      "Epoch 200, train loss: 0.05762472376227379, train acc: 0.981058785840141, val loss: 0.05784675106406212, val acc: 0.9776357827476039\n",
      "Value 's_m_22_val15: 0.9776357827476039' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06526371091604233, train acc: 0.9774633123689728, val loss: 0.06293236464262009, val acc: 0.9725806451612903\n",
      "Epoch 50, train loss: 0.06526371091604233, train acc: 0.9774633123689728, val loss: 0.06305339932441711, val acc: 0.9733870967741935\n",
      "Epoch 100, train loss: 0.06526371091604233, train acc: 0.9774633123689728, val loss: 0.06305393576622009, val acc: 0.9733870967741935\n",
      "Epoch 150, train loss: 0.06526371091604233, train acc: 0.9774633123689728, val loss: 0.06305393576622009, val acc: 0.9733870967741935\n",
      "Epoch 200, train loss: 0.06526371091604233, train acc: 0.9774633123689728, val loss: 0.06305393576622009, val acc: 0.9733870967741935\n",
      "Value 's_m_25_val15: 0.9733870967741935' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07536293566226959, train acc: 0.9736970287384316, val loss: 0.06948566436767578, val acc: 0.9682410423452769\n",
      "Epoch 50, train loss: 0.07536293566226959, train acc: 0.9736970287384316, val loss: 0.06951121985912323, val acc: 0.9682410423452769\n",
      "Epoch 100, train loss: 0.07536293566226959, train acc: 0.9736970287384316, val loss: 0.06951127201318741, val acc: 0.9682410423452769\n",
      "Epoch 150, train loss: 0.07536293566226959, train acc: 0.9736970287384316, val loss: 0.06951127201318741, val acc: 0.9682410423452769\n",
      "Epoch 200, train loss: 0.07536293566226959, train acc: 0.9736970287384316, val loss: 0.06951127201318741, val acc: 0.9682410423452769\n",
      "Value 's_m_28_val15: 0.9682410423452769' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08669538795948029, train acc: 0.969592937714566, val loss: 0.07750754803419113, val acc: 0.9629934210526315\n",
      "Epoch 50, train loss: 0.08669538795948029, train acc: 0.969592937714566, val loss: 0.07741360366344452, val acc: 0.9629934210526315\n",
      "Epoch 100, train loss: 0.08669538795948029, train acc: 0.969592937714566, val loss: 0.07741307467222214, val acc: 0.9629934210526315\n",
      "Epoch 150, train loss: 0.08669538795948029, train acc: 0.969592937714566, val loss: 0.07741307467222214, val acc: 0.9629934210526315\n",
      "Epoch 200, train loss: 0.08669538795948029, train acc: 0.969592937714566, val loss: 0.07741307467222214, val acc: 0.9629934210526315\n",
      "Value 's_m_31_val15: 0.9629934210526315' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09776894748210907, train acc: 0.9657613168724279, val loss: 0.08661744743585587, val acc: 0.957641196013289\n",
      "Epoch 50, train loss: 0.09776894748210907, train acc: 0.9657613168724279, val loss: 0.08639340102672577, val acc: 0.9584717607973422\n",
      "Epoch 100, train loss: 0.09776894748210907, train acc: 0.9657613168724279, val loss: 0.08639219403266907, val acc: 0.9584717607973422\n",
      "Epoch 150, train loss: 0.09776894748210907, train acc: 0.9657613168724279, val loss: 0.08639219403266907, val acc: 0.9584717607973422\n",
      "Epoch 200, train loss: 0.09776894748210907, train acc: 0.9657613168724279, val loss: 0.08639219403266907, val acc: 0.9584717607973422\n",
      "Value 's_m_34_val15: 0.9584717607973422' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1103571355342865, train acc: 0.9615448367313111, val loss: 0.09537005424499512, val acc: 0.9530201342281879\n",
      "Epoch 50, train loss: 0.1103571355342865, train acc: 0.9615448367313111, val loss: 0.0950402095913887, val acc: 0.9530201342281879\n",
      "Epoch 100, train loss: 0.1103571355342865, train acc: 0.9615448367313111, val loss: 0.09503843635320663, val acc: 0.9530201342281879\n",
      "Epoch 150, train loss: 0.1103571355342865, train acc: 0.9615448367313111, val loss: 0.09503843635320663, val acc: 0.9530201342281879\n",
      "Epoch 200, train loss: 0.1103571355342865, train acc: 0.9615448367313111, val loss: 0.09503843635320663, val acc: 0.9530201342281879\n",
      "Value 's_m_37_val15: 0.9530201342281879' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12458761781454086, train acc: 0.9569771323652145, val loss: 0.10440529137849808, val acc: 0.9474576271186441\n",
      "Epoch 50, train loss: 0.12458761781454086, train acc: 0.9569771323652145, val loss: 0.10395585745573044, val acc: 0.9474576271186441\n",
      "Epoch 100, train loss: 0.12458761781454086, train acc: 0.9569771323652145, val loss: 0.10395351052284241, val acc: 0.9474576271186441\n",
      "Epoch 150, train loss: 0.12458761781454086, train acc: 0.9569771323652145, val loss: 0.10395351052284241, val acc: 0.9474576271186441\n",
      "Epoch 200, train loss: 0.12458761781454086, train acc: 0.9569771323652145, val loss: 0.10395351052284241, val acc: 0.9474576271186441\n",
      "Value 's_m_40_val15: 0.9474576271186441' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1385660618543625, train acc: 0.9526811228777946, val loss: 0.11308978497982025, val acc: 0.9417808219178082\n",
      "Epoch 50, train loss: 0.1385660618543625, train acc: 0.9526811228777946, val loss: 0.11248638480901718, val acc: 0.9417808219178082\n",
      "Epoch 100, train loss: 0.1385660618543625, train acc: 0.9526811228777946, val loss: 0.1124831959605217, val acc: 0.9417808219178082\n",
      "Epoch 150, train loss: 0.1385660618543625, train acc: 0.9526811228777946, val loss: 0.1124831959605217, val acc: 0.9417808219178082\n",
      "Epoch 200, train loss: 0.1385660618543625, train acc: 0.9526811228777946, val loss: 0.1124831959605217, val acc: 0.9417808219178082\n",
      "Value 's_m_43_val15: 0.9417808219178082' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15274858474731445, train acc: 0.9482816996783477, val loss: 0.12258908152580261, val acc: 0.9359861591695502\n",
      "Epoch 50, train loss: 0.15274858474731445, train acc: 0.9482816996783477, val loss: 0.12186487019062042, val acc: 0.9359861591695502\n",
      "Epoch 100, train loss: 0.15274858474731445, train acc: 0.9482816996783477, val loss: 0.12186112254858017, val acc: 0.9359861591695502\n",
      "Epoch 150, train loss: 0.15274858474731445, train acc: 0.9482816996783477, val loss: 0.12186112254858017, val acc: 0.9359861591695502\n",
      "Epoch 200, train loss: 0.15274858474731445, train acc: 0.9482816996783477, val loss: 0.12186112254858017, val acc: 0.9359861591695502\n",
      "Value 's_m_46_val15: 0.9359861591695502' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16839535534381866, train acc: 0.9437766410912191, val loss: 0.13415145874023438, val acc: 0.9300699300699301\n",
      "Epoch 50, train loss: 0.16839535534381866, train acc: 0.9437766410912191, val loss: 0.13328079879283905, val acc: 0.9300699300699301\n",
      "Epoch 100, train loss: 0.16839535534381866, train acc: 0.9437766410912191, val loss: 0.13327623903751373, val acc: 0.9300699300699301\n",
      "Epoch 150, train loss: 0.16839535534381866, train acc: 0.9437766410912191, val loss: 0.13327623903751373, val acc: 0.9300699300699301\n",
      "Epoch 200, train loss: 0.16839535534381866, train acc: 0.9437766410912191, val loss: 0.13327623903751373, val acc: 0.9300699300699301\n",
      "Value 's_m_49_val15: 0.9300699300699301' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18526619672775269, train acc: 0.9392495277348446, val loss: 0.1474064588546753, val acc: 0.9240282685512368\n",
      "Epoch 50, train loss: 0.18526619672775269, train acc: 0.9392495277348446, val loss: 0.1463945209980011, val acc: 0.9249116607773852\n",
      "Epoch 100, train loss: 0.18526619672775269, train acc: 0.9392495277348446, val loss: 0.1463892012834549, val acc: 0.9249116607773852\n",
      "Epoch 150, train loss: 0.18526619672775269, train acc: 0.9392495277348446, val loss: 0.1463892012834549, val acc: 0.9249116607773852\n",
      "Epoch 200, train loss: 0.18526619672775269, train acc: 0.9392495277348446, val loss: 0.1463892012834549, val acc: 0.9249116607773852\n",
      "Value 's_m_52_val15: 0.9249116607773852' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set15_results.txt' successfully.\n"
     ]
    }
   ],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(1,53,step=3):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted, val_predicted, train_probs, val_probs= run_training(\n",
    "            train_dataloader, val_dataloader=val_dataloader, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "        file_name = 'set'+str(j)+'_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(max(val_accs))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 's_13_1:0.9780405405405406' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/s_13_1results.txt' successfully.\n"
     ]
    }
   ],
   "source": [
    "file_name = model_name + 'results.txt'\n",
    "file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/\" + file_name  # Replace with your desired file path\n",
    "value_to_save = model_name+\": \"+str(max(val_accs))  # Replace with the value you want to save\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"w\") as file:\n",
    "        # 2. Write the value to the file\n",
    "        file.write(value_to_save)\n",
    "        print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shift = 5  # Replace with the desired shift value\n",
    "\n",
    "# Create an empty DataFrame to store the shifted data\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "# Loop through unique trial values\n",
    "for trial_value in df['trial'].unique():\n",
    "    # Filter the DataFrame for the current trial\n",
    "    trial_df = df[df['trial'] == trial_value].copy()\n",
    "\n",
    "    # Create shifted columns for each column in columns_to_shift\n",
    "    for col in columns_to_shift:\n",
    "        new_col_name = col + '_minus_' + str(shift)\n",
    "        trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "    # Drop the last 'i' records for each trial\n",
    "    trial_df = trial_df.dropna()\n",
    "\n",
    "    # Append the modified trial_df to the shifted_df\n",
    "    shifted_df = shifted_df.append(trial_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.19731834530830383, train acc: 0.9342149454240134, val loss: 0.06737794727087021, val acc: 0.9834905660377359\n",
      "Epoch 50, train loss: 0.19731834530830383, train acc: 0.9342149454240134, val loss: 0.05727081373333931, val acc: 0.9882075471698113\n",
      "Epoch 100, train loss: 0.19731834530830383, train acc: 0.9342149454240134, val loss: 0.05722562596201897, val acc: 0.9882075471698113\n",
      "Epoch 150, train loss: 0.19731834530830383, train acc: 0.9342149454240134, val loss: 0.057225532829761505, val acc: 0.9882075471698113\n",
      "Epoch 200, train loss: 0.19731834530830383, train acc: 0.9342149454240134, val loss: 0.057225532829761505, val acc: 0.9882075471698113\n",
      "Value 's_m_55_val1: 0.9882075471698113 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.229063481092453, train acc: 0.9265505522514869, val loss: 0.059104789048433304, val acc: 0.9876237623762376\n",
      "Epoch 50, train loss: 0.229063481092453, train acc: 0.9265505522514869, val loss: 0.0604349784553051, val acc: 0.9876237623762376\n",
      "Epoch 100, train loss: 0.229063481092453, train acc: 0.9265505522514869, val loss: 0.06044141203165054, val acc: 0.9876237623762376\n",
      "Epoch 150, train loss: 0.229063481092453, train acc: 0.9265505522514869, val loss: 0.06044141203165054, val acc: 0.9876237623762376\n",
      "Epoch 200, train loss: 0.229063481092453, train acc: 0.9265505522514869, val loss: 0.06044141203165054, val acc: 0.9876237623762376\n",
      "Value 's_m_60_val1: 0.9876237623762376 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.26126521825790405, train acc: 0.9191315563198624, val loss: 0.06257358938455582, val acc: 0.9869791666666666\n",
      "Epoch 50, train loss: 0.26126521825790405, train acc: 0.9191315563198624, val loss: 0.06411983072757721, val acc: 0.9869791666666666\n",
      "Epoch 100, train loss: 0.26126521825790405, train acc: 0.9191315563198624, val loss: 0.06412713974714279, val acc: 0.9869791666666666\n",
      "Epoch 150, train loss: 0.26126521825790405, train acc: 0.9191315563198624, val loss: 0.0641271322965622, val acc: 0.9869791666666666\n",
      "Epoch 200, train loss: 0.26126521825790405, train acc: 0.9191315563198624, val loss: 0.0641271322965622, val acc: 0.9869791666666666\n",
      "Value 's_m_65_val1: 0.9869791666666666 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.29380524158477783, train acc: 0.9121845082680592, val loss: 0.06645027548074722, val acc: 0.9862637362637363\n",
      "Epoch 50, train loss: 0.29380524158477783, train acc: 0.9121845082680592, val loss: 0.06827040016651154, val acc: 0.9862637362637363\n",
      "Epoch 100, train loss: 0.29380524158477783, train acc: 0.9121845082680592, val loss: 0.06827940046787262, val acc: 0.9862637362637363\n",
      "Epoch 150, train loss: 0.29380524158477783, train acc: 0.9121845082680592, val loss: 0.06827940791845322, val acc: 0.9862637362637363\n",
      "Epoch 200, train loss: 0.29380524158477783, train acc: 0.9121845082680592, val loss: 0.06827940791845322, val acc: 0.9862637362637363\n",
      "Value 's_m_70_val1: 0.9862637362637363 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.32686886191368103, train acc: 0.9051541850220264, val loss: 0.0688491091132164, val acc: 0.9854651162790697\n",
      "Epoch 50, train loss: 0.32686886191368103, train acc: 0.9051541850220264, val loss: 0.07080861926078796, val acc: 0.9854651162790697\n",
      "Epoch 100, train loss: 0.32686886191368103, train acc: 0.9051541850220264, val loss: 0.07081808894872665, val acc: 0.9854651162790697\n",
      "Epoch 150, train loss: 0.32686886191368103, train acc: 0.9051541850220264, val loss: 0.07081808894872665, val acc: 0.9854651162790697\n",
      "Epoch 200, train loss: 0.32686886191368103, train acc: 0.9051541850220264, val loss: 0.07081808894872665, val acc: 0.9854651162790697\n",
      "Value 's_m_75_val1: 0.9854651162790697 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.361577570438385, train acc: 0.8982158786797502, val loss: 0.072077177464962, val acc: 0.9845679012345679\n",
      "Epoch 50, train loss: 0.361577570438385, train acc: 0.8982158786797502, val loss: 0.07420109212398529, val acc: 0.9845679012345679\n",
      "Epoch 100, train loss: 0.361577570438385, train acc: 0.8982158786797502, val loss: 0.07421138882637024, val acc: 0.9845679012345679\n",
      "Epoch 150, train loss: 0.361577570438385, train acc: 0.8982158786797502, val loss: 0.07421138137578964, val acc: 0.9845679012345679\n",
      "Epoch 200, train loss: 0.361577570438385, train acc: 0.8982158786797502, val loss: 0.07421138137578964, val acc: 0.9845679012345679\n",
      "Value 's_m_80_val1: 0.9845679012345679 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set1_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2095339149236679, train acc: 0.9320472873900293, val loss: 0.056565333157777786, val acc: 0.9727272727272728\n",
      "Epoch 50, train loss: 0.2095339149236679, train acc: 0.9320472873900293, val loss: 0.0599195621907711, val acc: 0.971900826446281\n",
      "Epoch 100, train loss: 0.2095339149236679, train acc: 0.9320472873900293, val loss: 0.05995163694024086, val acc: 0.971900826446281\n",
      "Epoch 150, train loss: 0.2095339149236679, train acc: 0.9320472873900293, val loss: 0.05995171144604683, val acc: 0.971900826446281\n",
      "Epoch 200, train loss: 0.2095339149236679, train acc: 0.9320472873900293, val loss: 0.05995171144604683, val acc: 0.971900826446281\n",
      "Value 's_m_55_val2: 0.9727272727272728 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24278514087200165, train acc: 0.9242016338655774, val loss: 0.07564675062894821, val acc: 0.9675\n",
      "Epoch 50, train loss: 0.24278514087200165, train acc: 0.9242016338655774, val loss: 0.07441982626914978, val acc: 0.9679166666666666\n",
      "Epoch 100, train loss: 0.24278514087200165, train acc: 0.9242016338655774, val loss: 0.07441379874944687, val acc: 0.9679166666666666\n",
      "Epoch 150, train loss: 0.24278514087200165, train acc: 0.9242016338655774, val loss: 0.07441380620002747, val acc: 0.9679166666666666\n",
      "Epoch 200, train loss: 0.24278514087200165, train acc: 0.9242016338655774, val loss: 0.07441380620002747, val acc: 0.9679166666666666\n",
      "Value 's_m_60_val2: 0.9679166666666666 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.27649950981140137, train acc: 0.9169018058690744, val loss: 0.09203101694583893, val acc: 0.9634453781512605\n",
      "Epoch 50, train loss: 0.27649950981140137, train acc: 0.9169018058690744, val loss: 0.09069260209798813, val acc: 0.9638655462184874\n",
      "Epoch 100, train loss: 0.27649950981140137, train acc: 0.9169018058690744, val loss: 0.09068607538938522, val acc: 0.9638655462184874\n",
      "Epoch 150, train loss: 0.27649950981140137, train acc: 0.9169018058690744, val loss: 0.09068607538938522, val acc: 0.9638655462184874\n",
      "Epoch 200, train loss: 0.27649950981140137, train acc: 0.9169018058690744, val loss: 0.09068607538938522, val acc: 0.9638655462184874\n",
      "Value 's_m_65_val2: 0.9638655462184874 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.31080877780914307, train acc: 0.9095501334349981, val loss: 0.10896643996238708, val acc: 0.9593220338983051\n",
      "Epoch 50, train loss: 0.31080877780914307, train acc: 0.9095501334349981, val loss: 0.10740865767002106, val acc: 0.9593220338983051\n",
      "Epoch 100, train loss: 0.31080877780914307, train acc: 0.9095501334349981, val loss: 0.10740116238594055, val acc: 0.9593220338983051\n",
      "Epoch 150, train loss: 0.31080877780914307, train acc: 0.9095501334349981, val loss: 0.10740116238594055, val acc: 0.9593220338983051\n",
      "Epoch 200, train loss: 0.31080877780914307, train acc: 0.9095501334349981, val loss: 0.10740116238594055, val acc: 0.9593220338983051\n",
      "Value 's_m_70_val2: 0.9593220338983051 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3457479476928711, train acc: 0.902047913446677, val loss: 0.1259927749633789, val acc: 0.9547008547008548\n",
      "Epoch 50, train loss: 0.3457479476928711, train acc: 0.902047913446677, val loss: 0.12394244968891144, val acc: 0.9555555555555556\n",
      "Epoch 100, train loss: 0.3457479476928711, train acc: 0.902047913446677, val loss: 0.123932383954525, val acc: 0.9555555555555556\n",
      "Epoch 150, train loss: 0.3457479476928711, train acc: 0.902047913446677, val loss: 0.123932383954525, val acc: 0.9555555555555556\n",
      "Epoch 200, train loss: 0.3457479476928711, train acc: 0.902047913446677, val loss: 0.123932383954525, val acc: 0.9555555555555556\n",
      "Value 's_m_75_val2: 0.9555555555555556 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.38247716426849365, train acc: 0.8943889541715628, val loss: 0.14331519603729248, val acc: 0.9508620689655173\n",
      "Epoch 50, train loss: 0.38247716426849365, train acc: 0.8943889541715628, val loss: 0.1409032791852951, val acc: 0.9512931034482759\n",
      "Epoch 100, train loss: 0.38247716426849365, train acc: 0.8943889541715628, val loss: 0.1408914178609848, val acc: 0.9512931034482759\n",
      "Epoch 150, train loss: 0.38247716426849365, train acc: 0.8943889541715628, val loss: 0.14089138805866241, val acc: 0.9512931034482759\n",
      "Epoch 200, train loss: 0.38247716426849365, train acc: 0.8943889541715628, val loss: 0.14089138805866241, val acc: 0.9512931034482759\n",
      "Value 's_m_80_val2: 0.9512931034482759 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set2_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19661293923854828, train acc: 0.9364108431614152, val loss: 0.24285119771957397, val acc: 0.9177631578947368\n",
      "Epoch 50, train loss: 0.19661293923854828, train acc: 0.9364108431614152, val loss: 0.23815424740314484, val acc: 0.9184210526315789\n",
      "Epoch 100, train loss: 0.19661293923854828, train acc: 0.9364108431614152, val loss: 0.23815472424030304, val acc: 0.9184210526315789\n",
      "Epoch 150, train loss: 0.19661293923854828, train acc: 0.9364108431614152, val loss: 0.2381548434495926, val acc: 0.9184210526315789\n",
      "Epoch 200, train loss: 0.19661293923854828, train acc: 0.9364108431614152, val loss: 0.2381548434495926, val acc: 0.9184210526315789\n",
      "Value 's_m_55_val3: 0.9184210526315789 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.22776906192302704, train acc: 0.9291124576724291, val loss: 0.28022313117980957, val acc: 0.9073333333333333\n",
      "Epoch 50, train loss: 0.22776906192302704, train acc: 0.9291124576724291, val loss: 0.27539899945259094, val acc: 0.908\n",
      "Epoch 100, train loss: 0.22776906192302704, train acc: 0.9291124576724291, val loss: 0.2753751277923584, val acc: 0.908\n",
      "Epoch 150, train loss: 0.22776906192302704, train acc: 0.9291124576724291, val loss: 0.2753751575946808, val acc: 0.908\n",
      "Epoch 200, train loss: 0.22776906192302704, train acc: 0.9291124576724291, val loss: 0.2753751575946808, val acc: 0.908\n",
      "Value 's_m_60_val3: 0.908 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.25916537642478943, train acc: 0.9219003789929615, val loss: 0.32065287232398987, val acc: 0.897972972972973\n",
      "Epoch 50, train loss: 0.25916537642478943, train acc: 0.9219003789929615, val loss: 0.31513896584510803, val acc: 0.897972972972973\n",
      "Epoch 100, train loss: 0.25916537642478943, train acc: 0.9219003789929615, val loss: 0.31511181592941284, val acc: 0.897972972972973\n",
      "Epoch 150, train loss: 0.25916537642478943, train acc: 0.9219003789929615, val loss: 0.31511181592941284, val acc: 0.897972972972973\n",
      "Epoch 200, train loss: 0.25916537642478943, train acc: 0.9219003789929615, val loss: 0.31511181592941284, val acc: 0.897972972972973\n",
      "Value 's_m_65_val3: 0.897972972972973 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.290546178817749, train acc: 0.9150977883385122, val loss: 0.3641279637813568, val acc: 0.8897260273972603\n",
      "Epoch 50, train loss: 0.290546178817749, train acc: 0.9150977883385122, val loss: 0.35818764567375183, val acc: 0.8904109589041096\n",
      "Epoch 100, train loss: 0.290546178817749, train acc: 0.9150977883385122, val loss: 0.35815855860710144, val acc: 0.8904109589041096\n",
      "Epoch 150, train loss: 0.290546178817749, train acc: 0.9150977883385122, val loss: 0.35815852880477905, val acc: 0.8904109589041096\n",
      "Epoch 200, train loss: 0.290546178817749, train acc: 0.9150977883385122, val loss: 0.35815852880477905, val acc: 0.8904109589041096\n",
      "Value 's_m_70_val3: 0.8904109589041096 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.322235643863678, train acc: 0.9081188668765043, val loss: 0.41209423542022705, val acc: 0.8819444444444444\n",
      "Epoch 50, train loss: 0.322235643863678, train acc: 0.9081188668765043, val loss: 0.4056822955608368, val acc: 0.8833333333333333\n",
      "Epoch 100, train loss: 0.322235643863678, train acc: 0.9081188668765043, val loss: 0.40565070509910583, val acc: 0.8833333333333333\n",
      "Epoch 150, train loss: 0.322235643863678, train acc: 0.9081188668765043, val loss: 0.40565070509910583, val acc: 0.8833333333333333\n",
      "Epoch 200, train loss: 0.322235643863678, train acc: 0.9081188668765043, val loss: 0.40565070509910583, val acc: 0.8833333333333333\n",
      "Value 's_m_75_val3: 0.8833333333333333 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.35540980100631714, train acc: 0.9013318326767961, val loss: 0.4653293788433075, val acc: 0.8746478873239436\n",
      "Epoch 50, train loss: 0.35540980100631714, train acc: 0.9013318326767961, val loss: 0.4583331346511841, val acc: 0.8753521126760564\n",
      "Epoch 100, train loss: 0.35540980100631714, train acc: 0.9013318326767961, val loss: 0.45829901099205017, val acc: 0.8753521126760564\n",
      "Epoch 150, train loss: 0.35540980100631714, train acc: 0.9013318326767961, val loss: 0.45829901099205017, val acc: 0.8753521126760564\n",
      "Epoch 200, train loss: 0.35540980100631714, train acc: 0.9013318326767961, val loss: 0.45829901099205017, val acc: 0.8753521126760564\n",
      "Value 's_m_80_val3: 0.8753521126760564 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set3_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19140803813934326, train acc: 0.9358149779735683, val loss: 0.22569558024406433, val acc: 0.9397668393782384\n",
      "Epoch 50, train loss: 0.19140803813934326, train acc: 0.9358149779735683, val loss: 0.23064561188220978, val acc: 0.935880829015544\n",
      "Epoch 100, train loss: 0.19140803813934326, train acc: 0.9358149779735683, val loss: 0.23068083822727203, val acc: 0.935880829015544\n",
      "Epoch 150, train loss: 0.19140803813934326, train acc: 0.9358149779735683, val loss: 0.23068086802959442, val acc: 0.935880829015544\n",
      "Epoch 200, train loss: 0.19140803813934326, train acc: 0.9358149779735683, val loss: 0.23068086802959442, val acc: 0.935880829015544\n",
      "Value 's_m_55_val4: 0.9397668393782384 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2221992015838623, train acc: 0.9281891168599464, val loss: 0.2729339897632599, val acc: 0.928477690288714\n",
      "Epoch 50, train loss: 0.2221992015838623, train acc: 0.9281891168599464, val loss: 0.2710205614566803, val acc: 0.9297900262467191\n",
      "Epoch 100, train loss: 0.2221992015838623, train acc: 0.9281891168599464, val loss: 0.27101120352745056, val acc: 0.9297900262467191\n",
      "Epoch 150, train loss: 0.2221992015838623, train acc: 0.9281891168599464, val loss: 0.27101120352745056, val acc: 0.9297900262467191\n",
      "Epoch 200, train loss: 0.2221992015838623, train acc: 0.9281891168599464, val loss: 0.27101120352745056, val acc: 0.9297900262467191\n",
      "Value 's_m_60_val4: 0.9297900262467191 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2535279393196106, train acc: 0.9207317073170732, val loss: 0.3147980272769928, val acc: 0.9222074468085106\n",
      "Epoch 50, train loss: 0.2535279393196106, train acc: 0.9207317073170732, val loss: 0.3127550184726715, val acc: 0.9235372340425532\n",
      "Epoch 100, train loss: 0.2535279393196106, train acc: 0.9207317073170732, val loss: 0.312745064496994, val acc: 0.9235372340425532\n",
      "Epoch 150, train loss: 0.2535279393196106, train acc: 0.9207317073170732, val loss: 0.312745064496994, val acc: 0.9235372340425532\n",
      "Epoch 200, train loss: 0.2535279393196106, train acc: 0.9207317073170732, val loss: 0.312745064496994, val acc: 0.9235372340425532\n",
      "Value 's_m_65_val4: 0.9235372340425532 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.285276859998703, train acc: 0.9132662397072279, val loss: 0.3574187457561493, val acc: 0.9157681940700808\n",
      "Epoch 50, train loss: 0.285276859998703, train acc: 0.9132662397072279, val loss: 0.35528630018234253, val acc: 0.9164420485175202\n",
      "Epoch 100, train loss: 0.285276859998703, train acc: 0.9132662397072279, val loss: 0.3552759289741516, val acc: 0.9164420485175202\n",
      "Epoch 150, train loss: 0.285276859998703, train acc: 0.9132662397072279, val loss: 0.3552759289741516, val acc: 0.9164420485175202\n",
      "Epoch 200, train loss: 0.285276859998703, train acc: 0.9132662397072279, val loss: 0.3552759289741516, val acc: 0.9164420485175202\n",
      "Value 's_m_70_val4: 0.9164420485175202 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.31736648082733154, train acc: 0.9061167747914736, val loss: 0.4019533693790436, val acc: 0.9084699453551912\n",
      "Epoch 50, train loss: 0.31736648082733154, train acc: 0.9061167747914736, val loss: 0.3998297154903412, val acc: 0.9084699453551912\n",
      "Epoch 100, train loss: 0.31736648082733154, train acc: 0.9061167747914736, val loss: 0.39981937408447266, val acc: 0.9084699453551912\n",
      "Epoch 150, train loss: 0.31736648082733154, train acc: 0.9061167747914736, val loss: 0.39981937408447266, val acc: 0.9084699453551912\n",
      "Epoch 200, train loss: 0.31736648082733154, train acc: 0.9061167747914736, val loss: 0.39981937408447266, val acc: 0.9084699453551912\n",
      "Value 's_m_75_val4: 0.9084699453551912 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.35106879472732544, train acc: 0.8992488262910798, val loss: 0.44865456223487854, val acc: 0.9002770083102493\n",
      "Epoch 50, train loss: 0.35106879472732544, train acc: 0.8992488262910798, val loss: 0.4464550018310547, val acc: 0.9009695290858726\n",
      "Epoch 100, train loss: 0.35106879472732544, train acc: 0.8992488262910798, val loss: 0.4464443624019623, val acc: 0.9009695290858726\n",
      "Epoch 150, train loss: 0.35106879472732544, train acc: 0.8992488262910798, val loss: 0.44644439220428467, val acc: 0.9009695290858726\n",
      "Epoch 200, train loss: 0.35106879472732544, train acc: 0.8992488262910798, val loss: 0.44644439220428467, val acc: 0.9009695290858726\n",
      "Value 's_m_80_val4: 0.9009695290858726 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set4_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19608527421951294, train acc: 0.9362948278969183, val loss: 0.09826482087373734, val acc: 0.95556640625\n",
      "Epoch 50, train loss: 0.19608527421951294, train acc: 0.9362948278969183, val loss: 0.09884234517812729, val acc: 0.95654296875\n",
      "Epoch 100, train loss: 0.19608527421951294, train acc: 0.9362948278969183, val loss: 0.09885343909263611, val acc: 0.95654296875\n",
      "Epoch 150, train loss: 0.19608527421951294, train acc: 0.9362948278969183, val loss: 0.0988534465432167, val acc: 0.95654296875\n",
      "Epoch 200, train loss: 0.19608527421951294, train acc: 0.9362948278969183, val loss: 0.0988534465432167, val acc: 0.95654296875\n",
      "Value 's_m_55_val5: 0.95751953125 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.22809799015522003, train acc: 0.9290016426355174, val loss: 0.11273403465747833, val acc: 0.9511834319526628\n",
      "Epoch 50, train loss: 0.22809799015522003, train acc: 0.9290016426355174, val loss: 0.11118921637535095, val acc: 0.9516765285996055\n",
      "Epoch 100, train loss: 0.22809799015522003, train acc: 0.9290016426355174, val loss: 0.11118163168430328, val acc: 0.9516765285996055\n",
      "Epoch 150, train loss: 0.22809799015522003, train acc: 0.9290016426355174, val loss: 0.11118163913488388, val acc: 0.9516765285996055\n",
      "Epoch 200, train loss: 0.22809799015522003, train acc: 0.9290016426355174, val loss: 0.11118163913488388, val acc: 0.9516765285996055\n",
      "Value 's_m_60_val5: 0.9516765285996055 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2606479525566101, train acc: 0.9214734701423554, val loss: 0.12628529965877533, val acc: 0.9467131474103586\n",
      "Epoch 50, train loss: 0.2606479525566101, train acc: 0.9214734701423554, val loss: 0.12450587004423141, val acc: 0.9472111553784861\n",
      "Epoch 100, train loss: 0.2606479525566101, train acc: 0.9214734701423554, val loss: 0.1244971752166748, val acc: 0.9472111553784861\n",
      "Epoch 150, train loss: 0.2606479525566101, train acc: 0.9214734701423554, val loss: 0.12449716776609421, val acc: 0.9472111553784861\n",
      "Epoch 200, train loss: 0.2606479525566101, train acc: 0.9214734701423554, val loss: 0.12449716776609421, val acc: 0.9472111553784861\n",
      "Value 's_m_65_val5: 0.9472111553784861 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.29357650876045227, train acc: 0.9143566210900917, val loss: 0.14141863584518433, val acc: 0.9416498993963782\n",
      "Epoch 50, train loss: 0.29357650876045227, train acc: 0.9143566210900917, val loss: 0.1392413079738617, val acc: 0.9416498993963782\n",
      "Epoch 100, train loss: 0.29357650876045227, train acc: 0.9143566210900917, val loss: 0.13923056423664093, val acc: 0.9416498993963782\n",
      "Epoch 150, train loss: 0.29357650876045227, train acc: 0.9143566210900917, val loss: 0.13923056423664093, val acc: 0.9416498993963782\n",
      "Epoch 200, train loss: 0.29357650876045227, train acc: 0.9143566210900917, val loss: 0.13923056423664093, val acc: 0.9416498993963782\n",
      "Value 's_m_70_val5: 0.9416498993963782 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.32700446248054504, train acc: 0.907430252419814, val loss: 0.15689168870449066, val acc: 0.9359756097560976\n",
      "Epoch 50, train loss: 0.32700446248054504, train acc: 0.907430252419814, val loss: 0.154333233833313, val acc: 0.9369918699186992\n",
      "Epoch 100, train loss: 0.32700446248054504, train acc: 0.907430252419814, val loss: 0.15432068705558777, val acc: 0.9369918699186992\n",
      "Epoch 150, train loss: 0.32700446248054504, train acc: 0.907430252419814, val loss: 0.15432067215442657, val acc: 0.9369918699186992\n",
      "Epoch 200, train loss: 0.32700446248054504, train acc: 0.907430252419814, val loss: 0.15432067215442657, val acc: 0.9369918699186992\n",
      "Value 's_m_75_val5: 0.9369918699186992 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3621537685394287, train acc: 0.9005577995768417, val loss: 0.17405453324317932, val acc: 0.9312114989733059\n",
      "Epoch 50, train loss: 0.3621537685394287, train acc: 0.9005577995768417, val loss: 0.1711525171995163, val acc: 0.9322381930184805\n",
      "Epoch 100, train loss: 0.3621537685394287, train acc: 0.9005577995768417, val loss: 0.1711382120847702, val acc: 0.9322381930184805\n",
      "Epoch 150, train loss: 0.3621537685394287, train acc: 0.9005577995768417, val loss: 0.1711382120847702, val acc: 0.9322381930184805\n",
      "Epoch 200, train loss: 0.3621537685394287, train acc: 0.9005577995768417, val loss: 0.1711382120847702, val acc: 0.9322381930184805\n",
      "Value 's_m_80_val5: 0.9322381930184805 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set5_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.20024608075618744, train acc: 0.9374771814530851, val loss: 0.14066004753112793, val acc: 0.9425385934819898\n",
      "Epoch 50, train loss: 0.20024608075618744, train acc: 0.9374771814530851, val loss: 0.12300843745470047, val acc: 0.9468267581475128\n",
      "Epoch 100, train loss: 0.20024608075618744, train acc: 0.9374771814530851, val loss: 0.1229289174079895, val acc: 0.9472555746140652\n",
      "Epoch 150, train loss: 0.20024608075618744, train acc: 0.9374771814530851, val loss: 0.12292864918708801, val acc: 0.9472555746140652\n",
      "Epoch 200, train loss: 0.20024608075618744, train acc: 0.9374771814530851, val loss: 0.12292864918708801, val acc: 0.9472555746140652\n",
      "Value 's_m_55_val6: 0.9472555746140652 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2319236546754837, train acc: 0.9304271449704142, val loss: 0.13948136568069458, val acc: 0.9403114186851211\n",
      "Epoch 50, train loss: 0.2319236546754837, train acc: 0.9304271449704142, val loss: 0.13631701469421387, val acc: 0.9411764705882353\n",
      "Epoch 100, train loss: 0.2319236546754837, train acc: 0.9304271449704142, val loss: 0.13630147278308868, val acc: 0.9411764705882353\n",
      "Epoch 150, train loss: 0.2319236546754837, train acc: 0.9304271449704142, val loss: 0.1363014578819275, val acc: 0.9411764705882353\n",
      "Epoch 200, train loss: 0.2319236546754837, train acc: 0.9304271449704142, val loss: 0.1363014578819275, val acc: 0.9411764705882353\n",
      "Value 's_m_60_val6: 0.9411764705882353 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2640402317047119, train acc: 0.923613713001124, val loss: 0.15271182358264923, val acc: 0.9341186736474695\n",
      "Epoch 50, train loss: 0.2640402317047119, train acc: 0.923613713001124, val loss: 0.14886930584907532, val acc: 0.9358638743455497\n",
      "Epoch 100, train loss: 0.2640402317047119, train acc: 0.923613713001124, val loss: 0.14885035157203674, val acc: 0.9358638743455497\n",
      "Epoch 150, train loss: 0.2640402317047119, train acc: 0.923613713001124, val loss: 0.14885033667087555, val acc: 0.9358638743455497\n",
      "Epoch 200, train loss: 0.2640402317047119, train acc: 0.923613713001124, val loss: 0.14885033667087555, val acc: 0.9358638743455497\n",
      "Value 's_m_65_val6: 0.9358638743455497 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2964926064014435, train acc: 0.9167615793470008, val loss: 0.1646621823310852, val acc: 0.9286971830985915\n",
      "Epoch 50, train loss: 0.2964926064014435, train acc: 0.9167615793470008, val loss: 0.16058474779129028, val acc: 0.929137323943662\n",
      "Epoch 100, train loss: 0.2964926064014435, train acc: 0.9167615793470008, val loss: 0.1605646163225174, val acc: 0.929137323943662\n",
      "Epoch 150, train loss: 0.2964926064014435, train acc: 0.9167615793470008, val loss: 0.160564586520195, val acc: 0.929137323943662\n",
      "Epoch 200, train loss: 0.2964926064014435, train acc: 0.9167615793470008, val loss: 0.160564586520195, val acc: 0.929137323943662\n",
      "Value 's_m_70_val6: 0.929137323943662 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3296971917152405, train acc: 0.9097248941900731, val loss: 0.17577514052391052, val acc: 0.9218472468916519\n",
      "Epoch 50, train loss: 0.3296971917152405, train acc: 0.9097248941900731, val loss: 0.17151111364364624, val acc: 0.9240674955595026\n",
      "Epoch 100, train loss: 0.3296971917152405, train acc: 0.9097248941900731, val loss: 0.17149008810520172, val acc: 0.9240674955595026\n",
      "Epoch 150, train loss: 0.3296971917152405, train acc: 0.9097248941900731, val loss: 0.17149007320404053, val acc: 0.9240674955595026\n",
      "Epoch 200, train loss: 0.3296971917152405, train acc: 0.9097248941900731, val loss: 0.17149007320404053, val acc: 0.9240674955595026\n",
      "Value 's_m_75_val6: 0.9240674955595026 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3648436963558197, train acc: 0.9024960998439937, val loss: 0.187036395072937, val acc: 0.9166666666666666\n",
      "Epoch 50, train loss: 0.3648436963558197, train acc: 0.9024960998439937, val loss: 0.18223297595977783, val acc: 0.9175627240143369\n",
      "Epoch 100, train loss: 0.3648436963558197, train acc: 0.9024960998439937, val loss: 0.18220923840999603, val acc: 0.9175627240143369\n",
      "Epoch 150, train loss: 0.3648436963558197, train acc: 0.9024960998439937, val loss: 0.18220920860767365, val acc: 0.9175627240143369\n",
      "Epoch 200, train loss: 0.3648436963558197, train acc: 0.9024960998439937, val loss: 0.18220920860767365, val acc: 0.9175627240143369\n",
      "Value 's_m_80_val6: 0.9175627240143369 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set6_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.20485801994800568, train acc: 0.9335923217550274, val loss: 0.11679411679506302, val acc: 0.9619289340101523\n",
      "Epoch 50, train loss: 0.20485801994800568, train acc: 0.9335923217550274, val loss: 0.11211064457893372, val acc: 0.9585448392554992\n",
      "Epoch 100, train loss: 0.20485801994800568, train acc: 0.9335923217550274, val loss: 0.11212075501680374, val acc: 0.9585448392554992\n",
      "Epoch 150, train loss: 0.20485801994800568, train acc: 0.9335923217550274, val loss: 0.11212071776390076, val acc: 0.9585448392554992\n",
      "Epoch 200, train loss: 0.20485801994800568, train acc: 0.9335923217550274, val loss: 0.11212071776390076, val acc: 0.9585448392554992\n",
      "Value 's_m_55_val7: 0.9619289340101523 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.23746851086616516, train acc: 0.9259722222222222, val loss: 0.13624054193496704, val acc: 0.9539249146757679\n",
      "Epoch 50, train loss: 0.23746851086616516, train acc: 0.9259722222222222, val loss: 0.1348581612110138, val acc: 0.9547781569965871\n",
      "Epoch 100, train loss: 0.23746851086616516, train acc: 0.9259722222222222, val loss: 0.13485145568847656, val acc: 0.9547781569965871\n",
      "Epoch 150, train loss: 0.23746851086616516, train acc: 0.9259722222222222, val loss: 0.13485145568847656, val acc: 0.9547781569965871\n",
      "Epoch 200, train loss: 0.23746851086616516, train acc: 0.9259722222222222, val loss: 0.13485145568847656, val acc: 0.9547781569965871\n",
      "Value 's_m_60_val7: 0.9547781569965871 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2703116834163666, train acc: 0.9182457786116323, val loss: 0.16331005096435547, val acc: 0.9500860585197934\n",
      "Epoch 50, train loss: 0.2703116834163666, train acc: 0.9182457786116323, val loss: 0.16190531849861145, val acc: 0.9500860585197934\n",
      "Epoch 100, train loss: 0.2703116834163666, train acc: 0.9182457786116323, val loss: 0.16189856827259064, val acc: 0.9500860585197934\n",
      "Epoch 150, train loss: 0.2703116834163666, train acc: 0.9182457786116323, val loss: 0.16189856827259064, val acc: 0.9500860585197934\n",
      "Epoch 200, train loss: 0.2703116834163666, train acc: 0.9182457786116323, val loss: 0.16189856827259064, val acc: 0.9500860585197934\n",
      "Value 's_m_65_val7: 0.9500860585197934 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.30355504155158997, train acc: 0.9104562737642585, val loss: 0.1923251450061798, val acc: 0.9453125\n",
      "Epoch 50, train loss: 0.30355504155158997, train acc: 0.9104562737642585, val loss: 0.1907798945903778, val acc: 0.9453125\n",
      "Epoch 100, train loss: 0.30355504155158997, train acc: 0.9104562737642585, val loss: 0.19077253341674805, val acc: 0.9453125\n",
      "Epoch 150, train loss: 0.30355504155158997, train acc: 0.9104562737642585, val loss: 0.19077254831790924, val acc: 0.9453125\n",
      "Epoch 200, train loss: 0.30355504155158997, train acc: 0.9104562737642585, val loss: 0.19077254831790924, val acc: 0.9453125\n",
      "Value 's_m_70_val7: 0.9453125 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.337287038564682, train acc: 0.9028420038535645, val loss: 0.22270821034908295, val acc: 0.9404553415061296\n",
      "Epoch 50, train loss: 0.337287038564682, train acc: 0.9028420038535645, val loss: 0.2207510620355606, val acc: 0.9404553415061296\n",
      "Epoch 100, train loss: 0.337287038564682, train acc: 0.9028420038535645, val loss: 0.22074146568775177, val acc: 0.9404553415061296\n",
      "Epoch 150, train loss: 0.337287038564682, train acc: 0.9028420038535645, val loss: 0.22074146568775177, val acc: 0.9404553415061296\n",
      "Epoch 200, train loss: 0.337287038564682, train acc: 0.9028420038535645, val loss: 0.22074146568775177, val acc: 0.9404553415061296\n",
      "Value 's_m_75_val7: 0.9404553415061296 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3729337453842163, train acc: 0.8953125, val loss: 0.2524743676185608, val acc: 0.9355123674911661\n",
      "Epoch 50, train loss: 0.3729337453842163, train acc: 0.8953125, val loss: 0.2502313554286957, val acc: 0.9359540636042403\n",
      "Epoch 100, train loss: 0.3729337453842163, train acc: 0.8953125, val loss: 0.2502204179763794, val acc: 0.9359540636042403\n",
      "Epoch 150, train loss: 0.3729337453842163, train acc: 0.8953125, val loss: 0.2502204179763794, val acc: 0.9359540636042403\n",
      "Epoch 200, train loss: 0.3729337453842163, train acc: 0.8953125, val loss: 0.2502204179763794, val acc: 0.9359540636042403\n",
      "Value 's_m_80_val7: 0.9359540636042403 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set7_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1880529820919037, train acc: 0.9374563852058618, val loss: 0.35959210991859436, val acc: 0.8936170212765957\n",
      "Epoch 50, train loss: 0.1880529820919037, train acc: 0.9374563852058618, val loss: 0.34788021445274353, val acc: 0.898936170212766\n",
      "Epoch 100, train loss: 0.1880529820919037, train acc: 0.9374563852058618, val loss: 0.3478510081768036, val acc: 0.898936170212766\n",
      "Epoch 150, train loss: 0.1880529820919037, train acc: 0.9374563852058618, val loss: 0.34785109758377075, val acc: 0.898936170212766\n",
      "Epoch 200, train loss: 0.1880529820919037, train acc: 0.9374563852058618, val loss: 0.34785109758377075, val acc: 0.898936170212766\n",
      "Value 's_m_55_val8: 0.898936170212766 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.21787075698375702, train acc: 0.9302808194984105, val loss: 0.4118429720401764, val acc: 0.8858024691358025\n",
      "Epoch 50, train loss: 0.21787075698375702, train acc: 0.9302808194984105, val loss: 0.406591534614563, val acc: 0.8865740740740741\n",
      "Epoch 100, train loss: 0.21787075698375702, train acc: 0.9302808194984105, val loss: 0.4065661132335663, val acc: 0.8865740740740741\n",
      "Epoch 150, train loss: 0.21787075698375702, train acc: 0.9302808194984105, val loss: 0.40656614303588867, val acc: 0.8865740740740741\n",
      "Epoch 200, train loss: 0.21787075698375702, train acc: 0.9302808194984105, val loss: 0.40656614303588867, val acc: 0.8865740740740741\n",
      "Value 's_m_60_val8: 0.8865740740740741 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2477235198020935, train acc: 0.9232832618025751, val loss: 0.47908881306648254, val acc: 0.8730407523510971\n",
      "Epoch 50, train loss: 0.2477235198020935, train acc: 0.9232832618025751, val loss: 0.4729975461959839, val acc: 0.8746081504702194\n",
      "Epoch 100, train loss: 0.2477235198020935, train acc: 0.9232832618025751, val loss: 0.4729679822921753, val acc: 0.8746081504702194\n",
      "Epoch 150, train loss: 0.2477235198020935, train acc: 0.9232832618025751, val loss: 0.4729680120944977, val acc: 0.8746081504702194\n",
      "Epoch 200, train loss: 0.2477235198020935, train acc: 0.9232832618025751, val loss: 0.4729680120944977, val acc: 0.8746081504702194\n",
      "Value 's_m_65_val8: 0.8753918495297806 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.27770259976387024, train acc: 0.9166968489677653, val loss: 0.5491944551467896, val acc: 0.8606687898089171\n",
      "Epoch 50, train loss: 0.27770259976387024, train acc: 0.9166968489677653, val loss: 0.5426515340805054, val acc: 0.8606687898089171\n",
      "Epoch 100, train loss: 0.27770259976387024, train acc: 0.9166968489677653, val loss: 0.5426195859909058, val acc: 0.8606687898089171\n",
      "Epoch 150, train loss: 0.27770259976387024, train acc: 0.9166968489677653, val loss: 0.5426195859909058, val acc: 0.8606687898089171\n",
      "Epoch 200, train loss: 0.27770259976387024, train acc: 0.9166968489677653, val loss: 0.5426195859909058, val acc: 0.8606687898089171\n",
      "Value 's_m_70_val8: 0.8606687898089171 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3082987070083618, train acc: 0.9100788701393984, val loss: 0.620509922504425, val acc: 0.8462783171521036\n",
      "Epoch 50, train loss: 0.3082987070083618, train acc: 0.9100788701393984, val loss: 0.6131332516670227, val acc: 0.8462783171521036\n",
      "Epoch 100, train loss: 0.3082987070083618, train acc: 0.9100788701393984, val loss: 0.6130972504615784, val acc: 0.8462783171521036\n",
      "Epoch 150, train loss: 0.3082987070083618, train acc: 0.9100788701393984, val loss: 0.6130973100662231, val acc: 0.8462783171521036\n",
      "Epoch 200, train loss: 0.3082987070083618, train acc: 0.9100788701393984, val loss: 0.6130973100662231, val acc: 0.8462783171521036\n",
      "Value 's_m_75_val8: 0.8462783171521036 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3408105671405792, train acc: 0.9037997027127462, val loss: 0.6910971403121948, val acc: 0.8314144736842105\n",
      "Epoch 50, train loss: 0.3408105671405792, train acc: 0.9037997027127462, val loss: 0.6826801300048828, val acc: 0.8330592105263158\n",
      "Epoch 100, train loss: 0.3408105671405792, train acc: 0.9037997027127462, val loss: 0.6826385259628296, val acc: 0.8330592105263158\n",
      "Epoch 150, train loss: 0.3408105671405792, train acc: 0.9037997027127462, val loss: 0.6826384663581848, val acc: 0.8330592105263158\n",
      "Epoch 200, train loss: 0.3408105671405792, train acc: 0.9037997027127462, val loss: 0.6826384663581848, val acc: 0.8330592105263158\n",
      "Value 's_m_80_val8: 0.8330592105263158 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set8_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.20076730847358704, train acc: 0.9351079267186649, val loss: 0.15010829269886017, val acc: 0.9575729927007299\n",
      "Epoch 50, train loss: 0.20076730847358704, train acc: 0.9351079267186649, val loss: 0.14396847784519196, val acc: 0.9584854014598541\n",
      "Epoch 100, train loss: 0.20076730847358704, train acc: 0.9351079267186649, val loss: 0.14394688606262207, val acc: 0.9584854014598541\n",
      "Epoch 150, train loss: 0.20076730847358704, train acc: 0.9351079267186649, val loss: 0.1439467966556549, val acc: 0.9584854014598541\n",
      "Epoch 200, train loss: 0.20076730847358704, train acc: 0.9351079267186649, val loss: 0.1439467966556549, val acc: 0.9584854014598541\n",
      "Value 's_m_55_val9: 0.958941605839416 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2329481989145279, train acc: 0.9274297262539041, val loss: 0.17361608147621155, val acc: 0.9521178637200737\n",
      "Epoch 50, train loss: 0.2329481989145279, train acc: 0.9274297262539041, val loss: 0.17196838557720184, val acc: 0.9525782688766115\n",
      "Epoch 100, train loss: 0.2329481989145279, train acc: 0.9274297262539041, val loss: 0.17196020483970642, val acc: 0.9525782688766115\n",
      "Epoch 150, train loss: 0.2329481989145279, train acc: 0.9274297262539041, val loss: 0.17196020483970642, val acc: 0.9525782688766115\n",
      "Epoch 200, train loss: 0.2329481989145279, train acc: 0.9274297262539041, val loss: 0.17196020483970642, val acc: 0.9525782688766115\n",
      "Value 's_m_60_val9: 0.9525782688766115 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.26625287532806396, train acc: 0.9193653452447422, val loss: 0.1976853907108307, val acc: 0.9474907063197026\n",
      "Epoch 50, train loss: 0.26625287532806396, train acc: 0.9193653452447422, val loss: 0.19566412270069122, val acc: 0.9479553903345725\n",
      "Epoch 100, train loss: 0.26625287532806396, train acc: 0.9193653452447422, val loss: 0.19565404951572418, val acc: 0.9479553903345725\n",
      "Epoch 150, train loss: 0.26625287532806396, train acc: 0.9193653452447422, val loss: 0.195654034614563, val acc: 0.9479553903345725\n",
      "Epoch 200, train loss: 0.26625287532806396, train acc: 0.9193653452447422, val loss: 0.195654034614563, val acc: 0.9479553903345725\n",
      "Value 's_m_65_val9: 0.9479553903345725 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.30001750588417053, train acc: 0.9117952102583443, val loss: 0.2234221249818802, val acc: 0.9427767354596623\n",
      "Epoch 50, train loss: 0.30001750588417053, train acc: 0.9117952102583443, val loss: 0.22121945023536682, val acc: 0.9427767354596623\n",
      "Epoch 100, train loss: 0.30001750588417053, train acc: 0.9117952102583443, val loss: 0.22120840847492218, val acc: 0.9427767354596623\n",
      "Epoch 150, train loss: 0.30001750588417053, train acc: 0.9117952102583443, val loss: 0.221208393573761, val acc: 0.9427767354596623\n",
      "Epoch 200, train loss: 0.30001750588417053, train acc: 0.9117952102583443, val loss: 0.221208393573761, val acc: 0.9427767354596623\n",
      "Value 's_m_70_val9: 0.9427767354596623 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.33414188027381897, train acc: 0.904213644181158, val loss: 0.2502800226211548, val acc: 0.9375\n",
      "Epoch 50, train loss: 0.33414188027381897, train acc: 0.904213644181158, val loss: 0.2485145777463913, val acc: 0.9375\n",
      "Epoch 100, train loss: 0.33414188027381897, train acc: 0.904213644181158, val loss: 0.24850574135780334, val acc: 0.9375\n",
      "Epoch 150, train loss: 0.33414188027381897, train acc: 0.904213644181158, val loss: 0.24850574135780334, val acc: 0.9375\n",
      "Epoch 200, train loss: 0.33414188027381897, train acc: 0.904213644181158, val loss: 0.24850574135780334, val acc: 0.9375\n",
      "Value 's_m_75_val9: 0.9375 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3699738383293152, train acc: 0.8963780747627348, val loss: 0.27900779247283936, val acc: 0.9326003824091779\n",
      "Epoch 50, train loss: 0.3699738383293152, train acc: 0.8963780747627348, val loss: 0.27728113532066345, val acc: 0.9326003824091779\n",
      "Epoch 100, train loss: 0.3699738383293152, train acc: 0.8963780747627348, val loss: 0.27727261185646057, val acc: 0.9326003824091779\n",
      "Epoch 150, train loss: 0.3699738383293152, train acc: 0.8963780747627348, val loss: 0.27727261185646057, val acc: 0.9326003824091779\n",
      "Epoch 200, train loss: 0.3699738383293152, train acc: 0.8963780747627348, val loss: 0.27727261185646057, val acc: 0.9326003824091779\n",
      "Value 's_m_80_val9: 0.9326003824091779 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set9_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18387390673160553, train acc: 0.9377587991718427, val loss: 0.35748913884162903, val acc: 0.8867924528301887\n",
      "Epoch 50, train loss: 0.18387390673160553, train acc: 0.9377587991718427, val loss: 0.37853267788887024, val acc: 0.8839622641509434\n",
      "Epoch 100, train loss: 0.18387390673160553, train acc: 0.9377587991718427, val loss: 0.3786430060863495, val acc: 0.8839622641509434\n",
      "Epoch 150, train loss: 0.18387390673160553, train acc: 0.9377587991718427, val loss: 0.37864330410957336, val acc: 0.8839622641509434\n",
      "Epoch 200, train loss: 0.18387390673160553, train acc: 0.9377587991718427, val loss: 0.37864330410957336, val acc: 0.8839622641509434\n",
      "Value 's_m_55_val10: 0.8867924528301887 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.21446916460990906, train acc: 0.9304051694027244, val loss: 0.4296453297138214, val acc: 0.8721153846153846\n",
      "Epoch 50, train loss: 0.21446916460990906, train acc: 0.9304051694027244, val loss: 0.4237261414527893, val acc: 0.8721153846153846\n",
      "Epoch 100, train loss: 0.21446916460990906, train acc: 0.9304051694027244, val loss: 0.4236968457698822, val acc: 0.8721153846153846\n",
      "Epoch 150, train loss: 0.21446916460990906, train acc: 0.9304051694027244, val loss: 0.4236968457698822, val acc: 0.8721153846153846\n",
      "Epoch 200, train loss: 0.21446916460990906, train acc: 0.9304051694027244, val loss: 0.4236968457698822, val acc: 0.8721153846153846\n",
      "Value 's_m_60_val10: 0.8721153846153846 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24539940059185028, train acc: 0.923532531824611, val loss: 0.4807050824165344, val acc: 0.8598039215686275\n",
      "Epoch 50, train loss: 0.24539940059185028, train acc: 0.923532531824611, val loss: 0.47419947385787964, val acc: 0.8607843137254902\n",
      "Epoch 100, train loss: 0.24539940059185028, train acc: 0.923532531824611, val loss: 0.4741673469543457, val acc: 0.8607843137254902\n",
      "Epoch 150, train loss: 0.24539940059185028, train acc: 0.923532531824611, val loss: 0.4741672873497009, val acc: 0.8607843137254902\n",
      "Epoch 200, train loss: 0.24539940059185028, train acc: 0.923532531824611, val loss: 0.4741672873497009, val acc: 0.8607843137254902\n",
      "Value 's_m_65_val10: 0.8607843137254902 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2765403389930725, train acc: 0.9169351951306839, val loss: 0.5366955399513245, val acc: 0.848\n",
      "Epoch 50, train loss: 0.2765403389930725, train acc: 0.9169351951306839, val loss: 0.530082106590271, val acc: 0.849\n",
      "Epoch 100, train loss: 0.2765403389930725, train acc: 0.9169351951306839, val loss: 0.5300496816635132, val acc: 0.849\n",
      "Epoch 150, train loss: 0.2765403389930725, train acc: 0.9169351951306839, val loss: 0.5300496816635132, val acc: 0.849\n",
      "Epoch 200, train loss: 0.2765403389930725, train acc: 0.9169351951306839, val loss: 0.5300496816635132, val acc: 0.849\n",
      "Value 's_m_70_val10: 0.849 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.30825433135032654, train acc: 0.9103063814358231, val loss: 0.592940092086792, val acc: 0.8367346938775511\n",
      "Epoch 50, train loss: 0.30825433135032654, train acc: 0.9103063814358231, val loss: 0.586105465888977, val acc: 0.8377551020408164\n",
      "Epoch 100, train loss: 0.30825433135032654, train acc: 0.9103063814358231, val loss: 0.5860718488693237, val acc: 0.8377551020408164\n",
      "Epoch 150, train loss: 0.30825433135032654, train acc: 0.9103063814358231, val loss: 0.5860718488693237, val acc: 0.8377551020408164\n",
      "Epoch 200, train loss: 0.30825433135032654, train acc: 0.9103063814358231, val loss: 0.5860718488693237, val acc: 0.8377551020408164\n",
      "Value 's_m_75_val10: 0.8377551020408164 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3417294919490814, train acc: 0.9035530664708042, val loss: 0.6500556468963623, val acc: 0.8239583333333333\n",
      "Epoch 50, train loss: 0.3417294919490814, train acc: 0.9035530664708042, val loss: 0.6425804495811462, val acc: 0.8260416666666667\n",
      "Epoch 100, train loss: 0.3417294919490814, train acc: 0.9035530664708042, val loss: 0.6425434947013855, val acc: 0.8260416666666667\n",
      "Epoch 150, train loss: 0.3417294919490814, train acc: 0.9035530664708042, val loss: 0.6425434947013855, val acc: 0.8260416666666667\n",
      "Epoch 200, train loss: 0.3417294919490814, train acc: 0.9035530664708042, val loss: 0.6425434947013855, val acc: 0.8260416666666667\n",
      "Value 's_m_80_val10: 0.8260416666666667 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set10_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19186824560165405, train acc: 0.9365599860090941, val loss: 0.2021622508764267, val acc: 0.9183673469387755\n",
      "Epoch 50, train loss: 0.19186824560165405, train acc: 0.9365599860090941, val loss: 0.21558460593223572, val acc: 0.9139941690962099\n",
      "Epoch 100, train loss: 0.19186824560165405, train acc: 0.9365599860090941, val loss: 0.21565647423267365, val acc: 0.9139941690962099\n",
      "Epoch 150, train loss: 0.19186824560165405, train acc: 0.9365599860090941, val loss: 0.2156566083431244, val acc: 0.9139941690962099\n",
      "Epoch 200, train loss: 0.19186824560165405, train acc: 0.9365599860090941, val loss: 0.2156566083431244, val acc: 0.9139941690962099\n",
      "Value 's_m_55_val11: 0.9183673469387755 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.22189095616340637, train acc: 0.9293112606232294, val loss: 0.27093416452407837, val acc: 0.9016272189349113\n",
      "Epoch 50, train loss: 0.22189095616340637, train acc: 0.9293112606232294, val loss: 0.26754042506217957, val acc: 0.9016272189349113\n",
      "Epoch 100, train loss: 0.22189095616340637, train acc: 0.9293112606232294, val loss: 0.2675233781337738, val acc: 0.9016272189349113\n",
      "Epoch 150, train loss: 0.22189095616340637, train acc: 0.9293112606232294, val loss: 0.26752331852912903, val acc: 0.9016272189349113\n",
      "Epoch 200, train loss: 0.22189095616340637, train acc: 0.9293112606232294, val loss: 0.26752331852912903, val acc: 0.9016272189349113\n",
      "Value 's_m_60_val11: 0.9016272189349113 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2532799541950226, train acc: 0.922508067407673, val loss: 0.3099411129951477, val acc: 0.8888888888888888\n",
      "Epoch 50, train loss: 0.2532799541950226, train acc: 0.922508067407673, val loss: 0.3059266209602356, val acc: 0.8888888888888888\n",
      "Epoch 100, train loss: 0.2532799541950226, train acc: 0.922508067407673, val loss: 0.3059064745903015, val acc: 0.8888888888888888\n",
      "Epoch 150, train loss: 0.2532799541950226, train acc: 0.922508067407673, val loss: 0.3059064745903015, val acc: 0.8888888888888888\n",
      "Epoch 200, train loss: 0.2532799541950226, train acc: 0.922508067407673, val loss: 0.3059064745903015, val acc: 0.8888888888888888\n",
      "Value 's_m_65_val11: 0.8888888888888888 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.28620466589927673, train acc: 0.915622730573711, val loss: 0.32981839776039124, val acc: 0.8795731707317073\n",
      "Epoch 50, train loss: 0.28620466589927673, train acc: 0.915622730573711, val loss: 0.3251914978027344, val acc: 0.8810975609756098\n",
      "Epoch 100, train loss: 0.28620466589927673, train acc: 0.915622730573711, val loss: 0.325168251991272, val acc: 0.8810975609756098\n",
      "Epoch 150, train loss: 0.28620466589927673, train acc: 0.915622730573711, val loss: 0.325168251991272, val acc: 0.8810975609756098\n",
      "Epoch 200, train loss: 0.28620466589927673, train acc: 0.915622730573711, val loss: 0.325168251991272, val acc: 0.8810975609756098\n",
      "Value 's_m_70_val11: 0.8810975609756098 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3199777901172638, train acc: 0.9086980507539537, val loss: 0.3439076542854309, val acc: 0.8738390092879257\n",
      "Epoch 50, train loss: 0.3199777901172638, train acc: 0.9086980507539537, val loss: 0.3390360474586487, val acc: 0.8746130030959752\n",
      "Epoch 100, train loss: 0.3199777901172638, train acc: 0.9086980507539537, val loss: 0.33901169896125793, val acc: 0.8746130030959752\n",
      "Epoch 150, train loss: 0.3199777901172638, train acc: 0.9086980507539537, val loss: 0.33901169896125793, val acc: 0.8746130030959752\n",
      "Epoch 200, train loss: 0.3199777901172638, train acc: 0.9086980507539537, val loss: 0.33901169896125793, val acc: 0.8746130030959752\n",
      "Value 's_m_75_val11: 0.8746130030959752 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3555188477039337, train acc: 0.9016859165424739, val loss: 0.3585532307624817, val acc: 0.8694968553459119\n",
      "Epoch 50, train loss: 0.3555188477039337, train acc: 0.9016859165424739, val loss: 0.3531440794467926, val acc: 0.8710691823899371\n",
      "Epoch 100, train loss: 0.3555188477039337, train acc: 0.9016859165424739, val loss: 0.35311710834503174, val acc: 0.8710691823899371\n",
      "Epoch 150, train loss: 0.3555188477039337, train acc: 0.9016859165424739, val loss: 0.35311710834503174, val acc: 0.8710691823899371\n",
      "Epoch 200, train loss: 0.3555188477039337, train acc: 0.9016859165424739, val loss: 0.35311710834503174, val acc: 0.8710691823899371\n",
      "Value 's_m_80_val11: 0.8710691823899371 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set11_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18366916477680206, train acc: 0.9380853277835588, val loss: 0.3420095145702362, val acc: 0.902542372881356\n",
      "Epoch 50, train loss: 0.18366916477680206, train acc: 0.9380853277835588, val loss: 0.34528088569641113, val acc: 0.9\n",
      "Epoch 100, train loss: 0.18366916477680206, train acc: 0.9380853277835588, val loss: 0.3452894985675812, val acc: 0.9\n",
      "Epoch 150, train loss: 0.18366916477680206, train acc: 0.9380853277835588, val loss: 0.34528928995132446, val acc: 0.9\n",
      "Epoch 200, train loss: 0.18366916477680206, train acc: 0.9380853277835588, val loss: 0.34528928995132446, val acc: 0.9\n",
      "Value 's_m_55_val12: 0.902542372881356 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.21439434587955475, train acc: 0.9306969803370787, val loss: 0.3938333988189697, val acc: 0.8896551724137931\n",
      "Epoch 50, train loss: 0.21439434587955475, train acc: 0.9306969803370787, val loss: 0.38847190141677856, val acc: 0.8905172413793103\n",
      "Epoch 100, train loss: 0.21439434587955475, train acc: 0.9306969803370787, val loss: 0.38844528794288635, val acc: 0.8905172413793103\n",
      "Epoch 150, train loss: 0.21439434587955475, train acc: 0.9306969803370787, val loss: 0.38844525814056396, val acc: 0.8905172413793103\n",
      "Epoch 200, train loss: 0.21439434587955475, train acc: 0.9306969803370787, val loss: 0.38844525814056396, val acc: 0.8905172413793103\n",
      "Value 's_m_60_val12: 0.8905172413793103 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2457450032234192, train acc: 0.9236580163526484, val loss: 0.43769484758377075, val acc: 0.8798245614035087\n",
      "Epoch 50, train loss: 0.2457450032234192, train acc: 0.9236580163526484, val loss: 0.4318982660770416, val acc: 0.8824561403508772\n",
      "Epoch 100, train loss: 0.2457450032234192, train acc: 0.9236580163526484, val loss: 0.4318695366382599, val acc: 0.8824561403508772\n",
      "Epoch 150, train loss: 0.2457450032234192, train acc: 0.9236580163526484, val loss: 0.4318695664405823, val acc: 0.8824561403508772\n",
      "Epoch 200, train loss: 0.2457450032234192, train acc: 0.9236580163526484, val loss: 0.4318695664405823, val acc: 0.8824561403508772\n",
      "Value 's_m_65_val12: 0.8824561403508772 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.27758076786994934, train acc: 0.9167566594672426, val loss: 0.48086291551589966, val acc: 0.8714285714285714\n",
      "Epoch 50, train loss: 0.27758076786994934, train acc: 0.9167566594672426, val loss: 0.47510772943496704, val acc: 0.8723214285714286\n",
      "Epoch 100, train loss: 0.27758076786994934, train acc: 0.9167566594672426, val loss: 0.47507935762405396, val acc: 0.8723214285714286\n",
      "Epoch 150, train loss: 0.27758076786994934, train acc: 0.9167566594672426, val loss: 0.47507935762405396, val acc: 0.8723214285714286\n",
      "Epoch 200, train loss: 0.27758076786994934, train acc: 0.9167566594672426, val loss: 0.47507935762405396, val acc: 0.8723214285714286\n",
      "Value 's_m_70_val12: 0.8723214285714286 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3098479211330414, train acc: 0.9099070360918702, val loss: 0.5282583236694336, val acc: 0.860909090909091\n",
      "Epoch 50, train loss: 0.3098479211330414, train acc: 0.9099070360918702, val loss: 0.5223562717437744, val acc: 0.8627272727272727\n",
      "Epoch 100, train loss: 0.3098479211330414, train acc: 0.9099070360918702, val loss: 0.522327184677124, val acc: 0.8627272727272727\n",
      "Epoch 150, train loss: 0.3098479211330414, train acc: 0.9099070360918702, val loss: 0.5223271250724792, val acc: 0.8627272727272727\n",
      "Epoch 200, train loss: 0.3098479211330414, train acc: 0.9099070360918702, val loss: 0.5223271250724792, val acc: 0.8627272727272727\n",
      "Value 's_m_75_val12: 0.8627272727272727 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3440011441707611, train acc: 0.9029265140324964, val loss: 0.5749085545539856, val acc: 0.850925925925926\n",
      "Epoch 50, train loss: 0.3440011441707611, train acc: 0.9029265140324964, val loss: 0.5685387253761292, val acc: 0.8518518518518519\n",
      "Epoch 100, train loss: 0.3440011441707611, train acc: 0.9029265140324964, val loss: 0.5685073733329773, val acc: 0.8518518518518519\n",
      "Epoch 150, train loss: 0.3440011441707611, train acc: 0.9029265140324964, val loss: 0.5685073733329773, val acc: 0.8518518518518519\n",
      "Epoch 200, train loss: 0.3440011441707611, train acc: 0.9029265140324964, val loss: 0.5685073733329773, val acc: 0.8518518518518519\n",
      "Value 's_m_80_val12: 0.8518518518518519 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set12_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19216974079608917, train acc: 0.9359809792180345, val loss: 0.22949402034282684, val acc: 0.9197127937336814\n",
      "Epoch 50, train loss: 0.19216974079608917, train acc: 0.9359809792180345, val loss: 0.22331149876117706, val acc: 0.9210182767624021\n",
      "Epoch 100, train loss: 0.19216974079608917, train acc: 0.9359809792180345, val loss: 0.22330552339553833, val acc: 0.9210182767624021\n",
      "Epoch 150, train loss: 0.19216974079608917, train acc: 0.9359809792180345, val loss: 0.22330541908740997, val acc: 0.9210182767624021\n",
      "Epoch 200, train loss: 0.19216974079608917, train acc: 0.9359809792180345, val loss: 0.22330541908740997, val acc: 0.9210182767624021\n",
      "Value 's_m_55_val13: 0.9216710182767625 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.22300143539905548, train acc: 0.9285841654778887, val loss: 0.2643425464630127, val acc: 0.91005291005291\n",
      "Epoch 50, train loss: 0.22300143539905548, train acc: 0.9285841654778887, val loss: 0.2620812952518463, val acc: 0.91005291005291\n",
      "Epoch 100, train loss: 0.22300143539905548, train acc: 0.9285841654778887, val loss: 0.2620704174041748, val acc: 0.91005291005291\n",
      "Epoch 150, train loss: 0.22300143539905548, train acc: 0.9285841654778887, val loss: 0.2620704174041748, val acc: 0.91005291005291\n",
      "Epoch 200, train loss: 0.22300143539905548, train acc: 0.9285841654778887, val loss: 0.2620704174041748, val acc: 0.91005291005291\n",
      "Value 's_m_60_val13: 0.91005291005291 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.25433072447776794, train acc: 0.9219483568075117, val loss: 0.3046136498451233, val acc: 0.8987935656836461\n",
      "Epoch 50, train loss: 0.25433072447776794, train acc: 0.9219483568075117, val loss: 0.3019699156284332, val acc: 0.8987935656836461\n",
      "Epoch 100, train loss: 0.25433072447776794, train acc: 0.9219483568075117, val loss: 0.3019571900367737, val acc: 0.8987935656836461\n",
      "Epoch 150, train loss: 0.25433072447776794, train acc: 0.9219483568075117, val loss: 0.3019571900367737, val acc: 0.8987935656836461\n",
      "Epoch 200, train loss: 0.25433072447776794, train acc: 0.9219483568075117, val loss: 0.3019571900367737, val acc: 0.8987935656836461\n",
      "Value 's_m_65_val13: 0.8987935656836461 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.28591179847717285, train acc: 0.915462692026335, val loss: 0.34764429926872253, val acc: 0.8872282608695652\n",
      "Epoch 50, train loss: 0.28591179847717285, train acc: 0.915462692026335, val loss: 0.3441677987575531, val acc: 0.8872282608695652\n",
      "Epoch 100, train loss: 0.28591179847717285, train acc: 0.915462692026335, val loss: 0.3441508412361145, val acc: 0.8872282608695652\n",
      "Epoch 150, train loss: 0.28591179847717285, train acc: 0.915462692026335, val loss: 0.3441508412361145, val acc: 0.8872282608695652\n",
      "Epoch 200, train loss: 0.28591179847717285, train acc: 0.915462692026335, val loss: 0.3441508412361145, val acc: 0.8872282608695652\n",
      "Value 's_m_70_val13: 0.8872282608695652 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3180682957172394, train acc: 0.9088088180807706, val loss: 0.3897397518157959, val acc: 0.8753443526170799\n",
      "Epoch 50, train loss: 0.3180682957172394, train acc: 0.9088088180807706, val loss: 0.38553306460380554, val acc: 0.8753443526170799\n",
      "Epoch 100, train loss: 0.3180682957172394, train acc: 0.9088088180807706, val loss: 0.38551247119903564, val acc: 0.8753443526170799\n",
      "Epoch 150, train loss: 0.3180682957172394, train acc: 0.9088088180807706, val loss: 0.38551247119903564, val acc: 0.8753443526170799\n",
      "Epoch 200, train loss: 0.3180682957172394, train acc: 0.9088088180807706, val loss: 0.38551247119903564, val acc: 0.8753443526170799\n",
      "Value 's_m_75_val13: 0.8753443526170799 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.35112327337265015, train acc: 0.9021208708708709, val loss: 0.4452761113643646, val acc: 0.8631284916201117\n",
      "Epoch 50, train loss: 0.35112327337265015, train acc: 0.9021208708708709, val loss: 0.4405566453933716, val acc: 0.8631284916201117\n",
      "Epoch 100, train loss: 0.35112327337265015, train acc: 0.9021208708708709, val loss: 0.4405333995819092, val acc: 0.8631284916201117\n",
      "Epoch 150, train loss: 0.35112327337265015, train acc: 0.9021208708708709, val loss: 0.4405333995819092, val acc: 0.8631284916201117\n",
      "Epoch 200, train loss: 0.35112327337265015, train acc: 0.9021208708708709, val loss: 0.4405333995819092, val acc: 0.8631284916201117\n",
      "Value 's_m_80_val13: 0.8631284916201117 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set13_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1907631903886795, train acc: 0.935783089546914, val loss: 0.22357948124408722, val acc: 0.9483516483516483\n",
      "Epoch 50, train loss: 0.1907631903886795, train acc: 0.935783089546914, val loss: 0.22891797125339508, val acc: 0.945054945054945\n",
      "Epoch 100, train loss: 0.1907631903886795, train acc: 0.935783089546914, val loss: 0.22896155714988708, val acc: 0.945054945054945\n",
      "Epoch 150, train loss: 0.1907631903886795, train acc: 0.935783089546914, val loss: 0.2289617359638214, val acc: 0.945054945054945\n",
      "Epoch 200, train loss: 0.1907631903886795, train acc: 0.935783089546914, val loss: 0.2289617359638214, val acc: 0.945054945054945\n",
      "Value 's_m_55_val14: 0.9483516483516483 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2198636680841446, train acc: 0.9284682080924855, val loss: 0.27791160345077515, val acc: 0.9361111111111111\n",
      "Epoch 50, train loss: 0.2198636680841446, train acc: 0.9284682080924855, val loss: 0.27646854519844055, val acc: 0.9377777777777778\n",
      "Epoch 100, train loss: 0.2198636680841446, train acc: 0.9284682080924855, val loss: 0.2764614522457123, val acc: 0.9377777777777778\n",
      "Epoch 150, train loss: 0.2198636680841446, train acc: 0.9284682080924855, val loss: 0.2764614522457123, val acc: 0.9377777777777778\n",
      "Epoch 200, train loss: 0.2198636680841446, train acc: 0.9284682080924855, val loss: 0.2764614522457123, val acc: 0.9377777777777778\n",
      "Value 's_m_60_val14: 0.9377777777777778 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24909983575344086, train acc: 0.921560556165386, val loss: 0.3277307450771332, val acc: 0.9292134831460674\n",
      "Epoch 50, train loss: 0.24909983575344086, train acc: 0.921560556165386, val loss: 0.32622379064559937, val acc: 0.9314606741573034\n",
      "Epoch 100, train loss: 0.24909983575344086, train acc: 0.921560556165386, val loss: 0.3262164890766144, val acc: 0.9314606741573034\n",
      "Epoch 150, train loss: 0.24909983575344086, train acc: 0.921560556165386, val loss: 0.3262164890766144, val acc: 0.9314606741573034\n",
      "Epoch 200, train loss: 0.24909983575344086, train acc: 0.921560556165386, val loss: 0.3262164890766144, val acc: 0.9314606741573034\n",
      "Value 's_m_65_val14: 0.9314606741573034 ' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/set14_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.27820834517478943, train acc: 0.915261304670126, val loss: 0.38137608766555786, val acc: 0.9221590909090909\n",
      "Epoch 50, train loss: 0.27820834517478943, train acc: 0.915261304670126, val loss: 0.3794110417366028, val acc: 0.9238636363636363\n",
      "Epoch 100, train loss: 0.27820834517478943, train acc: 0.915261304670126, val loss: 0.3794013559818268, val acc: 0.9238636363636363\n",
      "Epoch 150, train loss: 0.27820834517478943, train acc: 0.915261304670126, val loss: 0.3794013559818268, val acc: 0.9238636363636363\n"
     ]
    }
   ],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(55,81,step=5):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted, val_predicted, train_probs, val_probs= run_training(\n",
    "            train_dataloader, val_dataloader=val_dataloader, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "        file_name = 'set'+str(j)+'_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(max(val_accs))+\" \"  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
