{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If GPU is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)\n",
    "        #Xavier initialization for GRU weights\n",
    "        #for name, param in self.gru.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        init.xavier_uniform_(param.data)\n",
    "        #    elif 'bias' in name:\n",
    "        #        init.constant_(param.data, 0.0)\n",
    "                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sotfplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.sotfplus(out[:,-1]))\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out, h\n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "        #weight = next(self.parameters()).data\n",
    "        #hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        #return hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size > 1:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None, hidden_states=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "\n",
    "        hidden_states = []  # Initialize a list to store hidden states at each time step\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]\n",
    "\n",
    "            hidden_states.append(new_hidden_states)  # Append hidden states for the current time step\n",
    "\n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "\n",
    "        # Return the output and the list of hidden states for the entire sequence\n",
    "        return out, last_hidden_states, hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states_batch = []  # Initialize a list to store hidden states for the current batch        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        # Append the batch's hidden states to the list\n",
    "        for layer_state in last_hidden_states:\n",
    "            hidden_states_batch.append(layer_state.cpu().detach().numpy())\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        # Append the batch's hidden states to the overall hidden_states list\n",
    "        hidden_states.append(hidden_states_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step(loss)\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            \n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "        # Filter the DataFrame for the current trial\n",
    "        trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "        # Create shifted columns for each column in columns_to_shift\n",
    "        for col in columns_to_shift:\n",
    "            new_col_name = col + '_minus_' + str(shift)\n",
    "            trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "        # Drop the last 'i' records for each trial\n",
    "        trial_df = trial_df.dropna()\n",
    "\n",
    "        # Append the modified trial_df to the shifted_df\n",
    "        shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "    \n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['H_trial','H_id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['H_trial','H_id','s_1','s_2','s_3','s_4']] = df[['H_trial','H_id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['H_trial','H_id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,152,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['H_trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['H_trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['H_id', 'H_trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['H_trial']!=set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        test_set = shifted_df[shifted_df['H_trial']==set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        full_set = shifted_df.drop(columns=['H_id', 'H_trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished smh, left set 14,15 to train \n",
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            \n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "df.rename(columns={'H_id': 'id', 'H_trial': 'trial'}, inplace=True)\n",
    "data_to_scale = df.drop(columns=['trial','id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['trial','id','s_1','s_2','s_3','s_4']] = df[['trial','id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['trial','id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id','trial'])\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sh_overfitting_check.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = df.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "shuffled_df = pd.concat(shuffled_trial_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shift = 2\n",
    "set_values = 5\n",
    "model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the shifted data\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "# Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "    # Filter the DataFrame for the current trial\n",
    "    trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "    # Create shifted columns for each column in columns_to_shift\n",
    "    for col in columns_to_shift:\n",
    "        new_col_name = col + '_minus_' + str(shift)\n",
    "        trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "    # Drop the last 'i' records for each trial\n",
    "    trial_df = trial_df.dropna()\n",
    "\n",
    "    # Append the modified trial_df to the shifted_df\n",
    "    shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "\n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame 'train_set' with a 'trial' column\n",
    "# and you want to shuffle the trials while preserving the order of samples within each trial.\n",
    "\n",
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = train_set.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "train_set = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(1,142,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                        # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "        \n",
    "        X_train = X_train.drop(columns=['id','trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sm_shuffled.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_to_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "        # Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "        X_train = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "        X_train = train_set.drop(columns=['id', 'trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        X_train = X_train.drop(columns=labels)\n",
    "        \n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        #file_name = 'smh_shuffled.txt'\n",
    "        #file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('models_comparison_v3.csv')\n",
    "\n",
    "# Set the \"shift\" column as the index (optional, for better x-axis labels)\n",
    "df = df[::-1]\n",
    "df.set_index('T', inplace=True)\n",
    "\n",
    "# Create a line plot for each model's performance\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define custom colors for each model\n",
    "colors = {\n",
    "    'null': 'silver',\n",
    "    'solo': 'magenta',\n",
    "    'dyadic': 'blue'\n",
    "}\n",
    "\n",
    "# Create a line plot for each model's performance with custom colors\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for model in ['null','solo','dyadic']:\n",
    "    plt.plot(df.index, df[model], marker='o', label=model, color=colors[model])\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlim(max(df.index),min(df.index))\n",
    "plt.xlabel('T (shift in seconds)')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.title('Performance of Four Models')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1','2','3','4']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap plot\n",
    "heatmap = ax.imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Predicted State')\n",
    "ax.set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels\n",
    "ax.set_yticks(np.arange(len(y_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Create the heatmap plot in the first subplot\n",
    "heatmap = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_xlabel('Time Step')\n",
    "axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second plot (line plot) in the second subplot\n",
    "axs[1].plot(np.arange(len(y_val)), y_val, color='green', label='Actual Values')\n",
    "axs[1].set_xlabel('Time Step')\n",
    "axs[1].set_ylabel('Actual Value')\n",
    "axs[1].set_title('Actual Test Values for Y')\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_val\n",
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2,1, figsize=(8,10))\n",
    "\n",
    "# Create the first heatmap plot in the first subplot\n",
    "heatmap1 = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the first heatmap subplot\n",
    "cbar1 = plt.colorbar(heatmap1, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the first heatmap subplot\n",
    "#axs[0].set_xlabel('Time Step')\n",
    "#axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Predicted States Probabilities')\n",
    "\n",
    "# Set the y-axis ticks and labels for the first heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second heatmap plot in the second subplot\n",
    "heatmap2 = axs[1].imshow(actual_values.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the second heatmap subplot\n",
    "cbar2 = plt.colorbar(heatmap2, ax=axs[1])\n",
    "\n",
    "# Set the axis labels and title for the second heatmap subplot\n",
    "axs[1].set_xlabel('Time Step')\n",
    "#axs[1].set_ylabel('Predicted State')\n",
    "axs[1].set_title('True States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the second heatmap subplot\n",
    "axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "axs[1].set_yticklabels(y_labels)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gates and hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None, hidden_states=None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_dim)  # Initialize hidden state for the single layer\n",
    "\n",
    "        hidden_states = []  # Initialize a list to store hidden states at each time step\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            h = self.gru(input_t, h)  # Update the hidden state for the single layer\n",
    "            hidden_states.append(h)  # Append the hidden state for the current time step\n",
    "\n",
    "        h = self.batch_norm(h)  # Apply batch normalization to the final hidden state\n",
    "        out = self.fc(self.softplus(h))  # Calculate the output based on the final hidden state\n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "\n",
    "        # Return the output and the list of hidden states for the entire sequence\n",
    "        return out, h, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "            \n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    train_hidden_states, val_hidden_states = [], [] # Initialize lists to store hidden states\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs, train_hidden = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        train_hidden_states.extend(train_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs, val_hidden = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            val_hidden_states.extend(val_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final, train_hidden_states, val_hidden_states\n",
    "    else: \n",
    "        return train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y', 'headTop_z',\n",
       "       'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z',\n",
       "       'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x', 'lShoulder_y',\n",
       "       'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x',\n",
       "       'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z', 'rEar_x',\n",
       "       'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y', 'rShoulder_z',\n",
       "       'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x', 'rWrist_y', 'rWrist_z',\n",
       "       'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, batch_hidden_states = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hidden_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_hidden_states[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array = batch_hidden_states[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs.imshow(hidden_states_array, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs.set_ylabel('hidden unit')\n",
    "axs.set_xlabel('time step')\n",
    "axs.set_title('Heatmap of hidden states')\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_values = np.array([10,80,130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.99318927526474, train acc: 0.6004935459377373\n",
      "Epoch 50, train loss: 0.08497070521116257, train acc: 0.9844343204252088\n",
      "Epoch 0, train loss: 0.43113425374031067, train acc: 0.8743532560214095\n",
      "Epoch 50, train loss: 0.18211303651332855, train acc: 0.9378679750223016\n",
      "Epoch 0, train loss: 0.3486967086791992, train acc: 0.8874617737003058\n",
      "Epoch 50, train loss: 0.2525496780872345, train acc: 0.904638124362895\n",
      "Epoch 0, train loss: 0.43490543961524963, train acc: 0.9182550115018074\n",
      "Epoch 50, train loss: 0.07446524500846863, train acc: 0.9850065724613868\n",
      "Epoch 0, train loss: 0.3527289927005768, train acc: 0.8772032902467685\n",
      "Epoch 50, train loss: 0.20687879621982574, train acc: 0.9199471210340776\n",
      "Epoch 0, train loss: 0.3726874589920044, train acc: 0.8681343622333182\n",
      "Epoch 50, train loss: 0.34488749504089355, train acc: 0.8746028143440763\n",
      "Epoch 0, train loss: 0.20412753522396088, train acc: 0.9521866582158136\n",
      "Epoch 50, train loss: 0.16878153383731842, train acc: 0.9604262398985898\n",
      "Epoch 0, train loss: 0.1832096129655838, train acc: 0.9315794410054399\n",
      "Epoch 50, train loss: 0.18222378194332123, train acc: 0.9335490527105609\n",
      "Epoch 0, train loss: 0.3256382346153259, train acc: 0.876592528611531\n",
      "Epoch 50, train loss: 0.3240588307380676, train acc: 0.877780177067588\n",
      "Epoch 0, train loss: 0.16090203821659088, train acc: 0.9623314829500397\n",
      "Epoch 50, train loss: 0.16017913818359375, train acc: 0.9624504361617764\n",
      "Epoch 0, train loss: 0.18131472170352936, train acc: 0.9329107981220657\n",
      "Epoch 50, train loss: 0.18129833042621613, train acc: 0.9329107981220657\n",
      "Epoch 0, train loss: 0.3272723853588104, train acc: 0.8788648648648648\n",
      "Epoch 50, train loss: 0.3272233009338379, train acc: 0.8788648648648648\n",
      "Epoch 0, train loss: 0.17304927110671997, train acc: 0.956870043696391\n",
      "Epoch 50, train loss: 0.17301636934280396, train acc: 0.956870043696391\n",
      "Epoch 0, train loss: 0.18730944395065308, train acc: 0.9333044816310829\n",
      "Epoch 50, train loss: 0.18730823695659637, train acc: 0.9333044816310829\n",
      "Epoch 0, train loss: 0.33237120509147644, train acc: 0.8749722160480107\n",
      "Epoch 50, train loss: 0.33236998319625854, train acc: 0.8749722160480107\n",
      "Epoch 0, train loss: 0.1716112494468689, train acc: 0.9597658808120497\n",
      "Epoch 50, train loss: 0.17161104083061218, train acc: 0.9597658808120497\n",
      "Epoch 0, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 50, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 0, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 50, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 0, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 50, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 0, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 50, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 0, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 50, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 0, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 50, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 0, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 50, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 0, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 50, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 0, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 50, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 0, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 50, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 0, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 50, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 0, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 50, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 0, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 50, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 0, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 50, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 0, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 50, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 0, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 50, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 0, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 50, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 0, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 50, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 0, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 50, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 0, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 50, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 0, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 50, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 0, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 50, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 0, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 50, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 0, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 50, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 0, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 50, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 0, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 50, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 0, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 50, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 0, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 50, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 0, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n",
      "Epoch 50, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(1,16):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        \n",
    "        # Create a figure with a single subplot\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(16,10))  # Adjust the figsize as needed\n",
    "\n",
    "        # Create the heatmap plot in the single subplot\n",
    "        heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('hidden unit')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        axs[0].set_title('Heatmap of hidden states for s='+str(i)+' trial='+str(j))\n",
    "        \n",
    "        axs[1].imshow(all_probs_array.T, cmap='seismic', aspect='auto', interpolation='none')\n",
    "        y_labels = ['1', '2', '3', '4']\n",
    "        axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "        axs[1].set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "        plot_title = 'solo_s'+str(i)+'_val'+str(j)\n",
    "        #plt.title(plot_title)\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('predictions')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        #axs[0].set_title()\n",
    "        # Adjust spacing between the subplot\n",
    "        plt.tight_layout()\n",
    "        # Show the plot\n",
    "        # Specify the file path\n",
    "        # Specify the directory where you want to save the plots\n",
    "        save_directory = r'C:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\hidden_states_heatmaps_train'\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Define the plot title\n",
    "        plot_title = f'solo_s{i}_val{j}'\n",
    "\n",
    "        # Specify the file path using os.path.join\n",
    "        file_path = os.path.join(save_directory, f'{plot_title}.png')\n",
    "\n",
    "        # Save the plot to the specified path\n",
    "        fig.savefig(file_path)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 50, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 0, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 50, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 0, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 50, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 0, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 50, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 0, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 50, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 0, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n",
      "Epoch 50, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(13,15):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "        h_name = 'hidden_s'+str(i)+'_trial'+str(j)\n",
    "        p_name = 'probs_s'+str(i)+'_trial'+str(j)\n",
    "        np.save(h_name, hidden_states_array)\n",
    "        np.save(p_name, all_probs_array)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hidden_states.npy', hidden_states_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_value = list(hidden_states_dict.values())[0]\n",
    "type(first_value)\n",
    "hidden_states_array = first_value[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_ylabel('hidden unit')\n",
    "axs[0].set_xlabel('time step')\n",
    "axs[0].set_title('Heatmap of hidden states')\n",
    "\n",
    "\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y', 'headTop_z',\n",
       "       'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z',\n",
       "       'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x', 'lShoulder_y',\n",
       "       'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x',\n",
       "       'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z', 'rEar_x',\n",
       "       'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y', 'rShoulder_z',\n",
       "       'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x', 'rWrist_y', 'rWrist_z',\n",
       "       'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(13,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['nose_x_minus_80', 'nose_y_minus_80', 'nose_z_minus_80',\n",
      "       'headTop_x_minus_80', 'headTop_y_minus_80', 'headTop_z_minus_80',\n",
      "       'neck_x_minus_80', 'neck_y_minus_80', 'neck_z_minus_80',\n",
      "       'tailBase_x_minus_80', 'tailBase_y_minus_80', 'tailBase_z_minus_80',\n",
      "       'lEar_x_minus_80', 'lEar_y_minus_80', 'lEar_z_minus_80',\n",
      "       'lShoulder_x_minus_80', 'lShoulder_y_minus_80', 'lShoulder_z_minus_80',\n",
      "       'lElbow_x_minus_80', 'lElbow_y_minus_80', 'lElbow_z_minus_80',\n",
      "       'lWrist_x_minus_80', 'lWrist_y_minus_80', 'lWrist_z_minus_80',\n",
      "       'lHip_x_minus_80', 'lHip_y_minus_80', 'lHip_z_minus_80',\n",
      "       'rEar_x_minus_80', 'rEar_y_minus_80', 'rEar_z_minus_80',\n",
      "       'rShoulder_x_minus_80', 'rShoulder_y_minus_80', 'rShoulder_z_minus_80',\n",
      "       'rElbow_x_minus_80', 'rElbow_y_minus_80', 'rElbow_z_minus_80',\n",
      "       'rWrist_x_minus_80', 'rWrist_y_minus_80', 'rWrist_z_minus_80',\n",
      "       'rHip_x_minus_80', 'rHip_y_minus_80', 'rHip_z_minus_80', 's_1_minus_80',\n",
      "       's_2_minus_80', 's_3_minus_80', 's_4_minus_80'],\n",
      "      dtype='object')\n",
      "Index(['nose_x_minus_80', 'nose_y_minus_80', 'nose_z_minus_80',\n",
      "       'headTop_x_minus_80', 'headTop_y_minus_80', 'headTop_z_minus_80',\n",
      "       'neck_x_minus_80', 'neck_y_minus_80', 'neck_z_minus_80',\n",
      "       'tailBase_x_minus_80', 'tailBase_y_minus_80', 'tailBase_z_minus_80',\n",
      "       'lEar_x_minus_80', 'lEar_y_minus_80', 'lEar_z_minus_80',\n",
      "       'lShoulder_x_minus_80', 'lShoulder_y_minus_80', 'lShoulder_z_minus_80',\n",
      "       'lElbow_x_minus_80', 'lElbow_y_minus_80', 'lElbow_z_minus_80',\n",
      "       'lWrist_x_minus_80', 'lWrist_y_minus_80', 'lWrist_z_minus_80',\n",
      "       'lHip_x_minus_80', 'lHip_y_minus_80', 'lHip_z_minus_80',\n",
      "       'rEar_x_minus_80', 'rEar_y_minus_80', 'rEar_z_minus_80',\n",
      "       'rShoulder_x_minus_80', 'rShoulder_y_minus_80', 'rShoulder_z_minus_80',\n",
      "       'rElbow_x_minus_80', 'rElbow_y_minus_80', 'rElbow_z_minus_80',\n",
      "       'rWrist_x_minus_80', 'rWrist_y_minus_80', 'rWrist_z_minus_80',\n",
      "       'rHip_x_minus_80', 'rHip_y_minus_80', 'rHip_z_minus_80', 's_1_minus_80',\n",
      "       's_2_minus_80', 's_3_minus_80', 's_4_minus_80'],\n",
      "      dtype='object')\n",
      "Epoch 0, train loss: 0.09373012185096741, train acc: 0.9672015765765766\n",
      "Epoch 50, train loss: 0.09118632972240448, train acc: 0.9674361861861862\n",
      "Epoch 100, train loss: 0.08963960409164429, train acc: 0.9679523273273273\n",
      "Epoch 150, train loss: 0.08869606256484985, train acc: 0.9681869369369369\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(13,14):\n",
    "    for i in np.arange(80,81):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        print(X_train.columns)\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        print(X_train.columns)\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        \n",
    "        \n",
    "                # Specify the folder path and the model filename\n",
    "        state_dict = model.state_dict()\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "        \n",
    "        # Define the folder and model filename\n",
    "        load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "        model_filename = 's_m80_val13_gates.pth'\n",
    "        \n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            inputs.requires_grad = True  # Set requires_grad to True\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                output = torch.round(probabilities)\n",
    "\n",
    "# Load the model\n",
    "#checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "#model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "#model.eval()\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in train_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    inputs.requires_grad = True  # Set requires_grad to True\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, h, test_hidden_states = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        output = torch.round(probabilities)\n",
    "inputs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "        [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "        [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "        ...,\n",
       "        [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "        [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "        [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 1, 46])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input = torch.tensor(X_train.values)\n",
    "input = input.unsqueeze(1)\n",
    "input = input.float()\n",
    "input.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 88\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y231sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output\u001b[39m.\u001b[39;49msize\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "output.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.6528, 16.5354,  6.8842, -2.5370],\n",
       "         [15.7095, 16.3226,  6.9530, -2.3735],\n",
       "         [15.0120, 15.2652,  6.5979, -2.3344],\n",
       "         ...,\n",
       "         [ 4.9547,  3.1134, -4.5596, -2.0263],\n",
       "         [ 4.9473,  2.9779, -4.4284, -1.8249],\n",
       "         [ 4.9355,  3.0609, -4.3583, -1.8576]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2498, -4.0778,  4.4233,  ...,  6.9959, -0.7348, -1.3482],\n",
       "         [-0.2950, -3.5366,  4.4125,  ...,  6.9596, -0.6890, -1.5728],\n",
       "         [-0.2455, -3.2876,  4.0599,  ...,  6.5503, -0.7003, -1.6831],\n",
       "         ...,\n",
       "         [-0.8525,  0.8381, -0.3202,  ..., -0.0396,  2.6897,  2.1940],\n",
       "         [-0.8352,  0.8757, -0.3068,  ..., -0.0204,  2.7345,  1.8132],\n",
       "         [-0.7982,  0.8865, -0.3056,  ..., -0.0164,  2.7295,  1.5923]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0308,  0.0021,  0.0413,  ...,  0.2176,  0.0700, -0.3320],\n",
       "          [ 0.0290,  0.0097,  0.0412,  ...,  0.2165,  0.0713, -0.3338],\n",
       "          [ 0.0310,  0.0132,  0.0376,  ...,  0.2037,  0.0710, -0.3346],\n",
       "          ...,\n",
       "          [ 0.0065,  0.0711, -0.0079,  ..., -0.0026,  0.1707, -0.3043],\n",
       "          [ 0.0072,  0.0717, -0.0078,  ..., -0.0020,  0.1720, -0.3073],\n",
       "          [ 0.0087,  0.0718, -0.0078,  ..., -0.0018,  0.1719, -0.3090]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, hidden, probs = model(input)\n",
    "probabilities = torch.sigmoid(predictions)\n",
    "output = torch.round(probabilities)\n",
    "\n",
    "# Assuming `input` is your input data tensor\n",
    "input = input.requires_grad_(True)\n",
    "output.requires_grad_(True)\n",
    "target.requires_grad_(True)\n",
    "loss = loss_fn(output, target)\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradients for the input data\n",
    "input_gradients = input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 1, 46])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "gru_cells.0.weight_ih\n",
      "tensor([[ 0.1454, -0.0767,  0.1165,  ...,  0.0882,  0.0758,  0.0415],\n",
      "        [ 0.0745, -0.1682,  0.0754,  ...,  0.0346, -0.0484, -0.1178],\n",
      "        [-0.0083, -0.1615,  0.1327,  ...,  0.0147, -0.1474, -0.1203],\n",
      "        ...,\n",
      "        [ 0.0235, -0.0237, -0.0839,  ...,  0.0383, -0.1634,  0.1645],\n",
      "        [-0.0403,  0.0512,  0.0946,  ..., -0.0186,  0.1682,  0.1391],\n",
      "        [ 0.1168, -0.0078, -0.1271,  ..., -0.0558,  0.1122,  0.0342]])\n",
      "gru_cells.0.weight_hh\n",
      "tensor([[ 0.0654,  0.0306,  0.1648,  ..., -0.0643,  0.1707,  0.0210],\n",
      "        [-0.0169,  0.1101,  0.0372,  ...,  0.0406, -0.0684,  0.0486],\n",
      "        [ 0.1226, -0.1662, -0.0386,  ...,  0.0828,  0.0141, -0.0264],\n",
      "        ...,\n",
      "        [ 0.1211, -0.1242,  0.1029,  ...,  0.0835,  0.0161,  0.0562],\n",
      "        [-0.0618, -0.0938, -0.0629,  ...,  0.0225,  0.1529, -0.1611],\n",
      "        [-0.0711,  0.0437, -0.0786,  ...,  0.0553,  0.0523,  0.1267]])\n",
      "gru_cells.0.bias_ih\n",
      "tensor([-0.0448, -0.1460, -0.1346,  0.0724, -0.1092, -0.0197,  0.0069,  0.1191,\n",
      "        -0.1403,  0.1496,  0.1430,  0.1667, -0.1598, -0.1411, -0.1425, -0.1688,\n",
      "         0.0917, -0.0277,  0.1599,  0.0327,  0.0347, -0.1244,  0.0508, -0.0665,\n",
      "        -0.0295, -0.1343, -0.1673,  0.0276,  0.1174, -0.0098,  0.1094, -0.1628,\n",
      "         0.1079, -0.0417, -0.0304, -0.1414, -0.0490,  0.0350, -0.0239, -0.0371,\n",
      "        -0.1674, -0.0318, -0.0888,  0.1208,  0.0407,  0.1607,  0.0913, -0.1196,\n",
      "        -0.1434,  0.0520,  0.0007,  0.0231,  0.0250, -0.1312, -0.0793, -0.1102,\n",
      "        -0.1343,  0.0433, -0.1600, -0.0174,  0.1429,  0.0239, -0.1694,  0.0577,\n",
      "         0.1607,  0.1639,  0.0046, -0.0801,  0.0591, -0.0131,  0.0726, -0.0672,\n",
      "         0.0513, -0.0894, -0.1173,  0.1703, -0.1659, -0.1647, -0.0099, -0.0328,\n",
      "        -0.0361, -0.1085, -0.0121, -0.0948,  0.0095, -0.0053, -0.1561,  0.0304,\n",
      "         0.1408, -0.0091,  0.1302,  0.0871, -0.0493, -0.0203,  0.1120, -0.0558,\n",
      "        -0.0629,  0.1430,  0.0259,  0.1392, -0.0475,  0.0217])\n",
      "gru_cells.0.bias_hh\n",
      "tensor([-0.0332,  0.0442, -0.1472, -0.0456, -0.1438,  0.1043,  0.0451,  0.1555,\n",
      "         0.1268, -0.1691,  0.0426, -0.1560, -0.1143, -0.0966, -0.0023,  0.1359,\n",
      "        -0.0607, -0.1227, -0.0863, -0.0531, -0.0958,  0.0333, -0.0704,  0.0937,\n",
      "        -0.0370,  0.0593, -0.1396,  0.1397,  0.0136,  0.0929,  0.0389,  0.1503,\n",
      "         0.0905, -0.1106,  0.1559, -0.0772,  0.0270,  0.1678, -0.1394, -0.1400,\n",
      "        -0.0626,  0.0926, -0.1114,  0.1665, -0.1708,  0.0881,  0.1589,  0.0757,\n",
      "         0.1052,  0.0133, -0.0148,  0.1212, -0.0758,  0.0402,  0.1526,  0.0132,\n",
      "        -0.1624, -0.1158,  0.1666, -0.1401,  0.1222, -0.0086, -0.0232, -0.0315,\n",
      "        -0.1643,  0.1242, -0.0542,  0.1045, -0.0093, -0.0621,  0.1381,  0.1522,\n",
      "        -0.1240, -0.1321,  0.1714, -0.1469, -0.0439, -0.0988,  0.1347, -0.1438,\n",
      "         0.0290,  0.0139,  0.0760, -0.1633,  0.0066,  0.0641, -0.1302, -0.0282,\n",
      "        -0.1271, -0.1249, -0.0584,  0.1551,  0.0984, -0.1548,  0.1204,  0.1488,\n",
      "         0.1022,  0.0271, -0.1225,  0.0938, -0.0121,  0.0651])\n"
     ]
    }
   ],
   "source": [
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "print(len(gru_cell_weights))\n",
    "# Print or access the weights as needed\n",
    "for key, value in gru_cell_weights.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "weight_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "weight_hh = gru_cell_weights['gru_cells.0.weight_hh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_ih[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ih_np = weight_ih.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['nose_x_minus_80', 'nose_y_minus_80', 'nose_z_minus_80',\n",
    "       'headTop_x_minus_80', 'headTop_y_minus_80', 'headTop_z_minus_80',\n",
    "       'neck_x_minus_80', 'neck_y_minus_80', 'neck_z_minus_80',\n",
    "       'tailBase_x_minus_80', 'tailBase_y_minus_80', 'tailBase_z_minus_80',\n",
    "       'lEar_x_minus_80', 'lEar_y_minus_80', 'lEar_z_minus_80',\n",
    "       'lShoulder_x_minus_80', 'lShoulder_y_minus_80', 'lShoulder_z_minus_80',\n",
    "       'lElbow_x_minus_80', 'lElbow_y_minus_80', 'lElbow_z_minus_80',\n",
    "       'lWrist_x_minus_80', 'lWrist_y_minus_80', 'lWrist_z_minus_80',\n",
    "       'lHip_x_minus_80', 'lHip_y_minus_80', 'lHip_z_minus_80',\n",
    "       'rEar_x_minus_80', 'rEar_y_minus_80', 'rEar_z_minus_80',\n",
    "       'rShoulder_x_minus_80', 'rShoulder_y_minus_80', 'rShoulder_z_minus_80',\n",
    "       'rElbow_x_minus_80', 'rElbow_y_minus_80', 'rElbow_z_minus_80',\n",
    "       'rWrist_x_minus_80', 'rWrist_y_minus_80', 'rWrist_z_minus_80',\n",
    "       'rHip_x_minus_80', 'rHip_y_minus_80', 'rHip_z_minus_80', 's_1_minus_80',\n",
    "       's_2_minus_80', 's_3_minus_80', 's_4_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 46)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ih_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(weight_ih_np, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.118293\n",
       "1      0.057691\n",
       "2     -0.099260\n",
       "3      0.020036\n",
       "4      0.046773\n",
       "         ...   \n",
       "97    -0.108127\n",
       "98    -0.147265\n",
       "99     0.120991\n",
       "100   -0.109019\n",
       "101    0.127330\n",
       "Name: rWrist_x_minus_80, Length: 102, dtype: float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['rWrist_x_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.073618\n",
       "1      0.129745\n",
       "2     -0.052349\n",
       "3      0.136418\n",
       "4      0.078713\n",
       "         ...   \n",
       "97    -0.137238\n",
       "98    -0.041288\n",
       "99    -0.127240\n",
       "100   -0.085322\n",
       "101   -0.029799\n",
       "Name: lWrist_x_minus_80, Length: 102, dtype: float32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['lWrist_x_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 1\n",
    "hidden_dim = 1\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwrist_df = scaled_df[['rWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "lwrist_df = scaled_df[['lWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "labels = ['s_1','s_2','s_3','s_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8470, dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rWrist_loss.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_df = scaled_df[scaled_df['trial'] != 14]\n",
    "#target = filtered_df[['s_1','s_2','s_3','s_4']] \n",
    "target = torch.from_numpy(y_train.to_numpy())\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 102\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y223sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mgrad\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = loss_fn(output, target)\n",
    "model_loss.requires_grad_(True)\n",
    "model_loss.backward()\n",
    "\n",
    "\n",
    "# Access gradients for specific parameters\n",
    "gru_weight_ih_gradients = model.gru.weight_ih.grad  # Gradients for input-hidden weights of GRU\n",
    "gru_weight_hh_gradients = model.gru.weight_hh.grad  # Gradients for hidden-hidden weights of GRU\n",
    "linear_weight_gradients = model.fc.weight.grad  # Gradients for the weight of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6458, dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rWrist_x_minus_80</th>\n",
       "      <th>lWrist_x_minus_80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rWrist_x_minus_80  lWrist_x_minus_80\n",
       "0                  0.0                0.0\n",
       "1                  0.0                0.0\n",
       "2                  0.0                0.0\n",
       "3                  0.0                0.0\n",
       "4                  0.0                0.0\n",
       "..                 ...                ...\n",
       "97                 0.0                0.0\n",
       "98                 0.0                0.0\n",
       "99                 0.0                0.0\n",
       "100                0.0                0.0\n",
       "101                0.0                0.0\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_array = gru_weight_ih_gradients.numpy()\n",
    "gradients = pd.DataFrame(gradients_array, columns=features)\n",
    "gradients[['rWrist_x_minus_80','lWrist_x_minus_80']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.392157618e-06"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean(np.round(gradients['rWrist_x_minus_80'] - gradients['lWrist_x_minus_80'],5)),15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# Perform the backward pass on the combined loss\n",
    "combined_loss = rWrist_loss + lWrist_loss\n",
    "combined_loss.backward()\n",
    "gradients = combined_loss.grad\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lWrist_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GRUCellNet:\n\tsize mismatch for gru.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([4, 1]) from checkpoint, the shape in current model is torch.Size([4, 34]).\n\tsize mismatch for gru_cells.0.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru_cells.0.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru_cells.0.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru_cells.0.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for batch_norm.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 110\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(load_folder, model_filename))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set the model in evaluation mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GRUCellNet:\n\tsize mismatch for gru.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([4, 1]) from checkpoint, the shape in current model is torch.Size([4, 34]).\n\tsize mismatch for gru_cells.0.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru_cells.0.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru_cells.0.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru_cells.0.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for batch_norm.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34])."
     ]
    }
   ],
   "source": [
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 'lWrist_model.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "lWrist_weights_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "lWrist_weights_hh = gru_cell_weights['gru_cells.0.weight_hh']\n",
    "\n",
    "\n",
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 'rWrist_model.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "rWrist_weights_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "rWrist_weights_hh = gru_cell_weights['gru_cells.0.weight_hh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6961],\n",
      "        [ 0.6693],\n",
      "        [-0.4421]])\n",
      "tensor([[ 0.6961],\n",
      "        [ 0.6693],\n",
      "        [-0.4421]])\n"
     ]
    }
   ],
   "source": [
    "print(lWrist_weights_ih)\n",
    "print(rWrist_weights_ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rWrist_x'], dtype='object')\n",
      "Index(['rWrist_x'], dtype='object')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 1 expected 46",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 111\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mX_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#print(X_train.shape[0])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states \u001b[39m=\u001b[39m run_training(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     train_dataloader, val_dataloader\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, loss_fn\u001b[39m=\u001b[39;49mloss_fn, num_epochs\u001b[39m=\u001b[39;49mn_epochs, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models\u001b[39m\u001b[39m'\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 111\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_hidden_states, val_hidden_states \u001b[39m=\u001b[39m [], [] \u001b[39m# Initialize lists to store hidden states\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epoch_train_loss, epoch_train_acc, train_preds, train_probs, train_hidden \u001b[39m=\u001b[39m train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(epoch_train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accs\u001b[39m.\u001b[39mappend(epoch_train_acc)\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 111\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out, last_hidden_states, hidden_states_batch \u001b[39m=\u001b[39m model(x)  \u001b[39m# Collect hidden states here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y_prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Append the predicted probabilities to the list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 111\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     input_t \u001b[39m=\u001b[39m x[:, t, :]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(input_t, h)  \u001b[39m# Update the hidden state for the single layer\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     hidden_states\u001b[39m.\u001b[39mappend(h)  \u001b[39m# Append the hidden state for the current time step\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y221sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm(h)  \u001b[39m# Apply batch normalization to the final hidden state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1279\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[1;32m-> 1279\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru_cell(\n\u001b[0;32m   1280\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[0;32m   1281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[0;32m   1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[0;32m   1283\u001b[0m )\n\u001b[0;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1286\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 1 expected 46"
     ]
    }
   ],
   "source": [
    "rwrist_df = scaled_df[['rWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "lwrist_df = scaled_df[['lWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "\n",
    "#rwrist_df = lwrist_df.copy()\n",
    "set_values = 13 \n",
    "train_set = rwrist_df[rwrist_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "test_set = rwrist_df[rwrist_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = rwrist_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "print(X_train.columns)\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "print(X_train.columns)\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "#print(X_train.shape[0])\n",
    "\n",
    "\n",
    "train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "    train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models' \n",
    "model_filename = 'rWrist_model.pth'  \n",
    "\n",
    "# Combine the folder path and model filename\n",
    "full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "torch.save(state_dict, full_model)\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in train_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    inputs.requires_grad = True  # Set requires_grad to True\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, h, test_hidden_states = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        rWrist_output = torch.round(probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
