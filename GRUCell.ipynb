{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with scaling\n",
    "df = pd.read_csv(\"C:/Users/kacpe/Desktop/study/research lab/data_model_v2.csv\")\n",
    "# List of column names to drop\n",
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "#df = df[columns_to_keep]\n",
    "# Step 1: Separate 'id' and 'trial' columns from the rest of the data\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "\n",
    "# Step 2: Apply MinMaxScaler to the remaining columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "\n",
    "# Step 3: Merge 'id' and 'trial' columns with the scaled data\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "\n",
    "# Step 4: create variable t-1\n",
    "# List of columns to shift\n",
    "columns_to_shift = ['s_1', 's_2', 's_3','s_4']  # List all column names here\n",
    "\n",
    "# Create shifted columns for each column in the list\n",
    "for col in columns_to_shift:\n",
    "    new_col_name = col + '_minus_1'\n",
    "    scaled_df[new_col_name] = scaled_df[col].shift(1)\n",
    "    scaled_df[new_col_name] = scaled_df[new_col_name].fillna(1)  # Fill NaN in the first row with 0\n",
    "    scaled_df[new_col_name] = scaled_df[new_col_name].astype(int)\n",
    "\n",
    "#desired_column_order = [col for col in scaled_df.columns if col not in columns_to_shift] + columns_to_shift\n",
    "#scaled_df = scaled_df[desired_column_order]\n",
    "selected_columns = ['id', 'trial','s_1_minus_1','s_2_minus_1','s_3_minus_1','s_4_minus_1','s_1','s_2','s_3','s_4']\n",
    "scaled_df = scaled_df[selected_columns]\n",
    "\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = scaled_df[scaled_df['trial'].isin(range(1, 15))].drop(columns=['id', 'trial'])\n",
    "test_set = scaled_df[scaled_df['trial']==15].drop(columns=['id', 'trial'])\n",
    "val_set = scaled_df[scaled_df['trial']==16].drop(columns=['id', 'trial'])\n",
    "full_set = scaled_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.iloc[:,:-4], train_set.iloc[:,-4:]\n",
    "X_test, y_test = test_set.iloc[:,:-4], test_set.iloc[:,-4:]\n",
    "X_val, y_val = val_set.iloc[:,:-4], val_set.iloc[:,-4:]\n",
    "X, y = full_set.iloc[:,:-4], full_set.iloc[:,-4:]\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "# Create a DataLoader\n",
    "#batch_size = 5561#67  # Set your desired batch size\n",
    "#shuffle = False  # Set to False to preserve the order of your data\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)\n",
    "        #Xavier initialization for GRU weights\n",
    "        #for name, param in self.gru.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        init.xavier_uniform_(param.data)\n",
    "        #    elif 'bias' in name:\n",
    "        #        init.constant_(param.data, 0.0)\n",
    "                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sotfplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.sotfplus(out[:,-1]))\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out, h\n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "        #weight = next(self.parameters()).data\n",
    "        #hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        #return hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size > 1:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]  # Update input_t with the new hidden state for the next layer\n",
    "            hidden_states.append(new_hidden_states)\n",
    "        \n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        # Apply BatchNorm to the last hidden state\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        #all_hidden_states = torch.cat(hidden_states, dim=1)  # Stack all hidden states across time steps\n",
    "        \n",
    "        \n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "        #out = self.fc(self.softplus(all_hidden_states[:, -1]))  # Use the last hidden state for prediction\n",
    "        #probs = torch.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "        #preds = torch.round(probs)  # Convert probabilities to binary predictions\n",
    "        #out = F.sigmoid(out, dim=1)\n",
    "        \n",
    "        return out, last_hidden_states \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "     #   return nn.Parameter(torch.zeros([batch_size, self.gru_cells[0].hidden_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6107, 42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =1500\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.1657301187515259, train acc: 0.48640903880792535, val loss: 1.2263506650924683, val acc: 0.4246268656716418\n",
      "Epoch 50, train loss: 0.48762622475624084, train acc: 0.7950712297363681, val loss: 0.6215541362762451, val acc: 0.7574626865671642\n",
      "Epoch 100, train loss: 0.3846134841442108, train acc: 0.8505403635172752, val loss: 0.5233508944511414, val acc: 0.8276119402985075\n",
      "Epoch 150, train loss: 0.3335549235343933, train acc: 0.8855002456197806, val loss: 0.46335503458976746, val acc: 0.8656716417910447\n",
      "Epoch 200, train loss: 0.3041295111179352, train acc: 0.8977402980186671, val loss: 0.39804062247276306, val acc: 0.9059701492537313\n",
      "Epoch 250, train loss: 0.2835725247859955, train acc: 0.904372032094318, val loss: 0.399583101272583, val acc: 0.9067164179104478\n",
      "Epoch 300, train loss: 0.26798397302627563, train acc: 0.9098984771573604, val loss: 0.4157782196998596, val acc: 0.9134328358208955\n",
      "Epoch 350, train loss: 0.2562747299671173, train acc: 0.914769936138857, val loss: 0.4214875102043152, val acc: 0.9119402985074627\n",
      "Epoch 400, train loss: 0.24722586572170258, train acc: 0.9184132962174554, val loss: 0.4344578981399536, val acc: 0.8977611940298508\n",
      "Epoch 450, train loss: 0.23994581401348114, train acc: 0.9207057475028656, val loss: 0.45684751868247986, val acc: 0.8925373134328358\n",
      "Epoch 500, train loss: 0.2340131253004074, train acc: 0.9232438185688554, val loss: 0.48236310482025146, val acc: 0.8865671641791045\n",
      "Epoch 550, train loss: 0.22919495403766632, train acc: 0.925700016374652, val loss: 0.505706250667572, val acc: 0.8820895522388059\n",
      "Epoch 600, train loss: 0.22531461715698242, train acc: 0.92717373505813, val loss: 0.524260938167572, val acc: 0.8783582089552239\n",
      "Epoch 650, train loss: 0.2222098708152771, train acc: 0.9284837072212215, val loss: 0.5386714935302734, val acc: 0.8761194029850746\n",
      "Epoch 700, train loss: 0.21973493695259094, train acc: 0.9297527427542165, val loss: 0.5498936772346497, val acc: 0.8753731343283582\n",
      "Epoch 750, train loss: 0.21776600182056427, train acc: 0.9305305387260521, val loss: 0.558578610420227, val acc: 0.8746268656716418\n",
      "Epoch 800, train loss: 0.21620173752307892, train acc: 0.9308989683969215, val loss: 0.5651346445083618, val acc: 0.8731343283582089\n",
      "Epoch 850, train loss: 0.2149602621793747, train acc: 0.9313492713279843, val loss: 0.5701217651367188, val acc: 0.8731343283582089\n",
      "Epoch 900, train loss: 0.21397589147090912, train acc: 0.931594891108564, val loss: 0.5738848447799683, val acc: 0.8746268656716418\n",
      "Epoch 950, train loss: 0.21319618821144104, train acc: 0.9319633207794334, val loss: 0.5767347812652588, val acc: 0.8746268656716418\n",
      "Epoch 1000, train loss: 0.2125791758298874, train acc: 0.9322908138202063, val loss: 0.5789210796356201, val acc: 0.8723880597014926\n",
      "Epoch 1050, train loss: 0.21209140121936798, train acc: 0.9324545603405927, val loss: 0.5805633068084717, val acc: 0.8716417910447761\n",
      "Epoch 1100, train loss: 0.21170610189437866, train acc: 0.9327001801211724, val loss: 0.5818522572517395, val acc: 0.8716417910447761\n",
      "Epoch 1150, train loss: 0.2114020586013794, train acc: 0.9328229900114623, val loss: 0.5828654766082764, val acc: 0.8708955223880597\n",
      "Epoch 1200, train loss: 0.21116222441196442, train acc: 0.9329457999017521, val loss: 0.5836359262466431, val acc: 0.8708955223880597\n",
      "Epoch 1250, train loss: 0.21097323298454285, train acc: 0.9330276731619453, val loss: 0.5842205286026001, val acc: 0.8708955223880597\n",
      "Epoch 1300, train loss: 0.21082442998886108, train acc: 0.9330686097920419, val loss: 0.5847076177597046, val acc: 0.8708955223880597\n",
      "Epoch 1350, train loss: 0.21070724725723267, train acc: 0.9331504830522351, val loss: 0.5850668549537659, val acc: 0.8708955223880597\n",
      "Epoch 1400, train loss: 0.2106151282787323, train acc: 0.9331914196823318, val loss: 0.5853514075279236, val acc: 0.8716417910447761\n",
      "Epoch 1450, train loss: 0.21054266393184662, train acc: 0.9331914196823318, val loss: 0.5855733752250671, val acc: 0.8716417910447761\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_losses, val_accs, train_predicted, val_predicted, train_probs, val_probs= run_training(\n",
    "    train_dataloader, val_dataloader=val_dataloader, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('gru.weight_ih', tensor([[-0.1942,  0.1579, -0.1187,  ...,  0.1951,  0.0081, -0.2321],\n",
      "        [ 0.0738, -0.0785, -0.0909,  ...,  0.2302, -0.0865, -0.2449],\n",
      "        [ 0.2258, -0.1072, -0.1338,  ...,  0.0844, -0.1174, -0.1332],\n",
      "        ...,\n",
      "        [-0.0904, -0.1083,  0.1081,  ..., -0.1008,  0.1647, -0.0004],\n",
      "        [-0.0898, -0.0198,  0.0253,  ...,  0.1581,  0.0917, -0.0720],\n",
      "        [ 0.0870, -0.0490, -0.0476,  ..., -0.0609, -0.2150,  0.2474]])), ('gru.weight_hh', tensor([[-8.8365e-02,  2.3985e-01,  2.0377e-01,  2.5127e-02, -1.8867e-01,\n",
      "         -2.4518e-01, -1.4189e-01, -1.0810e-01, -1.7020e-01, -3.3798e-02,\n",
      "          1.2047e-01,  1.6403e-01, -1.9342e-01, -5.3630e-02,  1.1273e-01,\n",
      "          1.5279e-01],\n",
      "        [-4.7585e-02,  9.1215e-02, -5.8537e-02,  2.2467e-01, -6.4348e-03,\n",
      "         -2.0686e-01,  1.7284e-01, -1.5168e-01,  6.3865e-02,  4.9067e-02,\n",
      "          1.3338e-01,  1.3188e-01, -3.7033e-02,  1.1104e-01, -2.2279e-01,\n",
      "          3.7783e-04],\n",
      "        [ 1.5429e-01, -1.5404e-01, -1.5327e-01,  1.7240e-01, -4.6628e-02,\n",
      "          2.4793e-01, -7.7011e-02, -1.2452e-01, -1.5417e-01,  3.2912e-02,\n",
      "         -4.4128e-04,  2.1324e-01, -3.9604e-02,  4.3342e-02,  2.4036e-01,\n",
      "          1.9303e-01],\n",
      "        [ 2.3444e-01,  2.1743e-01, -6.6429e-02, -1.7950e-01, -2.9895e-02,\n",
      "         -1.7536e-01,  9.8718e-02, -4.1173e-02, -1.3231e-01, -1.4322e-01,\n",
      "          2.4552e-01,  2.5491e-02, -2.2548e-01, -1.0388e-01, -6.0580e-02,\n",
      "          3.3696e-02],\n",
      "        [ 2.3639e-01, -1.4420e-01, -1.0491e-01,  2.9098e-02, -7.1066e-02,\n",
      "         -1.9390e-01,  5.7596e-02, -3.9672e-02, -1.2700e-01, -3.4946e-02,\n",
      "          4.0872e-02, -2.3732e-01,  8.6362e-02,  2.2862e-01, -2.3139e-01,\n",
      "         -1.3575e-01],\n",
      "        [ 1.6274e-01, -7.3670e-02, -1.8691e-01,  2.0044e-01,  2.4119e-01,\n",
      "          1.5400e-01,  9.6860e-02,  2.7827e-02,  1.5733e-01, -1.9921e-01,\n",
      "         -2.3403e-01, -1.8817e-01,  5.1354e-02,  1.8393e-01, -6.8121e-02,\n",
      "         -1.3136e-01],\n",
      "        [ 1.4297e-02,  9.8638e-02, -1.3397e-01,  5.7231e-02, -1.9331e-02,\n",
      "         -2.2067e-01,  1.9855e-01,  1.0177e-01, -3.6538e-02,  9.2367e-02,\n",
      "         -1.7602e-01,  2.2333e-01, -1.1522e-01,  1.9440e-01, -4.8659e-02,\n",
      "          1.6244e-01],\n",
      "        [ 4.6436e-02,  2.3151e-01, -2.4722e-01,  1.0843e-01,  6.1489e-02,\n",
      "         -2.4739e-02,  1.4497e-01,  2.2659e-01,  1.6070e-01, -5.2690e-02,\n",
      "          9.3255e-02, -2.3937e-01, -1.7438e-01, -1.2626e-01,  2.2636e-01,\n",
      "          2.6489e-02],\n",
      "        [ 1.8589e-01, -5.6141e-02, -1.6728e-01, -1.3453e-01,  1.7431e-01,\n",
      "         -8.0077e-02, -1.2777e-01, -1.5577e-01,  2.3169e-01,  2.1199e-01,\n",
      "         -2.0869e-01,  2.1142e-01, -9.1440e-02, -1.2932e-01, -1.4961e-01,\n",
      "         -2.0725e-01],\n",
      "        [ 1.7932e-01, -1.4948e-01, -1.0953e-01,  2.3757e-01, -4.4627e-02,\n",
      "         -2.0863e-01,  2.3815e-01, -9.8722e-02, -1.9918e-01, -4.2714e-02,\n",
      "          1.5765e-01,  7.3456e-02,  2.4687e-02, -1.9143e-01,  2.0183e-01,\n",
      "         -3.8274e-02],\n",
      "        [ 2.3020e-01,  2.1570e-01, -3.4879e-02, -1.2924e-01, -2.2663e-01,\n",
      "         -1.9595e-01, -7.1919e-02, -4.6506e-02, -3.1278e-02,  2.2095e-01,\n",
      "          1.4233e-01,  2.1976e-01, -2.6708e-02,  1.6283e-01,  9.8839e-02,\n",
      "          2.0425e-01],\n",
      "        [-1.3353e-01, -2.1976e-01,  1.0153e-02,  1.7381e-01,  1.3957e-01,\n",
      "          6.6262e-02,  1.9127e-01,  1.6682e-01,  2.3248e-02, -8.4573e-02,\n",
      "          1.3616e-01,  2.1774e-01, -2.1800e-01,  2.9789e-03,  2.3572e-01,\n",
      "          6.8735e-03],\n",
      "        [ 4.4157e-02,  7.9636e-02, -2.1224e-01, -7.0627e-02, -8.8475e-02,\n",
      "          1.7787e-01,  5.6090e-02,  2.2253e-01,  2.1878e-01,  2.3844e-01,\n",
      "         -1.5026e-01,  2.4545e-01, -2.1641e-01, -4.3963e-02, -5.9974e-02,\n",
      "          2.4571e-01],\n",
      "        [ 2.0911e-01, -1.9341e-01, -8.2390e-02, -1.0333e-01, -1.0372e-01,\n",
      "          1.6253e-01,  3.9372e-02,  2.4373e-01,  7.1852e-02,  1.2267e-01,\n",
      "         -1.4784e-01,  1.0002e-01, -2.2053e-01, -2.2690e-01,  1.3077e-01,\n",
      "          1.8524e-01],\n",
      "        [ 5.4743e-02,  3.6820e-03,  5.2818e-02,  1.1294e-01, -6.3076e-02,\n",
      "          1.4368e-01,  2.6063e-03, -9.4792e-02, -1.6372e-01, -2.4610e-01,\n",
      "         -2.2887e-01, -4.4143e-02, -2.4642e-01,  4.6321e-02,  2.4964e-01,\n",
      "          5.0751e-02],\n",
      "        [ 2.3238e-01,  1.9548e-01,  1.4260e-01,  1.5621e-02,  1.0600e-01,\n",
      "          1.3349e-03,  1.8776e-01, -7.1141e-02, -1.8023e-01,  2.0037e-01,\n",
      "          1.4600e-01,  1.2580e-01,  8.0734e-02,  2.5646e-02, -1.3776e-01,\n",
      "          2.0043e-01],\n",
      "        [-1.4622e-01,  1.5493e-01, -3.5302e-02,  6.8482e-02,  3.8077e-02,\n",
      "          3.6559e-02,  7.9871e-02,  4.0611e-02, -1.6181e-01, -9.3967e-05,\n",
      "         -1.6786e-02, -5.0353e-02,  1.9583e-01, -1.1679e-01, -4.5757e-03,\n",
      "          3.6787e-03],\n",
      "        [ 9.0150e-02,  1.9570e-01, -1.0250e-01, -1.1199e-01,  2.0350e-01,\n",
      "         -9.1477e-02, -2.0302e-01,  1.2762e-01, -1.5134e-01,  1.6475e-01,\n",
      "         -2.4513e-02, -4.0642e-02,  4.6588e-02,  1.3646e-01, -5.4218e-02,\n",
      "          1.3001e-01],\n",
      "        [-1.5648e-01,  1.9646e-01, -2.3147e-01, -4.2474e-02, -2.0352e-01,\n",
      "          1.6820e-01, -1.3077e-01, -1.0858e-02,  4.6325e-02,  2.0456e-02,\n",
      "         -4.9469e-02, -1.5210e-01, -8.4746e-02, -2.4454e-01,  1.2773e-01,\n",
      "         -1.8782e-01],\n",
      "        [ 4.7221e-02, -7.4432e-02,  2.4826e-02,  7.1443e-02,  3.9836e-02,\n",
      "         -4.2367e-04,  2.1324e-01, -3.1233e-02,  8.0487e-02,  3.9137e-02,\n",
      "         -1.6234e-01,  1.0274e-01,  2.4355e-01,  6.1546e-02, -8.2807e-02,\n",
      "          1.6098e-01],\n",
      "        [-1.4573e-01,  1.4733e-01, -1.7842e-01,  1.5568e-02,  1.9521e-01,\n",
      "         -1.7765e-01,  1.6264e-01,  1.5438e-01, -5.6962e-02,  5.5318e-02,\n",
      "          7.9894e-02,  1.0217e-02,  2.1423e-01, -7.0283e-02, -1.8997e-01,\n",
      "          7.9296e-02],\n",
      "        [-8.9790e-02, -1.6210e-01, -2.0585e-01,  9.9958e-02,  1.3928e-01,\n",
      "          1.1960e-01, -4.9562e-02, -7.2893e-02,  1.9677e-01, -2.1003e-01,\n",
      "          1.6485e-01, -8.8226e-02, -9.7722e-02, -8.5793e-02,  7.0570e-02,\n",
      "          2.0531e-01],\n",
      "        [-1.5670e-01, -2.1985e-01,  2.5817e-02,  2.0891e-01, -2.0259e-01,\n",
      "         -1.1714e-01,  6.0761e-02, -9.7769e-02,  5.9211e-02,  3.6886e-04,\n",
      "         -1.3652e-01,  1.4051e-01, -2.0166e-01, -1.2504e-01, -1.1086e-01,\n",
      "         -2.1127e-01],\n",
      "        [-8.2633e-02, -1.6355e-01,  2.4924e-01,  2.2786e-01, -2.3474e-01,\n",
      "         -2.2166e-01,  1.7451e-01, -2.2927e-01,  2.1821e-01,  4.9856e-02,\n",
      "         -5.6459e-02, -1.4054e-01,  2.0275e-01,  8.6595e-03,  3.6083e-03,\n",
      "          1.7195e-02],\n",
      "        [-2.2969e-01,  2.0103e-01, -1.1334e-01, -1.7503e-03,  2.4861e-01,\n",
      "         -1.5832e-01,  1.6223e-02,  1.4140e-01,  2.6437e-03,  1.2247e-01,\n",
      "          7.4488e-02, -2.8388e-02,  1.3317e-01, -1.5882e-01, -2.4526e-01,\n",
      "          1.1108e-01],\n",
      "        [ 2.2465e-01,  2.0243e-01,  2.1302e-01,  5.4629e-03,  4.8183e-03,\n",
      "         -1.2641e-01, -2.2156e-01, -2.1489e-01,  5.8050e-02,  1.1969e-02,\n",
      "         -8.8384e-02, -1.5621e-01,  6.1562e-02, -1.0228e-02,  4.7288e-02,\n",
      "          2.0716e-01],\n",
      "        [-2.3780e-01, -9.2312e-02, -2.1343e-01,  2.9940e-03, -1.4054e-01,\n",
      "         -2.2986e-02, -2.1348e-01, -1.2021e-01, -1.1490e-01,  2.4036e-01,\n",
      "          1.1746e-02,  9.0165e-03, -1.3601e-02,  4.3377e-02, -8.1921e-02,\n",
      "         -1.7430e-01],\n",
      "        [-4.9983e-02,  1.0889e-01, -1.3242e-01,  1.1748e-01, -2.5037e-02,\n",
      "          9.9914e-02,  6.7169e-03, -9.4258e-03, -1.9738e-01, -6.9205e-02,\n",
      "          5.2327e-02,  1.3503e-01,  1.3104e-01,  1.4479e-01, -3.7269e-02,\n",
      "          2.4409e-01],\n",
      "        [-1.7686e-01,  5.3448e-02,  1.4096e-01,  1.6237e-01,  3.5607e-03,\n",
      "          1.4589e-01, -1.9727e-01, -1.5938e-01, -1.3862e-01, -1.6256e-01,\n",
      "          5.3152e-02, -2.3896e-01, -8.9587e-02,  1.3243e-01,  1.1299e-01,\n",
      "         -3.4525e-02],\n",
      "        [ 2.4350e-01, -7.1082e-04, -1.5449e-01,  1.3438e-01, -2.4956e-01,\n",
      "         -1.2231e-02,  2.0075e-02, -2.3084e-01, -2.1356e-01, -2.2915e-01,\n",
      "          2.1746e-01, -5.6543e-02, -2.4368e-01, -1.5918e-01, -1.7397e-01,\n",
      "          1.5441e-01],\n",
      "        [ 9.3735e-02,  1.6548e-01,  8.7729e-02,  2.3632e-01,  1.2347e-01,\n",
      "          2.9919e-02, -1.6740e-01, -8.8374e-02, -2.3136e-01, -4.0816e-02,\n",
      "         -9.7657e-02, -4.2440e-02,  4.4642e-02,  1.4673e-01, -7.6899e-02,\n",
      "          2.2687e-01],\n",
      "        [-2.1167e-01,  1.5351e-02,  6.9710e-02,  2.4002e-01, -8.6157e-02,\n",
      "          1.6632e-01, -2.2334e-04, -6.2973e-02, -8.9344e-02, -2.4074e-01,\n",
      "          3.5628e-02,  2.1050e-01,  1.7572e-01, -6.0252e-02,  1.0081e-01,\n",
      "         -1.2973e-01],\n",
      "        [-1.2176e-01, -1.3366e-02, -1.2098e-01, -1.9959e-01,  2.4889e-02,\n",
      "         -7.9516e-02,  4.1340e-02,  1.9723e-01,  1.9386e-01, -1.6067e-02,\n",
      "          1.9174e-01,  1.1192e-01, -1.0176e-01, -1.9739e-01, -1.9573e-01,\n",
      "          1.4306e-01],\n",
      "        [-9.1978e-02, -9.6982e-02,  2.2000e-01,  1.8963e-01, -4.5890e-02,\n",
      "          2.2505e-01,  2.2471e-01,  1.6806e-01, -8.2773e-02, -1.0507e-01,\n",
      "          2.4103e-01,  4.4256e-02, -8.8908e-02,  2.4984e-01,  1.5803e-01,\n",
      "         -1.3334e-01],\n",
      "        [ 8.7412e-02, -2.4116e-01,  2.0442e-01,  1.1623e-01, -9.5328e-02,\n",
      "          8.1383e-02, -1.9514e-02,  1.8227e-01,  2.0512e-01,  5.2495e-02,\n",
      "          1.6048e-01,  2.2870e-03,  1.5441e-01, -1.0682e-01, -5.0603e-02,\n",
      "          1.2394e-01],\n",
      "        [ 2.3212e-01, -4.2183e-02, -4.3416e-03,  4.9022e-02, -1.9846e-01,\n",
      "          2.0981e-01,  8.6979e-03,  1.2946e-01, -5.6343e-02,  2.1420e-01,\n",
      "          1.9790e-01, -1.7413e-01, -1.3788e-01, -6.6039e-02, -3.6733e-02,\n",
      "          2.3506e-01],\n",
      "        [ 1.5926e-01, -8.4249e-02, -1.2949e-01, -1.9608e-01, -1.2035e-01,\n",
      "          1.3981e-01, -2.0621e-01, -1.7672e-01, -1.1979e-01,  1.5720e-01,\n",
      "         -4.8751e-02,  1.1711e-02, -1.5858e-01, -2.9226e-02,  6.7110e-02,\n",
      "          3.0704e-03],\n",
      "        [ 1.8716e-01, -2.3506e-01,  2.3149e-01, -5.9717e-02, -2.0075e-01,\n",
      "         -1.3374e-01,  1.5238e-01,  6.3463e-02, -7.1453e-03,  1.9605e-03,\n",
      "          4.3979e-02,  4.3270e-02,  5.9969e-02, -2.3673e-01, -6.7427e-02,\n",
      "         -1.9904e-01],\n",
      "        [-9.2225e-03, -1.5921e-01,  1.9397e-02,  1.5586e-01,  1.0574e-02,\n",
      "         -7.5829e-02, -1.0429e-01, -9.8465e-02, -1.5717e-01,  2.0976e-01,\n",
      "         -9.7447e-02, -1.5217e-02, -7.9847e-02, -3.5174e-02,  1.5349e-01,\n",
      "          1.1255e-01],\n",
      "        [-1.0477e-01, -7.6647e-02, -2.0622e-01, -2.0288e-01,  3.9263e-02,\n",
      "          1.1561e-01,  2.3697e-01, -1.9818e-01, -1.1242e-01,  2.4001e-01,\n",
      "         -1.4952e-02,  9.7306e-02,  2.3822e-01, -1.6000e-01, -2.6350e-02,\n",
      "         -7.2905e-04],\n",
      "        [-9.7194e-02, -3.2723e-02,  1.5100e-01, -3.1750e-02, -1.5406e-01,\n",
      "          2.2683e-01, -1.8074e-02,  1.4836e-01, -1.4180e-01, -2.1241e-01,\n",
      "          9.5985e-02,  1.1880e-01,  2.3020e-02,  7.2832e-02,  6.6611e-02,\n",
      "         -1.4356e-01],\n",
      "        [ 2.3808e-01, -1.8063e-03, -5.7509e-02,  2.0966e-01,  9.5926e-02,\n",
      "          1.0327e-01,  6.2664e-02,  4.9808e-02, -2.2846e-01, -2.7573e-02,\n",
      "         -6.7065e-03, -6.5280e-03,  2.1492e-01, -2.1695e-01, -2.1379e-01,\n",
      "          2.0014e-01],\n",
      "        [ 1.8473e-02,  2.4949e-01,  1.8319e-01,  2.4532e-01,  1.0572e-02,\n",
      "         -1.3546e-02, -1.3230e-01,  1.9797e-01, -9.5955e-02, -2.2317e-01,\n",
      "         -2.4891e-01, -2.1216e-01, -1.7358e-01, -7.0721e-03, -8.6894e-02,\n",
      "         -1.9624e-01],\n",
      "        [ 6.1046e-02,  2.3013e-01,  2.2643e-01,  2.2529e-01,  1.5406e-01,\n",
      "         -7.2682e-02, -1.4445e-01,  2.4428e-01, -2.3260e-01, -1.7800e-01,\n",
      "          1.4366e-01, -8.6365e-02,  1.3852e-01,  1.0490e-01,  1.1702e-02,\n",
      "         -3.3695e-02],\n",
      "        [-6.1171e-02, -9.8836e-02,  1.6090e-01,  2.3839e-01, -2.2262e-01,\n",
      "         -6.7805e-02, -4.2527e-02,  6.1896e-02,  1.1428e-01, -1.5355e-01,\n",
      "          1.0875e-01, -3.3347e-02,  2.4456e-01,  1.8426e-01, -2.0528e-01,\n",
      "          4.5688e-02],\n",
      "        [-2.4197e-01, -1.3485e-01,  5.8100e-02,  1.4080e-01, -5.7277e-02,\n",
      "         -1.8196e-01,  6.4114e-02,  1.6075e-01, -1.2201e-01, -4.8636e-02,\n",
      "          1.6052e-01, -1.3339e-01,  5.4076e-03, -1.2248e-02,  4.0502e-02,\n",
      "         -2.1598e-01],\n",
      "        [ 1.9385e-01,  1.5717e-01,  2.0161e-01,  1.7639e-01,  2.6717e-02,\n",
      "          2.1147e-01, -1.2165e-01, -1.9557e-01,  2.4899e-01, -3.9628e-03,\n",
      "          1.1844e-01,  1.8169e-01, -1.3561e-01,  1.5266e-01, -1.4598e-01,\n",
      "          1.2612e-01],\n",
      "        [-3.7612e-02, -7.0749e-02, -4.7260e-02, -1.8702e-01,  2.3134e-01,\n",
      "          1.2035e-01,  1.8827e-02,  1.5374e-01, -1.3269e-01, -1.1656e-01,\n",
      "         -2.0084e-01,  1.3337e-02, -1.5095e-01,  5.9958e-02, -2.2869e-01,\n",
      "         -1.1862e-01]])), ('gru.bias_ih', tensor([ 0.1278, -0.1971,  0.2458, -0.1590, -0.0713,  0.0746,  0.0120,  0.0797,\n",
      "        -0.2248, -0.1748,  0.1231,  0.2172,  0.1538, -0.0081, -0.0714, -0.1192,\n",
      "         0.0986,  0.1233, -0.1728, -0.2032,  0.1755, -0.0569, -0.1159,  0.2014,\n",
      "        -0.1799, -0.2288, -0.1428, -0.0901,  0.0647, -0.1779,  0.2055,  0.0670,\n",
      "        -0.1235, -0.2153,  0.1893, -0.0448,  0.0463, -0.1441,  0.1015,  0.1465,\n",
      "        -0.0143, -0.0488,  0.0982, -0.1475,  0.2052, -0.1338, -0.0567, -0.0529])), ('gru.bias_hh', tensor([-0.1128, -0.0695,  0.1322, -0.0905, -0.0191,  0.2465,  0.1251,  0.1821,\n",
      "         0.2098,  0.2290, -0.2491,  0.0754,  0.1256, -0.0880,  0.1650, -0.1993,\n",
      "        -0.1848,  0.1570,  0.1540,  0.0021,  0.0638, -0.1587,  0.2162, -0.1062,\n",
      "        -0.0296,  0.1711,  0.0662, -0.0923,  0.0826, -0.0427, -0.0938,  0.2477,\n",
      "         0.2472,  0.0368,  0.0797,  0.1354, -0.2320,  0.1755,  0.2318,  0.1166,\n",
      "         0.1811, -0.2014,  0.1388, -0.2458,  0.1347, -0.0199,  0.1574,  0.1105])), ('fc.weight', tensor([[-0.5785, -0.6779, -0.0852,  0.5592,  0.6544,  0.0169,  0.5636,  0.5308,\n",
      "         -0.0518,  0.2120,  0.5863, -0.4940,  0.4581,  0.3571, -0.2798,  0.7057],\n",
      "        [ 0.2910,  0.9247,  0.6014,  0.6401,  0.5535, -0.5835,  0.4397, -0.3494,\n",
      "          0.3118, -0.2975,  0.3008, -0.6418, -0.0431,  0.2817,  0.1769,  0.4955],\n",
      "        [ 0.3355,  0.3852, -0.0046,  0.3011, -0.3278, -0.6332,  0.5165, -0.1244,\n",
      "          0.3530, -0.2879,  0.6092,  0.3987, -0.0132,  0.4921, -1.0737,  0.4567],\n",
      "        [-1.0702,  0.1554,  1.3752, -0.8338, -0.7443, -0.3570, -0.9703,  0.9578,\n",
      "          0.8157, -0.8907,  0.5323,  0.7430, -1.2910,  1.0523,  0.6685, -0.8982]])), ('fc.bias', tensor([-0.0019, -0.1672, -0.0550, -0.4076])), ('gru_cells.0.weight_ih', tensor([[ 0.5666,  0.0870, -0.1255,  ..., -0.4168, -0.2786, -0.2880],\n",
      "        [-0.4883, -0.1303, -0.0866,  ..., -0.1532,  0.4751,  0.1685],\n",
      "        [-0.3269, -0.3051,  0.2439,  ...,  0.8895, -0.0860, -0.2267],\n",
      "        ...,\n",
      "        [ 0.1382, -0.4220, -0.2182,  ...,  0.0397,  0.3956, -0.2447],\n",
      "        [-0.0098,  0.2782, -0.1034,  ..., -0.1529,  0.2475,  0.1965],\n",
      "        [-0.3427,  0.1596,  0.2083,  ...,  0.1660, -0.1232, -0.0753]])), ('gru_cells.0.weight_hh', tensor([[ 2.4492e-01, -2.1379e-01, -9.4240e-02,  2.1507e-01,  1.0696e-01,\n",
      "         -7.3976e-02, -1.9870e-01, -9.6201e-02,  2.1774e-01,  2.0991e-01,\n",
      "          1.6994e-02, -1.5059e-01,  1.0285e-01, -2.1415e-01, -2.1764e-01,\n",
      "          1.3636e-01],\n",
      "        [ 4.9129e-02,  1.9541e-01, -7.0312e-02, -4.1558e-02, -5.5704e-02,\n",
      "          1.6026e-01, -1.8517e-01,  1.9405e-01, -1.5565e-02,  6.0744e-02,\n",
      "         -1.3852e-01,  2.3376e-01, -1.0742e-01, -1.9455e-01, -2.7342e-02,\n",
      "          2.8270e-02],\n",
      "        [ 1.4673e-01, -2.2210e-01, -1.7256e-01,  1.8029e-01,  1.8938e-01,\n",
      "         -9.2362e-02, -3.9601e-02, -1.2035e-01,  2.4433e-01, -1.0988e-01,\n",
      "         -3.6660e-02, -2.4462e-01, -2.0021e-01,  1.3980e-01, -2.3135e-01,\n",
      "         -4.5601e-02],\n",
      "        [-2.1213e-01, -1.3766e-01, -2.3215e-01,  1.1806e-01, -1.8845e-01,\n",
      "          2.3539e-01,  4.6744e-02,  2.0976e-01,  6.3921e-02, -2.2936e-01,\n",
      "         -2.2588e-01, -1.4788e-01,  1.6643e-01,  2.4361e-01, -1.9520e-01,\n",
      "         -8.8282e-02],\n",
      "        [-1.4547e-01,  1.6435e-01, -2.1413e-01, -2.0212e-01,  6.8486e-02,\n",
      "          2.2896e-01,  1.5403e-01,  8.6679e-02,  7.9886e-02, -2.1209e-01,\n",
      "          2.0515e-01,  7.6765e-02,  2.0030e-01,  2.2735e-01, -1.2189e-01,\n",
      "         -1.9123e-01],\n",
      "        [-1.3314e-01,  1.9520e-01,  1.9114e-01,  2.2279e-02,  5.7389e-02,\n",
      "          4.6146e-02, -1.2856e-01,  3.0910e-02, -1.1908e-01, -1.8259e-01,\n",
      "         -1.0920e-01, -7.8823e-02, -1.4626e-01, -2.0530e-01, -3.5929e-02,\n",
      "          2.3566e-01],\n",
      "        [-8.1002e-02,  1.5085e-01,  9.9195e-02,  7.9752e-02, -1.0312e-01,\n",
      "         -1.6524e-01,  3.1009e-02,  2.4381e-01,  6.6297e-02,  1.6658e-01,\n",
      "          7.3225e-02,  1.3623e-01, -1.7813e-01,  1.4717e-01, -2.2736e-01,\n",
      "          1.5124e-01],\n",
      "        [ 1.3499e-01, -2.4102e-02, -1.8025e-01, -2.4207e-01,  2.0640e-01,\n",
      "         -1.9248e-01, -1.7456e-01,  7.1717e-02,  1.1050e-01,  4.4384e-02,\n",
      "          1.5523e-01,  3.9777e-02,  2.9169e-02, -1.1773e-01,  9.2181e-02,\n",
      "          2.5910e-03],\n",
      "        [ 1.1131e-01, -1.4255e-02, -2.2610e-02, -8.4890e-02, -1.5503e-01,\n",
      "         -1.4272e-01,  1.0581e-01,  1.0897e-01, -9.2139e-02, -1.6974e-02,\n",
      "         -2.4783e-01,  5.4025e-02,  7.8513e-02,  1.5905e-02, -1.6627e-01,\n",
      "          2.1080e-01],\n",
      "        [-6.1792e-02, -1.6140e-01,  7.6780e-02, -2.4148e-01,  2.0126e-01,\n",
      "          2.0976e-01,  1.5896e-01, -1.5849e-01, -4.1832e-02, -3.2630e-03,\n",
      "          1.6288e-02,  6.0269e-02, -5.1975e-02, -1.1098e-01,  2.0559e-02,\n",
      "          1.9227e-01],\n",
      "        [ 8.1665e-02,  1.0725e-02,  1.9155e-01,  1.6466e-01,  1.8766e-02,\n",
      "          1.7622e-01,  2.3180e-01,  7.2982e-02, -1.0655e-01, -1.0372e-01,\n",
      "          1.6735e-01,  1.2074e-01, -1.4430e-01, -1.0339e-01, -2.4453e-01,\n",
      "         -2.4377e-01],\n",
      "        [ 1.6481e-01,  1.4008e-01,  1.6210e-01,  7.1241e-02,  1.8791e-01,\n",
      "          1.0820e-01,  3.0363e-02, -9.9162e-02,  8.0168e-02,  1.7695e-01,\n",
      "          4.8358e-03, -1.4163e-01,  9.6994e-02, -1.8945e-01,  1.6765e-02,\n",
      "          2.3582e-01],\n",
      "        [-2.1763e-01,  2.4621e-01, -1.8831e-01,  8.7118e-02, -7.0134e-02,\n",
      "         -1.2227e-01, -9.5780e-02, -8.5044e-02,  8.7796e-02, -1.5942e-02,\n",
      "         -9.3596e-02, -7.9735e-02, -2.6487e-02, -4.6411e-02, -1.1189e-01,\n",
      "         -3.8343e-02],\n",
      "        [-1.1685e-01, -8.1323e-02, -2.0377e-01, -2.3784e-01, -1.0310e-01,\n",
      "         -2.2476e-01,  2.2395e-01, -2.2277e-01, -2.3638e-01,  2.2824e-01,\n",
      "         -5.0167e-02,  1.8691e-02,  2.7151e-03, -6.3088e-02,  1.5116e-01,\n",
      "          6.4604e-02],\n",
      "        [ 2.2194e-01,  2.1116e-01,  1.5491e-01,  1.5219e-01, -8.7701e-02,\n",
      "          2.5157e-02, -3.7468e-02, -2.1592e-02,  4.1330e-02,  2.2540e-01,\n",
      "          3.4533e-02,  1.2028e-01,  1.7568e-01, -1.8749e-01,  4.0001e-02,\n",
      "         -5.4664e-02],\n",
      "        [ 1.7136e-01,  2.2616e-01, -1.9486e-01,  2.4782e-01, -5.0313e-02,\n",
      "         -2.7079e-02,  1.2187e-01,  1.4375e-01,  1.1625e-01,  3.7557e-02,\n",
      "         -1.4050e-01,  2.2964e-02,  1.8554e-01, -1.3116e-01,  2.2903e-01,\n",
      "         -1.0845e-02],\n",
      "        [-1.1294e-01, -2.3729e-01,  1.4119e-01,  4.4836e-02,  1.6504e-03,\n",
      "          8.3428e-02,  5.3717e-02,  1.8953e-01,  5.4058e-02,  2.4827e-01,\n",
      "          1.5088e-01, -7.3923e-02,  9.6765e-02,  1.0512e-01, -5.7806e-02,\n",
      "         -2.0678e-01],\n",
      "        [ 1.4116e-01, -8.5514e-02,  1.6802e-01, -2.1490e-01, -3.0710e-02,\n",
      "         -1.3290e-01, -1.8243e-01,  6.7372e-02,  2.2942e-02,  8.6253e-02,\n",
      "         -2.6945e-03, -8.5934e-02, -1.0075e-01, -1.8909e-01, -2.0967e-01,\n",
      "         -3.3194e-02],\n",
      "        [-1.8036e-01, -1.6393e-01,  3.5985e-02,  1.2932e-01, -1.6541e-01,\n",
      "         -1.2115e-01,  9.8534e-02, -1.7619e-01,  4.1058e-02, -1.7854e-01,\n",
      "          2.0630e-01,  2.2516e-01, -2.3179e-01,  7.2542e-02,  9.7380e-02,\n",
      "          1.0385e-01],\n",
      "        [ 1.1148e-01,  4.9712e-02, -1.0503e-01, -2.1894e-01, -3.0310e-02,\n",
      "          3.6210e-02, -2.3779e-01, -1.4474e-01, -9.6167e-02,  1.1630e-01,\n",
      "          1.1233e-02,  2.3712e-01,  1.6853e-01, -1.6939e-01,  2.4752e-01,\n",
      "          6.2558e-02],\n",
      "        [-1.1409e-01,  7.3141e-02,  2.2874e-01,  1.9230e-01, -7.3749e-02,\n",
      "         -8.0644e-02,  1.9196e-01,  1.3161e-01,  2.2009e-01, -9.1216e-02,\n",
      "         -9.6394e-02,  6.4567e-02, -2.8819e-02, -1.1312e-01, -1.3705e-03,\n",
      "         -2.0789e-01],\n",
      "        [ 2.2317e-02, -1.5602e-01,  1.2368e-02,  1.6709e-01, -2.8199e-02,\n",
      "          2.4226e-01,  2.3172e-01, -2.4157e-01,  2.3871e-01,  1.8519e-01,\n",
      "         -2.4728e-01, -1.4814e-01, -2.1016e-02, -5.4566e-02, -5.5259e-02,\n",
      "         -1.1771e-01],\n",
      "        [-6.1448e-02,  5.7870e-02, -1.1494e-02,  2.2170e-01, -1.0402e-01,\n",
      "          2.3492e-01, -6.0231e-03, -4.7438e-02,  2.7010e-02, -1.0804e-01,\n",
      "         -3.7124e-02,  6.1347e-02, -1.5761e-01, -4.1930e-02,  2.3690e-01,\n",
      "         -2.4711e-01],\n",
      "        [-1.1327e-01, -1.6103e-01,  1.0705e-01,  2.1891e-01, -3.2944e-02,\n",
      "          1.5415e-01,  5.8085e-02, -1.4485e-01,  2.4301e-01, -1.4645e-01,\n",
      "         -2.2488e-01, -1.0609e-01,  6.5084e-02,  8.5559e-02, -6.1738e-02,\n",
      "          3.8871e-04],\n",
      "        [ 1.4793e-02,  6.7053e-02,  1.9061e-01, -1.1200e-01, -7.9166e-03,\n",
      "          7.5152e-02,  8.1725e-02,  2.3765e-01, -2.1495e-01,  1.6050e-02,\n",
      "          1.4567e-01,  2.0513e-01,  1.1535e-02, -1.3071e-01,  2.4173e-01,\n",
      "         -8.5703e-02],\n",
      "        [ 4.8230e-02, -1.9381e-01,  1.4825e-01,  2.0526e-01,  1.3867e-01,\n",
      "         -7.1925e-02,  1.4566e-01, -2.4134e-01,  1.3532e-01,  1.8073e-01,\n",
      "          1.2328e-01, -1.1172e-01,  1.4308e-02,  2.4374e-01,  2.3521e-01,\n",
      "          4.9961e-02],\n",
      "        [ 3.3462e-02, -6.7017e-02,  2.9529e-02,  8.1360e-06,  6.0510e-02,\n",
      "         -2.3671e-01, -9.4274e-02, -7.3342e-02, -1.3081e-01, -2.4175e-01,\n",
      "          6.8913e-02, -1.8108e-01,  4.6262e-02,  1.7060e-01, -1.9258e-01,\n",
      "          7.2618e-02],\n",
      "        [ 3.7617e-02,  1.9436e-01, -1.3993e-01,  1.5398e-01,  1.5865e-01,\n",
      "         -2.1014e-01,  6.7047e-02,  1.6248e-01,  1.0585e-01, -1.6545e-01,\n",
      "          1.7638e-01,  2.0476e-01,  2.0443e-01,  1.5013e-01, -2.4547e-01,\n",
      "          2.3479e-01],\n",
      "        [ 9.3546e-02, -4.9222e-02, -9.5996e-02, -1.8773e-01,  7.0449e-02,\n",
      "          2.1226e-01, -9.5954e-02, -2.0640e-01,  2.4802e-02,  6.6082e-02,\n",
      "          1.6003e-01,  7.6370e-02, -1.4154e-01, -5.4519e-02,  2.1745e-01,\n",
      "          2.3169e-01],\n",
      "        [-9.3988e-02,  1.1662e-01, -8.4696e-02, -1.5206e-01, -2.2851e-01,\n",
      "          1.1553e-01,  1.8494e-02,  2.8321e-02,  2.2742e-01, -8.9224e-02,\n",
      "         -3.3015e-02, -2.9860e-02, -1.9379e-01, -1.2698e-01, -8.0817e-02,\n",
      "         -1.5239e-02],\n",
      "        [-1.8694e-01, -2.3529e-01, -1.9419e-01,  5.6398e-02,  4.0276e-02,\n",
      "         -2.2925e-01,  3.9812e-02,  3.6138e-02, -9.6623e-02, -1.5519e-01,\n",
      "         -1.5244e-02, -1.1790e-01,  2.0099e-01, -1.0345e-01, -1.9911e-01,\n",
      "          4.8635e-02],\n",
      "        [-3.7663e-02, -2.3808e-01,  1.5408e-01, -4.3728e-02, -1.8955e-01,\n",
      "         -1.9675e-01, -2.3065e-01,  2.0729e-01, -1.2302e-01, -4.7972e-02,\n",
      "         -3.8394e-02, -4.7246e-02,  7.9467e-02, -1.7903e-01,  1.4559e-01,\n",
      "          1.5769e-01],\n",
      "        [ 1.1061e-01, -5.5357e-02,  2.2229e-01,  1.4437e-01, -1.5607e-01,\n",
      "          1.9190e-01,  1.6133e-01,  1.4028e-01,  6.8189e-02, -2.2897e-01,\n",
      "          2.3440e-02, -1.3061e-01, -9.6475e-02, -4.2624e-02, -2.3455e-01,\n",
      "         -1.1170e-01],\n",
      "        [ 9.6238e-02, -1.3278e-01, -1.6127e-01,  1.2828e-02,  2.0819e-01,\n",
      "          4.8849e-04,  2.1931e-01,  1.3691e-01,  4.2867e-02,  1.1475e-01,\n",
      "         -2.1574e-01,  1.1830e-01,  1.4515e-01,  8.1680e-02, -3.8905e-02,\n",
      "          2.3689e-01],\n",
      "        [-1.5302e-01,  1.9717e-01,  2.2884e-01, -6.9823e-02, -1.0367e-01,\n",
      "          1.9919e-02, -1.9774e-01,  4.1482e-02,  5.6162e-03,  5.0213e-02,\n",
      "         -4.6103e-02,  4.5942e-02, -4.2556e-02,  1.9903e-01,  5.1017e-02,\n",
      "         -5.0493e-02],\n",
      "        [-2.0441e-01, -5.2589e-02, -1.4675e-01,  2.0099e-01, -1.1333e-01,\n",
      "          2.8408e-02,  1.6815e-01, -4.5847e-02,  1.1825e-01,  2.7499e-02,\n",
      "          6.7772e-02,  1.1174e-01, -1.9857e-01,  8.1666e-02,  1.1925e-01,\n",
      "         -8.7373e-02],\n",
      "        [-4.1010e-02,  1.1798e-01,  2.7816e-02, -7.6188e-02, -2.4591e-01,\n",
      "         -1.8430e-01,  1.4958e-02,  2.3037e-01,  1.5861e-01,  3.3460e-02,\n",
      "         -1.9235e-01, -2.3572e-01,  8.1479e-02,  2.0020e-01,  2.1874e-01,\n",
      "         -1.9748e-01],\n",
      "        [-1.5098e-01, -2.1260e-02, -1.3084e-01,  7.3934e-02,  1.0711e-03,\n",
      "          2.0928e-01, -8.3979e-02,  6.8021e-02,  1.0293e-01,  1.5239e-01,\n",
      "          1.8959e-01, -1.3650e-01, -5.2568e-02,  6.6199e-02,  1.3536e-01,\n",
      "         -1.4644e-01],\n",
      "        [ 4.4371e-02, -8.7292e-02, -1.6950e-01, -1.8616e-01,  1.2465e-01,\n",
      "         -1.0511e-01,  1.9361e-01, -1.3454e-01, -2.2423e-01, -2.6946e-02,\n",
      "         -7.9931e-02, -5.1184e-02, -1.6249e-01, -1.5024e-01,  1.2357e-01,\n",
      "          5.3056e-02],\n",
      "        [ 1.3613e-01,  4.3536e-02, -2.0929e-01, -6.4948e-02,  8.4352e-02,\n",
      "          1.6944e-01, -7.4750e-02,  2.1972e-02,  2.4038e-01, -8.1779e-02,\n",
      "          1.2380e-01,  1.3288e-01, -6.7453e-02,  2.2059e-01,  1.8198e-02,\n",
      "         -2.2923e-01],\n",
      "        [-1.9332e-01,  3.8890e-02,  1.8368e-01, -1.0303e-01, -1.5091e-01,\n",
      "         -2.3915e-01, -1.0712e-01,  9.4064e-02,  1.7498e-01,  1.3377e-01,\n",
      "          6.8371e-03, -1.7917e-02, -1.5868e-01,  1.1657e-01, -2.8531e-02,\n",
      "          2.0651e-01],\n",
      "        [ 1.8194e-02, -2.9402e-02,  1.4955e-01, -9.2991e-02, -5.3638e-02,\n",
      "         -3.8172e-02, -1.9985e-01,  1.5222e-01, -2.4440e-01,  1.3393e-01,\n",
      "         -1.5335e-01, -9.8717e-02, -1.2222e-01, -2.8371e-02,  5.4288e-02,\n",
      "         -1.1098e-01],\n",
      "        [ 6.9213e-02,  1.6748e-01,  5.8753e-02, -5.5374e-02, -6.5864e-02,\n",
      "          3.3212e-02,  1.2813e-01,  2.3350e-01,  6.2447e-02,  4.0799e-02,\n",
      "         -9.6787e-02, -2.2446e-01,  1.4597e-01, -9.6933e-02,  4.7337e-02,\n",
      "         -2.4176e-01],\n",
      "        [ 1.9469e-01, -2.2379e-02,  1.1926e-02, -7.6519e-02,  1.5715e-01,\n",
      "         -1.8596e-01,  1.0741e-01,  1.5524e-01, -1.5710e-01, -2.0823e-01,\n",
      "          4.8183e-02,  1.0562e-01,  1.2850e-02,  1.1512e-01,  1.8715e-01,\n",
      "         -8.6709e-02],\n",
      "        [ 6.6339e-02,  7.1800e-02, -1.6993e-01,  1.7219e-01,  6.9610e-02,\n",
      "          4.0852e-02,  8.7145e-02,  1.8829e-01, -1.3229e-02, -1.7427e-01,\n",
      "         -2.3201e-01, -1.1331e-01, -1.9947e-01, -1.1336e-01, -7.6686e-02,\n",
      "         -1.7479e-02],\n",
      "        [-1.6670e-01,  1.0572e-01, -1.9430e-01,  2.3662e-01,  1.5930e-01,\n",
      "          2.2693e-01, -6.3341e-02, -8.2109e-02, -1.2276e-01, -1.9325e-01,\n",
      "          6.3022e-02, -1.4067e-01,  4.3248e-02,  6.6231e-02,  1.7303e-01,\n",
      "          1.6221e-01],\n",
      "        [ 5.5586e-02, -1.9066e-01, -8.0141e-02, -1.9005e-01, -2.2986e-01,\n",
      "          2.4851e-01, -1.7353e-01,  1.5139e-02,  2.2531e-01, -1.2724e-01,\n",
      "          3.2650e-02, -1.2633e-01, -6.8689e-02,  2.4917e-01,  1.3895e-01,\n",
      "          6.9924e-03],\n",
      "        [-1.2908e-02, -6.0685e-03,  2.3569e-01, -6.3875e-02, -1.9401e-01,\n",
      "         -6.2387e-02, -2.3645e-01,  6.3251e-02, -5.6375e-02, -1.3735e-01,\n",
      "         -2.2102e-01, -1.1273e-01, -2.1847e-01,  2.0270e-01,  1.0508e-01,\n",
      "         -1.0741e-01]])), ('gru_cells.0.bias_ih', tensor([-0.6253,  0.2865,  0.1509, -0.2643,  0.0592, -0.3789, -0.6745, -0.2876,\n",
      "         0.3942, -0.3978,  0.4545, -0.1163,  0.0929,  0.0849,  0.3178,  0.0743,\n",
      "        -0.1960,  0.1340, -0.2693,  0.0913, -0.3697, -0.0351,  0.2496, -0.5813,\n",
      "         0.1533,  0.3125,  0.3275, -0.0762, -0.3500,  0.8503,  0.3161,  0.0499,\n",
      "        -0.1034,  0.1342, -0.0509, -0.3059, -0.3325,  0.1148, -0.5731, -0.4238,\n",
      "        -0.1084, -0.2069, -0.4084,  0.1686, -0.2010, -0.1773,  0.2493, -0.1596])), ('gru_cells.0.bias_hh', tensor([-0.3726,  0.1780, -0.2724, -0.2754, -0.1595, -0.4445, -0.3463, -0.4058,\n",
      "         0.1627, -0.1103,  0.3375, -0.3274,  0.0459,  0.3007,  0.1202, -0.2184,\n",
      "         0.0023,  0.3078, -0.1740,  0.2027, -0.3038,  0.2777,  0.1475, -0.8951,\n",
      "         0.2872,  0.4649,  0.3766, -0.3290, -0.0631,  1.2118,  0.2718, -0.2283,\n",
      "         0.7139, -0.4688, -0.6103,  0.4281,  0.2418,  0.7000,  0.2898,  0.1066,\n",
      "        -0.6116,  0.2955, -0.9461, -0.1843,  0.1485, -0.4517,  0.4582, -0.2759])), ('batch_norm.weight', tensor([2.4306, 2.3493, 2.3927, 2.1587, 2.3563, 1.9807, 2.2209, 2.2359, 2.1913,\n",
      "        2.1413, 2.2623, 2.7209, 2.2809, 2.6705, 2.8919, 2.0761])), ('batch_norm.bias', tensor([0.6338, 1.2875, 0.3308, 0.6628, 0.5654, 0.2968, 0.6535, 0.5218, 0.0620,\n",
      "        0.1760, 0.3288, 0.1804, 0.6895, 0.2465, 0.6435, 0.6541])), ('batch_norm.running_mean', tensor([-0.1282, -0.0961, -0.2588, -0.0624, -0.3870,  0.3088, -0.1192, -0.4371,\n",
      "        -0.0151, -0.0295, -0.1218,  0.0242, -0.2792, -0.0054,  0.0565, -0.2469])), ('batch_norm.running_var', tensor([0.0003, 0.0009, 0.0006, 0.0002, 0.0001, 0.0017, 0.0045, 0.0010, 0.0025,\n",
      "        0.0078, 0.0025, 0.0002, 0.0004, 0.0008, 0.0003, 0.0018])), ('batch_norm.num_batches_tracked', tensor(1500))])\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model is named 'model'\n",
    "state_dict = model.state_dict()\n",
    "print(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model is named 'model'\n",
    "full_model = 'model_standard20.pth'\n",
    "torch.save(model.state_dict(), full_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUCellNet(\n",
       "  (gru): GRUCell(4, 4)\n",
       "  (fc): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (softplus): Softplus(beta=1, threshold=20)\n",
       "  (relu): ReLU()\n",
       "  (gru_cells): ModuleList(\n",
       "    (0): GRUCell(4, 4)\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved weights into the model\n",
    "#torch.save(model.state_dict(), 'model_weights.pth')\n",
    "#model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model.load_state_dict(torch.load('baseline_model.pth'))\n",
    "model.eval()  # Set the model in evaluation mode\n",
    "#gowno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Forward pass to get predictions\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m----> 3\u001b[0m epoch_loss, accuracy, predicted_labels, predicted_probs \u001b[39m=\u001b[39m validate(test_dataloader,model,loss_fn)\n\u001b[0;32m      4\u001b[0m predicted_labels\n",
      "Cell \u001b[1;32mIn[10], line 27\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(dataloader, model, loss_fn, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m         epoch_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mnumel()\n\u001b[0;32m     25\u001b[0m         predicted_labels\u001b[39m.\u001b[39mextend(\u001b[39mzip\u001b[39m(y_pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), y\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()))\n\u001b[1;32m---> 27\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "# Forward pass to get predictions\n",
    "model.eval()\n",
    "epoch_loss, accuracy, predicted_labels, predicted_probs = validate(test_dataloader,model,loss_fn)\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('baseline_model.pth'))  # Load saved weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "# Initialize an empty list to store predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in test_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, _ = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        preds = torch.round(probabilities)\n",
    "\n",
    "    # Append predictions to the list\n",
    "    all_predictions.append(preds)\n",
    "\n",
    "# Concatenate the predicted batches\n",
    "all_predictions = torch.cat(all_predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each output state:\n",
      "0.9970588235294118\n"
     ]
    }
   ],
   "source": [
    "# Convert the tensor of predictions to a DataFrame\n",
    "predictions_df = pd.DataFrame(all_predictions.numpy(), columns=y_test.columns, index=y_test.index)\n",
    "# Calculate accuracy for each output state\n",
    "accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "print(\"Accuracy for each output state:\")\n",
    "print(np.mean(accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
