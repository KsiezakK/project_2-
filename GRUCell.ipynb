{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data (with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with scaling\n",
    "df = pd.read_csv(\"C:/Users/kacpe/Desktop/study/research lab/data_model_v2.csv\")\n",
    "# List of column names to drop\n",
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "#columns_to_keep =  ['id', 'trial','lShoulder_x', 'lShoulder_y', 'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x', 'lWrist_y', 'lWrist_z']\n",
    "#df = df.drop(columns=columns_to_drop)\n",
    "#df = df[columns_to_keep]\n",
    "# Step 1: Separate 'id' and 'trial' columns from the rest of the data\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "\n",
    "# Step 2: Apply MinMaxScaler to the remaining columns\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "\n",
    "# Step 3: Merge 'id' and 'trial' columns with the scaled data\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "\n",
    "# Step 4: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = scaled_df[scaled_df['trial'].isin(range(1, 15))].drop(columns=['id', 'trial'])\n",
    "test_set = scaled_df[scaled_df['trial']==15].drop(columns=['id', 'trial'])\n",
    "val_set = scaled_df[scaled_df['trial']==16].drop(columns=['id', 'trial'])\n",
    "full_set = scaled_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.iloc[:,:-4], train_set.iloc[:,-4:]\n",
    "X_test, y_test = test_set.iloc[:,:-4], test_set.iloc[:,-4:]\n",
    "X_val, y_val = val_set.iloc[:,:-4], val_set.iloc[:,-4:]\n",
    "X, y = full_set.iloc[:,:-4], full_set.iloc[:,-4:]\n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "# Create a DataLoader\n",
    "#batch_size = 5561#67  # Set your desired batch size\n",
    "#shuffle = False  # Set to False to preserve the order of your data\n",
    "#fullset_b_size = X.shape[0]/2\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)\n",
    "        #Xavier initialization for GRU weights\n",
    "        #for name, param in self.gru.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        init.xavier_uniform_(param.data)\n",
    "        #    elif 'bias' in name:\n",
    "        #        init.constant_(param.data, 0.0)\n",
    "                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sotfplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.sotfplus(out[:,-1]))\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out, h\n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "        #weight = next(self.parameters()).data\n",
    "        #hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        #return hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size > 1:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.gru_cells2 = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]  # Update input_t with the new hidden state for the next layer\n",
    "            hidden_states.append(new_hidden_states)\n",
    "        \n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        # Apply BatchNorm to the last hidden state\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        #all_hidden_states = torch.cat(hidden_states, dim=1)  # Stack all hidden states across time steps\n",
    "        \n",
    "        \n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "        #out = self.fc(self.softplus(all_hidden_states[:, -1]))  # Use the last hidden state for prediction\n",
    "        #probs = torch.sigmoid(out)  # Apply sigmoid activation to get probabilities\n",
    "        #preds = torch.round(probs)  # Convert probabilities to binary predictions\n",
    "        #out = F.sigmoid(out, dim=1)\n",
    "        \n",
    "        return out, last_hidden_states \n",
    "    \n",
    "    \n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "     #   return nn.Parameter(torch.zeros([batch_size, self.gru_cells[0].hidden_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6107, 54)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "input_dim = 54\n",
    "hidden_dim = 22\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =2500\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.9960874915122986, train acc: 0.6050433928279024, val loss: 1.1395457983016968, val acc: 0.41044776119402987\n",
      "Epoch 50, train loss: 0.42967745661735535, train acc: 0.8333879155067955, val loss: 0.5757481455802917, val acc: 0.7977611940298508\n",
      "Epoch 100, train loss: 0.31419649720191956, train acc: 0.8929916489274603, val loss: 0.5936166644096375, val acc: 0.8298507462686567\n",
      "Epoch 150, train loss: 0.2575354278087616, train acc: 0.9199688881611265, val loss: 0.5125163793563843, val acc: 0.8880597014925373\n",
      "Epoch 200, train loss: 0.22230887413024902, train acc: 0.9311855248075979, val loss: 0.4626430571079254, val acc: 0.8835820895522388\n",
      "Epoch 250, train loss: 0.19782328605651855, train acc: 0.9391272310463402, val loss: 0.5209224224090576, val acc: 0.8813432835820896\n",
      "Epoch 300, train loss: 0.17835651338100433, train acc: 0.9460045849025708, val loss: 0.5869655609130859, val acc: 0.8835820895522388\n",
      "Epoch 350, train loss: 0.1628793627023697, train acc: 0.9500163746520387, val loss: 0.6388669610023499, val acc: 0.8835820895522388\n",
      "Epoch 400, train loss: 0.1504082977771759, train acc: 0.9560340592762404, val loss: 0.6731618642807007, val acc: 0.8858208955223881\n",
      "Epoch 450, train loss: 0.14069879055023193, train acc: 0.959963975765515, val loss: 0.7105636596679688, val acc: 0.8850746268656716\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, val_losses, val_accs, train_predicted, val_predicted, train_probs, val_probs= run_training(\n",
    "    train_dataloader, val_dataloader=val_dataloader, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUCellNet(\n",
       "  (gru): GRUCell(54, 20)\n",
       "  (fc): Linear(in_features=20, out_features=4, bias=True)\n",
       "  (softplus): Softplus(beta=1, threshold=20)\n",
       "  (relu): ReLU()\n",
       "  (gru_cells2): ModuleList(\n",
       "    (0): GRUCell(54, 20)\n",
       "  )\n",
       "  (gru_cells): ModuleList(\n",
       "    (0): GRUCell(54, 20)\n",
       "  )\n",
       "  (batch_norm): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved weights into the model\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "#model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set the model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03769258, 0.99743134, 0.99305105, 0.58299375], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_probs[0][500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0.], dtype=float32),\n",
       " array([1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds[15261393:15261393+6107][:6108][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>s_3</th>\n",
       "      <th>s_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6104</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6106</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6107 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      s_1  s_2  s_3  s_4\n",
       "0     1.0  1.0  1.0  1.0\n",
       "1     1.0  1.0  1.0  1.0\n",
       "2     1.0  1.0  1.0  1.0\n",
       "3     1.0  1.0  1.0  1.0\n",
       "4     1.0  1.0  1.0  1.0\n",
       "...   ...  ...  ...  ...\n",
       "6102  0.0  1.0  0.0  0.0\n",
       "6103  0.0  1.0  0.0  0.0\n",
       "6104  0.0  1.0  0.0  0.0\n",
       "6105  0.0  1.0  0.0  0.0\n",
       "6106  0.0  1.0  0.0  0.0\n",
       "\n",
       "[6107 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred_probs, _, hidden = model(x)  # Get probabilities and hidden states\n",
    "        \n",
    "        loss = loss_fn(y_pred_probs, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_pred_probs)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_probs.extend(y_pred_probs.cpu().detach().numpy())\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().numpy(), y.cpu().numpy()))\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    #all_hidden_states = torch.cat([torch.stack(layer_states) for layer_states in hidden_states], dim=1)  # Concatenate tensors within each time step\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels,predicted_probs #all_hidden_states \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "    train_predicted_labels = []  \n",
    "    val_predicted_labels = []  \n",
    "    train_hidden_states = []  \n",
    "    val_hidden_states = []  \n",
    "    train_predicted_probs = []  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #epoch_train_loss, epoch_train_acc, train_preds, train_hidden, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        train_predicted_labels.extend(train_preds)\n",
    "        train_hidden_states.extend(train_hidden)\n",
    "        train_predicted_probs.extend(train_probs)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_hidden, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "            val_predicted_labels.extend(val_preds)\n",
    "            val_hidden_states.extend(val_hidden)\n",
    "            val_predicted_probs.extend(val_probs)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "            \n",
    "    return (\n",
    "        train_losses, train_accs, val_losses, val_accs,\n",
    "        train_predicted_labels, val_predicted_labels,\n",
    "        train_predicted_probs, val_predicted_probs )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
