{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If GPU is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]  # Update input_t with the new hidden state for the next layer\n",
    "            hidden_states.append(new_hidden_states)\n",
    "        \n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        # Apply BatchNorm to the last hidden state\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "        #out = self.fc(last_hidden_states[-1])\n",
    "        #out = torch.sigmoid(out)\n",
    "        \n",
    "        return out, last_hidden_states "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states_batch = []  # Initialize a list to store hidden states for the current batch        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        # Append the batch's hidden states to the list\n",
    "        for layer_state in last_hidden_states:\n",
    "            hidden_states_batch.append(layer_state.cpu().detach().numpy())\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        # Append the batch's hidden states to the overall hidden_states list\n",
    "        hidden_states.append(hidden_states_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step(loss)\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m columns_to_drop \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlKnee_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlKnee_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlKnee_z\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlAnkle_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlAnkle_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlAnkle_z\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrKnee_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrKnee_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrKnee_z\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrAnkle_x\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrAnkle_y\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mrAnkle_z\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata_model_v3.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39mcolumns_to_drop)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39ms_1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ms_2\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ms_3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39ms_4\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        predictions_dict_sm = {}\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "        # Construct the key for the combination\n",
    "        key = f\"sm_shift{shift}_set{set_values}\"\n",
    "\n",
    "        # Store the predictions as NumPy arrays in the dictionary\n",
    "        predictions_dict_sm[key] = all_preds_array \n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        predictions_dict_smh = {}\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "        # Construct the key for the combination\n",
    "        key = f\"shift{shift}_set{set_values}\"\n",
    "\n",
    "        # Store the predictions as NumPy arrays in the dictionary\n",
    "        predictions_dict_smh[key] = all_preds_array\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "        # Filter the DataFrame for the current trial\n",
    "        trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "        # Create shifted columns for each column in columns_to_shift\n",
    "        for col in columns_to_shift:\n",
    "            new_col_name = col + '_minus_' + str(shift)\n",
    "            trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "        # Drop the last 'i' records for each trial\n",
    "        trial_df = trial_df.dropna()\n",
    "\n",
    "        # Append the modified trial_df to the shifted_df\n",
    "        shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "    \n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['H_trial','H_id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['H_trial','H_id','s_1','s_2','s_3','s_4']] = df[['H_trial','H_id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['H_trial','H_id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,152,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['H_trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['H_trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['H_id', 'H_trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['H_trial']!=set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        test_set = shifted_df[shifted_df['H_trial']==set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        full_set = shifted_df.drop(columns=['H_id', 'H_trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished smh, left set 14,15 to train \n",
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            \n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "df.rename(columns={'H_id': 'id', 'H_trial': 'trial'}, inplace=True)\n",
    "data_to_scale = df.drop(columns=['trial','id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['trial','id','s_1','s_2','s_3','s_4']] = df[['trial','id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['trial','id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id','trial'])\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sh_overfitting_check.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = df.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "shuffled_df = pd.concat(shuffled_trial_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shift = 2\n",
    "set_values = 5\n",
    "model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the shifted data\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "# Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "    # Filter the DataFrame for the current trial\n",
    "    trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "    # Create shifted columns for each column in columns_to_shift\n",
    "    for col in columns_to_shift:\n",
    "        new_col_name = col + '_minus_' + str(shift)\n",
    "        trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "    # Drop the last 'i' records for each trial\n",
    "    trial_df = trial_df.dropna()\n",
    "\n",
    "    # Append the modified trial_df to the shifted_df\n",
    "    shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "\n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame 'train_set' with a 'trial' column\n",
    "# and you want to shuffle the trials while preserving the order of samples within each trial.\n",
    "\n",
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = train_set.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "train_set = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(1,142,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                        # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "        \n",
    "        X_train = X_train.drop(columns=['id','trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sm_shuffled.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_to_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "        # Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "        X_train = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "        X_train = train_set.drop(columns=['id', 'trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        X_train = X_train.drop(columns=labels)\n",
    "        \n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        #file_name = 'smh_shuffled.txt'\n",
    "        #file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('models_comparison_v3.csv')\n",
    "\n",
    "# Set the \"shift\" column as the index (optional, for better x-axis labels)\n",
    "df = df[::-1]\n",
    "df.set_index('T', inplace=True)\n",
    "\n",
    "# Create a line plot for each model's performance\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define custom colors for each model\n",
    "colors = {\n",
    "    'null': 'silver',\n",
    "    'solo': 'magenta',\n",
    "    'dyadic': 'blue'\n",
    "}\n",
    "\n",
    "# Create a line plot for each model's performance with custom colors\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for model in ['null','solo','dyadic']:\n",
    "    plt.plot(df.index, df[model], marker='o', label=model, color=colors[model])\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlim(max(df.index),min(df.index))\n",
    "plt.xlabel('T (shift in seconds)')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.title('Performance of Four Models')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1','2','3','4']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap plot\n",
    "heatmap = ax.imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Predicted State')\n",
    "ax.set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels\n",
    "ax.set_yticks(np.arange(len(y_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Create the heatmap plot in the first subplot\n",
    "heatmap = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_xlabel('Time Step')\n",
    "axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second plot (line plot) in the second subplot\n",
    "axs[1].plot(np.arange(len(y_val)), y_val, color='green', label='Actual Values')\n",
    "axs[1].set_xlabel('Time Step')\n",
    "axs[1].set_ylabel('Actual Value')\n",
    "axs[1].set_title('Actual Test Values for Y')\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_val\n",
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2,1, figsize=(8,10))\n",
    "\n",
    "# Create the first heatmap plot in the first subplot\n",
    "heatmap1 = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the first heatmap subplot\n",
    "cbar1 = plt.colorbar(heatmap1, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the first heatmap subplot\n",
    "#axs[0].set_xlabel('Time Step')\n",
    "#axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Predicted States Probabilities')\n",
    "\n",
    "# Set the y-axis ticks and labels for the first heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second heatmap plot in the second subplot\n",
    "heatmap2 = axs[1].imshow(actual_values.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the second heatmap subplot\n",
    "cbar2 = plt.colorbar(heatmap2, ax=axs[1])\n",
    "\n",
    "# Set the axis labels and title for the second heatmap subplot\n",
    "axs[1].set_xlabel('Time Step')\n",
    "#axs[1].set_ylabel('Predicted State')\n",
    "axs[1].set_title('True States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the second heatmap subplot\n",
    "axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "axs[1].set_yticklabels(y_labels)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gates and hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None, hidden_states=None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_dim)  # Initialize hidden state for the single layer\n",
    "\n",
    "        hidden_states = []  # Initialize a list to store hidden states at each time step\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            h = self.gru(input_t, h)  # Update the hidden state for the single layer\n",
    "            hidden_states.append(h)  # Append the hidden state for the current time step\n",
    "\n",
    "        h = self.batch_norm(h)  # Apply batch normalization to the final hidden state\n",
    "        out = self.fc(self.softplus(h))  # Calculate the output based on the final hidden state\n",
    "        out = torch.sigmoid(out)\n",
    "        out = torch.round(out)\n",
    "\n",
    "        # Return the output and the list of hidden states for the entire sequence\n",
    "        return out, h, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "            \n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    train_hidden_states, val_hidden_states = [], [] # Initialize lists to store hidden states\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs, train_hidden = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        train_hidden_states.extend(train_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs, val_hidden = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            val_hidden_states.extend(val_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final, train_hidden_states, val_hidden_states\n",
    "    else: \n",
    "        return train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y', 'headTop_z',\n",
       "       'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z',\n",
       "       'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x', 'lShoulder_y',\n",
       "       'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x',\n",
       "       'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z', 'rEar_x',\n",
       "       'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y', 'rShoulder_z',\n",
       "       'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x', 'rWrist_y', 'rWrist_z',\n",
       "       'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, batch_hidden_states = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hidden_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_hidden_states[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array = batch_hidden_states[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs.imshow(hidden_states_array, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs.set_ylabel('hidden unit')\n",
    "axs.set_xlabel('time step')\n",
    "axs.set_title('Heatmap of hidden states')\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_values = np.array([10,80,130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.99318927526474, train acc: 0.6004935459377373\n",
      "Epoch 50, train loss: 0.08497070521116257, train acc: 0.9844343204252088\n",
      "Epoch 0, train loss: 0.43113425374031067, train acc: 0.8743532560214095\n",
      "Epoch 50, train loss: 0.18211303651332855, train acc: 0.9378679750223016\n",
      "Epoch 0, train loss: 0.3486967086791992, train acc: 0.8874617737003058\n",
      "Epoch 50, train loss: 0.2525496780872345, train acc: 0.904638124362895\n",
      "Epoch 0, train loss: 0.43490543961524963, train acc: 0.9182550115018074\n",
      "Epoch 50, train loss: 0.07446524500846863, train acc: 0.9850065724613868\n",
      "Epoch 0, train loss: 0.3527289927005768, train acc: 0.8772032902467685\n",
      "Epoch 50, train loss: 0.20687879621982574, train acc: 0.9199471210340776\n",
      "Epoch 0, train loss: 0.3726874589920044, train acc: 0.8681343622333182\n",
      "Epoch 50, train loss: 0.34488749504089355, train acc: 0.8746028143440763\n",
      "Epoch 0, train loss: 0.20412753522396088, train acc: 0.9521866582158136\n",
      "Epoch 50, train loss: 0.16878153383731842, train acc: 0.9604262398985898\n",
      "Epoch 0, train loss: 0.1832096129655838, train acc: 0.9315794410054399\n",
      "Epoch 50, train loss: 0.18222378194332123, train acc: 0.9335490527105609\n",
      "Epoch 0, train loss: 0.3256382346153259, train acc: 0.876592528611531\n",
      "Epoch 50, train loss: 0.3240588307380676, train acc: 0.877780177067588\n",
      "Epoch 0, train loss: 0.16090203821659088, train acc: 0.9623314829500397\n",
      "Epoch 50, train loss: 0.16017913818359375, train acc: 0.9624504361617764\n",
      "Epoch 0, train loss: 0.18131472170352936, train acc: 0.9329107981220657\n",
      "Epoch 50, train loss: 0.18129833042621613, train acc: 0.9329107981220657\n",
      "Epoch 0, train loss: 0.3272723853588104, train acc: 0.8788648648648648\n",
      "Epoch 50, train loss: 0.3272233009338379, train acc: 0.8788648648648648\n",
      "Epoch 0, train loss: 0.17304927110671997, train acc: 0.956870043696391\n",
      "Epoch 50, train loss: 0.17301636934280396, train acc: 0.956870043696391\n",
      "Epoch 0, train loss: 0.18730944395065308, train acc: 0.9333044816310829\n",
      "Epoch 50, train loss: 0.18730823695659637, train acc: 0.9333044816310829\n",
      "Epoch 0, train loss: 0.33237120509147644, train acc: 0.8749722160480107\n",
      "Epoch 50, train loss: 0.33236998319625854, train acc: 0.8749722160480107\n",
      "Epoch 0, train loss: 0.1716112494468689, train acc: 0.9597658808120497\n",
      "Epoch 50, train loss: 0.17161104083061218, train acc: 0.9597658808120497\n",
      "Epoch 0, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 50, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 0, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 50, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 0, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 50, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 0, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 50, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 0, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 50, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 0, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 50, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 0, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 50, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 0, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 50, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 0, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 50, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 0, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 50, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 0, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 50, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 0, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 50, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 0, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 50, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 0, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 50, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 0, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 50, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 0, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 50, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 0, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 50, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 0, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 50, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 0, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 50, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 0, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 50, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 0, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 50, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 0, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 50, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 0, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 50, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 0, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 50, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 0, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 50, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 0, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 50, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 0, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 50, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 0, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 50, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 0, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n",
      "Epoch 50, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(1,16):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        \n",
    "        # Create a figure with a single subplot\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(16,10))  # Adjust the figsize as needed\n",
    "\n",
    "        # Create the heatmap plot in the single subplot\n",
    "        heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('hidden unit')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        axs[0].set_title('Heatmap of hidden states for s='+str(i)+' trial='+str(j))\n",
    "        \n",
    "        axs[1].imshow(all_probs_array.T, cmap='seismic', aspect='auto', interpolation='none')\n",
    "        y_labels = ['1', '2', '3', '4']\n",
    "        axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "        axs[1].set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "        plot_title = 'solo_s'+str(i)+'_val'+str(j)\n",
    "        #plt.title(plot_title)\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('predictions')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        #axs[0].set_title()\n",
    "        # Adjust spacing between the subplot\n",
    "        plt.tight_layout()\n",
    "        # Show the plot\n",
    "        # Specify the file path\n",
    "        # Specify the directory where you want to save the plots\n",
    "        save_directory = r'C:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\hidden_states_heatmaps_train'\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Define the plot title\n",
    "        plot_title = f'solo_s{i}_val{j}'\n",
    "\n",
    "        # Specify the file path using os.path.join\n",
    "        file_path = os.path.join(save_directory, f'{plot_title}.png')\n",
    "\n",
    "        # Save the plot to the specified path\n",
    "        fig.savefig(file_path)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 50, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 0, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 50, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 0, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 50, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 0, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 50, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 0, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 50, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 0, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n",
      "Epoch 50, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(13,15):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "        h_name = 'hidden_s'+str(i)+'_trial'+str(j)\n",
    "        p_name = 'probs_s'+str(i)+'_trial'+str(j)\n",
    "        np.save(h_name, hidden_states_array)\n",
    "        np.save(p_name, all_probs_array)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hidden_states.npy', hidden_states_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_value = list(hidden_states_dict.values())[0]\n",
    "type(first_value)\n",
    "hidden_states_array = first_value[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_ylabel('hidden unit')\n",
    "axs[0].set_xlabel('time step')\n",
    "axs[0].set_title('Heatmap of hidden states')\n",
    "\n",
    "\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y', 'headTop_z',\n",
       "       'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z',\n",
       "       'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x', 'lShoulder_y',\n",
       "       'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x',\n",
       "       'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z', 'rEar_x',\n",
       "       'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y', 'rShoulder_z',\n",
       "       'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x', 'rWrist_y', 'rWrist_z',\n",
       "       'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =10\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.9, mode=\"min\", patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function GRUCellNet.forward at 0x000001948FC91670>\n"
     ]
    }
   ],
   "source": [
    "print(GRUCellNet.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 1.093435525894165, train acc: 0.41514227642276424\n",
      "s_m10_val12_gates\n",
      "0.7213235294117647\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "predictions_dict_sm = {}\n",
    "\n",
    "for j in np.arange(12,13):\n",
    "    for i in np.arange(10,11,10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        #print(X_train.columns)\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        #print(X_train.columns)\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        \n",
    "        \n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_gradient/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        \n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "        # Construct the key for the combination\n",
    "        key = f\"sm_shift{shift}_set{set_values}\"\n",
    "\n",
    "        # Store the predictions as NumPy arrays in the dictionary\n",
    "        predictions_dict_sm[key] = all_preds_array \n",
    "\n",
    "        print(model_name)\n",
    "        print(np.mean(accuracies))\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a dictionary named predictions_dict\n",
    "# and you want to extract values associated with the first key\n",
    "first_key = next(iter(predictions_dict_sm))  # Get the first key\n",
    "\n",
    "# Access the values associated with the first key\n",
    "values_for_first_key = predictions_dict_sm[first_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm_shift10_set12\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(first_key)\n",
    "print(values_for_first_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = values_for_first_key\n",
    "len(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"y_tilda.csv\"\n",
    "y_tilda = pd.read_csv(file)\n",
    "y_tilda = y_tilda[y_tilda['trial']==12].drop(columns=['trial','id'])\n",
    "#y_tilda = y_tilda.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tilda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['s_1', 's_2', 's_3', 's_4', 's_1_minus_10', 's_2_minus_10',\n",
      "       's_3_minus_10', 's_4_minus_10'],\n",
      "      dtype='object')\n",
      "(340, 4)\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the shifted data\n",
    "file = \"y_tilda.csv\"\n",
    "y_tilda = pd.read_csv(file)\n",
    "y_tilda = y_tilda[y_tilda['trial']==12].drop(columns=['trial','id'])\n",
    "shift = 10 \n",
    "col_shift = ['s_1', 's_2', 's_3', 's_4']\n",
    "# Loop through unique trial values\n",
    "    # Create shifted columns for each column in columns_to_shift\n",
    "for col in col_shift:\n",
    "    new_col_name = col + '_minus_' + str(shift)\n",
    "    y_tilda[new_col_name] = y_tilda[col].shift(shift)\n",
    "\n",
    "# Drop the last 'i' records for each trial\n",
    "y_tilda = y_tilda.dropna()\n",
    "print(y_tilda.columns)\n",
    "y_tilda = y_tilda.drop(columns=['s_1', 's_2', 's_3', 's_4'])\n",
    "new_column_names = {'s_1_minus_10': 's_1',\n",
    "                    's_2_minus_10': 's_2',\n",
    "                    's_3_minus_10': 's_3',\n",
    "                    's_4_minus_10': 's_4'}\n",
    "\n",
    "y_tilda = y_tilda.rename(columns=new_column_names)\n",
    "y_tilda = y_tilda.to_numpy()\n",
    "print(y_tilda.shape)\n",
    "y_tilda = torch.tensor(y_tilda)\n",
    "y_hat = torch.tensor(y_hat, dtype=torch.float32, requires_grad=True)\n",
    "y_tilda = torch.tensor(y_tilda, dtype=torch.float32, requires_grad=True)\n",
    "#y_hat = torch.tensor(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kacpe\\AppData\\Local\\Temp\\ipykernel_23732\\2719183788.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tilda = torch.tensor(y_tilda, dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "y_hat = torch.tensor(y_hat, dtype=torch.float32, requires_grad=True)\n",
    "y_tilda = torch.tensor(y_tilda, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x_minus_10', 'nose_y_minus_10', 'nose_z_minus_10',\n",
       "       'headTop_x_minus_10', 'headTop_y_minus_10', 'headTop_z_minus_10',\n",
       "       'neck_x_minus_10', 'neck_y_minus_10', 'neck_z_minus_10',\n",
       "       'tailBase_x_minus_10', 'tailBase_y_minus_10', 'tailBase_z_minus_10',\n",
       "       'lEar_x_minus_10', 'lEar_y_minus_10', 'lEar_z_minus_10',\n",
       "       'lShoulder_x_minus_10', 'lShoulder_y_minus_10', 'lShoulder_z_minus_10',\n",
       "       'lElbow_x_minus_10', 'lElbow_y_minus_10', 'lElbow_z_minus_10',\n",
       "       'lWrist_x_minus_10', 'lWrist_y_minus_10', 'lWrist_z_minus_10',\n",
       "       'lHip_x_minus_10', 'lHip_y_minus_10', 'lHip_z_minus_10',\n",
       "       'rEar_x_minus_10', 'rEar_y_minus_10', 'rEar_z_minus_10',\n",
       "       'rShoulder_x_minus_10', 'rShoulder_y_minus_10', 'rShoulder_z_minus_10',\n",
       "       'rElbow_x_minus_10', 'rElbow_y_minus_10', 'rElbow_z_minus_10',\n",
       "       'rWrist_x_minus_10', 'rWrist_y_minus_10', 'rWrist_z_minus_10',\n",
       "       'rHip_x_minus_10', 'rHip_y_minus_10', 'rHip_z_minus_10', 's_1_minus_10',\n",
       "       's_2_minus_10', 's_3_minus_10', 's_4_minus_10'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340, 46])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kacpe\\AppData\\Local\\Temp\\ipykernel_23732\\3382397077.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_hat = torch.tensor(y_hat, dtype=torch.float32)  # Convert to float\n",
      "C:\\Users\\kacpe\\AppData\\Local\\Temp\\ipykernel_23732\\3382397077.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_tilda = torch.tensor(y_tilda, dtype=torch.float32)  # Convert to float\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have 'y_hat' and 'y_tilda' as tensors containing binary values (0 or 1)\n",
    "y_hat = torch.tensor(y_hat, dtype=torch.float32)  # Convert to float\n",
    "y_tilda = torch.tensor(y_tilda, dtype=torch.float32)  # Convert to float\n",
    "\n",
    "y_hat = y_hat.requires_grad_(True)\n",
    "y_tilda = y_tilda.requires_grad_(True)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Calculate the loss\n",
    "binary_cross_entropy_loss = loss_fn(y_hat, y_tilda)\n",
    "input = torch.tensor(X_test.values)\n",
    "input = input.unsqueeze(1)\n",
    "input = input.float()\n",
    "# set input to require_grad\n",
    "input = input.requires_grad_(True)\n",
    "model.eval()\n",
    "output, f = model(input)\n",
    "\n",
    "# calculating loss \n",
    "output = output.requires_grad_(True)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(output, y_tilda)\n",
    "loss.backward()\n",
    "#print(input)\n",
    "\n",
    "# Access the gradients for the input data\n",
    "input_gradients = input.grad.squeeze(1)\n",
    "print(input_gradients.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rWrist_x_minus_10'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lWrist_x_gradient = input_gradients[:,21].numpy()\n",
    "rWrist_x_gradient = input_gradients[:,-10].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.76474368e-05, -1.76542217e-05, -1.75890345e-05, -1.53147794e-05,\n",
       "       -1.52373414e-05, -1.54065729e-05, -1.53684487e-05, -1.54638383e-05,\n",
       "       -1.53771980e-05, -1.53767942e-05, -1.53613546e-05, -1.53780456e-05,\n",
       "       -1.53096698e-05, -1.47437058e-05, -1.50416718e-05, -1.48826812e-05,\n",
       "       -1.49087191e-05, -1.49521038e-05, -1.48654781e-05, -1.52426392e-05,\n",
       "       -1.53065903e-05, -1.53166438e-05, -1.54639529e-05, -1.53494366e-05,\n",
       "       -1.54897043e-05, -1.54544141e-05, -1.55553607e-05, -1.59791598e-05,\n",
       "       -1.61623902e-05, -1.58945604e-05, -1.63379354e-05, -1.60139989e-05,\n",
       "       -1.59030424e-05, -1.59875017e-05, -1.52859720e-05, -1.52791599e-05,\n",
       "       -1.53782294e-05, -1.55587986e-05, -1.56673359e-05, -1.57688792e-05,\n",
       "       -1.58552466e-05,  1.39490121e-05,  1.40369311e-05,  1.40971624e-05,\n",
       "        1.41446872e-05,  1.42036261e-05,  1.42420649e-05,  1.45333961e-05,\n",
       "        1.45750673e-05,  1.45582308e-05,  1.47301071e-05,  1.47653136e-05,\n",
       "        1.48840008e-05,  1.48873341e-05,  1.49060406e-05,  1.49042726e-05,\n",
       "        1.49377847e-05,  1.49444077e-05,  1.49469915e-05,  1.49543757e-05,\n",
       "        1.49523803e-05,  1.49455782e-05,  1.49432708e-05,  1.49234456e-05,\n",
       "        1.49191274e-05,  1.49106745e-05,  1.48916461e-05,  1.55136477e-05,\n",
       "        1.56061360e-05,  1.55711641e-05,  1.55599955e-05,  1.55126982e-05,\n",
       "        1.55410999e-05,  1.55571706e-05,  1.55528669e-05,  1.55686357e-05,\n",
       "        1.55674006e-05,  1.55544203e-05,  1.55148464e-05,  1.54809768e-05,\n",
       "        1.54181744e-05,  1.53622441e-05,  1.52841585e-05,  1.52412167e-05,\n",
       "        1.51949262e-05,  1.51528620e-05,  1.51118238e-05,  1.50691885e-05,\n",
       "        1.50057949e-05,  1.49532825e-05,  1.48850595e-05,  1.48828758e-05,\n",
       "        1.48730169e-05,  1.48648242e-05,  1.48500167e-05,  1.48425415e-05,\n",
       "        1.48345644e-05,  1.48488598e-05,  1.48279287e-05,  1.47805549e-05,\n",
       "        1.47641804e-05,  1.47626715e-05,  1.47557557e-05,  1.47571700e-05,\n",
       "        1.47901865e-05,  1.47861119e-05,  1.47610672e-05,  1.47309565e-05,\n",
       "        1.46620650e-05,  1.46130642e-05,  1.45277863e-05,  1.50453343e-05,\n",
       "        1.50146807e-05,  1.48708414e-05,  1.47406754e-05,  1.47261917e-05,\n",
       "        1.46052507e-05,  1.44630803e-05,  1.44666792e-05,  1.44058213e-05,\n",
       "        1.43438774e-05,  1.42450399e-05,  1.39259955e-05,  1.39104122e-05,\n",
       "        1.40302354e-05,  1.40062857e-05,  1.37346833e-05,  1.38059922e-05,\n",
       "        1.37789029e-05,  1.37675615e-05,  1.33194180e-05,  1.31092684e-05,\n",
       "        1.30752842e-05,  1.30362332e-05,  1.30475055e-05,  1.30274293e-05,\n",
       "        1.26230971e-05,  1.56546885e-05,  1.70353342e-05,  1.65309175e-05,\n",
       "        1.63674449e-05,  1.60679647e-05,  1.57044196e-05,  1.55311791e-05,\n",
       "        1.52864013e-05,  1.49694561e-05,  1.47375094e-05,  1.44708720e-05,\n",
       "        1.41343962e-05,  1.35213140e-05,  1.33888825e-05,  1.32417990e-05,\n",
       "        1.26469477e-05,  1.24208109e-05,  1.20741943e-05,  1.11972513e-05,\n",
       "        1.09021921e-05,  1.03604980e-05,  9.84565486e-06,  9.42497809e-06,\n",
       "        8.89511739e-06,  8.41756355e-06,  7.93387062e-06,  7.40603627e-06,\n",
       "        6.78801325e-06,  6.12272561e-06,  5.85522503e-06,  5.64980473e-06,\n",
       "        5.04100490e-06,  4.79877644e-06,  4.40799704e-06,  4.23249594e-06,\n",
       "        4.12484542e-06,  3.88707849e-06,  3.70855696e-06,  3.60621289e-06,\n",
       "        3.50619871e-06,  3.43519241e-06,  3.32033142e-06,  3.25511292e-06,\n",
       "        3.01674299e-06,  2.97568226e-06,  2.77588106e-06,  2.94507117e-06,\n",
       "        2.71175168e-06,  2.49988034e-06,  2.77485879e-06,  2.53585313e-06,\n",
       "        2.63500488e-06,  2.47598064e-06,  2.46996342e-06,  2.47462003e-06,\n",
       "       -3.61193679e-06, -3.75537184e-06, -3.34812739e-06, -5.13326631e-06,\n",
       "       -5.55956967e-06, -6.03606850e-06, -6.46219496e-06, -6.56418297e-06,\n",
       "       -6.68598159e-06, -6.26612200e-06, -6.60695923e-06, -7.19135915e-06,\n",
       "       -7.03013029e-06, -7.16286013e-06, -7.01286308e-06, -7.32541685e-06,\n",
       "       -7.42949669e-06, -7.38445578e-06, -7.27450652e-06, -7.23087487e-06,\n",
       "       -7.07560821e-06, -7.08131483e-06, -6.90601019e-06, -6.64744448e-06,\n",
       "       -6.43817020e-06, -6.18150034e-06, -5.87964269e-06, -5.57257044e-06,\n",
       "       -5.15444253e-06, -4.73140290e-06, -4.44214538e-06, -4.16951070e-06,\n",
       "       -3.69490272e-06, -3.36285621e-06, -3.19021410e-06, -2.97835777e-06,\n",
       "       -2.81922394e-06, -2.63888251e-06, -2.42049055e-06, -2.61203922e-06,\n",
       "       -2.58377986e-06, -2.60215893e-06, -2.37273980e-06, -3.01536238e-06,\n",
       "       -3.06484390e-06, -3.17505464e-06, -3.35555842e-06, -3.55058501e-06,\n",
       "       -3.71591705e-06, -3.92367838e-06, -4.16392550e-06, -4.45701835e-06,\n",
       "       -4.75744537e-06, -4.95556969e-06, -5.07668892e-06, -5.12958468e-06,\n",
       "       -5.34622222e-06, -5.52539404e-06, -5.72802173e-06, -5.67243069e-06,\n",
       "       -5.57599787e-06, -5.70213706e-06, -5.91048047e-06, -5.95179199e-06,\n",
       "       -6.13866314e-06, -5.92780634e-06, -5.90483614e-06, -5.99553323e-06,\n",
       "       -5.99197347e-06, -6.07105585e-06, -6.13251723e-06, -6.11783707e-06,\n",
       "       -6.10753114e-06, -6.13177690e-06, -6.08817209e-06, -6.07804714e-06,\n",
       "       -6.06619415e-06, -6.04926345e-06, -6.03868921e-06, -5.86377519e-06,\n",
       "       -5.83989413e-06, -5.77963783e-06, -5.59690125e-06, -5.49054721e-06,\n",
       "       -5.31295109e-06, -5.03864158e-06, -4.97073961e-06, -4.84458178e-06,\n",
       "       -4.70818622e-06, -4.54708470e-06, -4.27109944e-06, -4.03394733e-06,\n",
       "       -3.85852582e-06, -3.64268953e-06, -3.50304845e-06, -3.39375401e-06,\n",
       "       -3.24939401e-06, -3.06786205e-06, -2.97993120e-06, -2.88285537e-06,\n",
       "       -2.81976736e-06, -2.88771116e-06, -2.93888206e-06, -2.88844240e-06,\n",
       "       -3.15508714e-06, -3.21040534e-06, -3.22551750e-06, -3.31136198e-06,\n",
       "       -3.32572472e-06, -3.35799723e-06, -3.36412631e-06, -3.48411413e-06,\n",
       "       -3.58854777e-06, -3.67967459e-06, -3.77860761e-06, -3.85063777e-06,\n",
       "       -3.88772150e-06, -3.94062954e-06, -3.95011102e-06, -3.95785264e-06,\n",
       "       -3.92751099e-06, -3.92085349e-06, -3.85364456e-06, -3.78931145e-06,\n",
       "       -3.76643402e-06, -3.68846122e-06, -3.45267654e-06, -3.18833099e-06,\n",
       "       -3.02388980e-06, -2.78470634e-06, -2.57400097e-06, -2.37316090e-06,\n",
       "       -1.66940754e-06, -1.71893771e-06, -1.11228292e-06, -1.46338516e-06,\n",
       "       -1.17567470e-06, -1.27076510e-06, -7.95455890e-07,  1.92454536e-07,\n",
       "        3.00189640e-07,  6.50475613e-07,  1.25516908e-06,  1.81896303e-06,\n",
       "        2.86996419e-06,  2.51287292e-06,  2.79560982e-06,  2.86397790e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rWrist_x_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8794117647058823"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rWrist_x_gradient > lWrist_x_gradient)/len(rWrist_x_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in train_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    inputs.requires_grad = True  # Set requires_grad to True\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, h, test_hidden_states = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        output = torch.round(probabilities)\n",
    "inputs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        ...,\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "        [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "        [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "        ...,\n",
       "        [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "        [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "        [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 1, 46])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input = torch.tensor(X_train.values)\n",
    "input = input.unsqueeze(1)\n",
    "input = input.float()\n",
    "input.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 88\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y231sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output\u001b[39m.\u001b[39;49msize\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "output.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.6528, 16.5354,  6.8842, -2.5370],\n",
       "         [15.7095, 16.3226,  6.9530, -2.3735],\n",
       "         [15.0120, 15.2652,  6.5979, -2.3344],\n",
       "         ...,\n",
       "         [ 4.9547,  3.1134, -4.5596, -2.0263],\n",
       "         [ 4.9473,  2.9779, -4.4284, -1.8249],\n",
       "         [ 4.9355,  3.0609, -4.3583, -1.8576]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2498, -4.0778,  4.4233,  ...,  6.9959, -0.7348, -1.3482],\n",
       "         [-0.2950, -3.5366,  4.4125,  ...,  6.9596, -0.6890, -1.5728],\n",
       "         [-0.2455, -3.2876,  4.0599,  ...,  6.5503, -0.7003, -1.6831],\n",
       "         ...,\n",
       "         [-0.8525,  0.8381, -0.3202,  ..., -0.0396,  2.6897,  2.1940],\n",
       "         [-0.8352,  0.8757, -0.3068,  ..., -0.0204,  2.7345,  1.8132],\n",
       "         [-0.7982,  0.8865, -0.3056,  ..., -0.0164,  2.7295,  1.5923]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0308,  0.0021,  0.0413,  ...,  0.2176,  0.0700, -0.3320],\n",
       "          [ 0.0290,  0.0097,  0.0412,  ...,  0.2165,  0.0713, -0.3338],\n",
       "          [ 0.0310,  0.0132,  0.0376,  ...,  0.2037,  0.0710, -0.3346],\n",
       "          ...,\n",
       "          [ 0.0065,  0.0711, -0.0079,  ..., -0.0026,  0.1707, -0.3043],\n",
       "          [ 0.0072,  0.0717, -0.0078,  ..., -0.0020,  0.1720, -0.3073],\n",
       "          [ 0.0087,  0.0718, -0.0078,  ..., -0.0018,  0.1719, -0.3090]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.5881, 17.0496,  7.1234, -2.7169],\n",
       "         [15.6907, 16.8004,  7.1833, -2.5166],\n",
       "         [14.9813, 15.7438,  6.8294, -2.4401],\n",
       "         ...,\n",
       "         [ 5.0684,  3.3052, -4.5890, -2.3956],\n",
       "         [ 5.0625,  3.1659, -4.4379, -2.1808],\n",
       "         [ 5.0584,  3.2695, -4.3667, -2.2210]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.1361, -3.9054,  4.3457,  ...,  6.9570, -0.9351, -1.0151],\n",
       "         [-0.1834, -3.3678,  4.3394,  ...,  6.9214, -0.8772, -1.2564],\n",
       "         [-0.1403, -3.1291,  3.9980,  ...,  6.4979, -0.8859, -1.4158],\n",
       "         ...,\n",
       "         [-0.8940,  0.8611, -0.3193,  ...,  0.0296,  2.5547,  2.2299],\n",
       "         [-0.8732,  0.9012, -0.3054,  ...,  0.0481,  2.5995,  1.8107],\n",
       "         [-0.8328,  0.9142, -0.3030,  ...,  0.0511,  2.5933,  1.5676]],\n",
       "        grad_fn=<NativeBatchNormBackward0>),\n",
       " [tensor([[ 0.0378,  0.0049,  0.0404,  ...,  0.1942,  0.0496, -0.3348],\n",
       "          [ 0.0359,  0.0124,  0.0403,  ...,  0.1932,  0.0511, -0.3366],\n",
       "          [ 0.0376,  0.0157,  0.0368,  ...,  0.1812,  0.0509, -0.3379],\n",
       "          ...,\n",
       "          [ 0.0076,  0.0714, -0.0078,  ..., -0.0019,  0.1424, -0.3098],\n",
       "          [ 0.0084,  0.0720, -0.0077,  ..., -0.0014,  0.1436, -0.3130],\n",
       "          [ 0.0100,  0.0722, -0.0076,  ..., -0.0013,  0.1434, -0.3149]],\n",
       "         grad_fn=<AddBackward0>)])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_tilda instead of target \n",
    "#output = y_hat \n",
    "# creating input\n",
    "input = torch.tensor(X_test.values)\n",
    "input = input.unsqueeze(1)\n",
    "input = input.float()\n",
    "# set input to require_grad\n",
    "input = input.requires_grad_(True)\n",
    "model.eval()\n",
    "output, f, f = model(input)\n",
    "\n",
    "# calculating loss \n",
    "output = output.requires_grad_(True)\n",
    "loss = loss_fn(output, target)\n",
    "loss.backward()\n",
    "\n",
    "# Access the gradients for the input data\n",
    "input_gradients = input.grad.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.grad.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "gru_cells.0.weight_ih\n",
      "tensor([[ 0.1454, -0.0767,  0.1165,  ...,  0.0882,  0.0758,  0.0415],\n",
      "        [ 0.0745, -0.1682,  0.0754,  ...,  0.0346, -0.0484, -0.1178],\n",
      "        [-0.0083, -0.1615,  0.1327,  ...,  0.0147, -0.1474, -0.1203],\n",
      "        ...,\n",
      "        [ 0.0235, -0.0237, -0.0839,  ...,  0.0383, -0.1634,  0.1645],\n",
      "        [-0.0403,  0.0512,  0.0946,  ..., -0.0186,  0.1682,  0.1391],\n",
      "        [ 0.1168, -0.0078, -0.1271,  ..., -0.0558,  0.1122,  0.0342]])\n",
      "gru_cells.0.weight_hh\n",
      "tensor([[ 0.0654,  0.0306,  0.1648,  ..., -0.0643,  0.1707,  0.0210],\n",
      "        [-0.0169,  0.1101,  0.0372,  ...,  0.0406, -0.0684,  0.0486],\n",
      "        [ 0.1226, -0.1662, -0.0386,  ...,  0.0828,  0.0141, -0.0264],\n",
      "        ...,\n",
      "        [ 0.1211, -0.1242,  0.1029,  ...,  0.0835,  0.0161,  0.0562],\n",
      "        [-0.0618, -0.0938, -0.0629,  ...,  0.0225,  0.1529, -0.1611],\n",
      "        [-0.0711,  0.0437, -0.0786,  ...,  0.0553,  0.0523,  0.1267]])\n",
      "gru_cells.0.bias_ih\n",
      "tensor([-0.0448, -0.1460, -0.1346,  0.0724, -0.1092, -0.0197,  0.0069,  0.1191,\n",
      "        -0.1403,  0.1496,  0.1430,  0.1667, -0.1598, -0.1411, -0.1425, -0.1688,\n",
      "         0.0917, -0.0277,  0.1599,  0.0327,  0.0347, -0.1244,  0.0508, -0.0665,\n",
      "        -0.0295, -0.1343, -0.1673,  0.0276,  0.1174, -0.0098,  0.1094, -0.1628,\n",
      "         0.1079, -0.0417, -0.0304, -0.1414, -0.0490,  0.0350, -0.0239, -0.0371,\n",
      "        -0.1674, -0.0318, -0.0888,  0.1208,  0.0407,  0.1607,  0.0913, -0.1196,\n",
      "        -0.1434,  0.0520,  0.0007,  0.0231,  0.0250, -0.1312, -0.0793, -0.1102,\n",
      "        -0.1343,  0.0433, -0.1600, -0.0174,  0.1429,  0.0239, -0.1694,  0.0577,\n",
      "         0.1607,  0.1639,  0.0046, -0.0801,  0.0591, -0.0131,  0.0726, -0.0672,\n",
      "         0.0513, -0.0894, -0.1173,  0.1703, -0.1659, -0.1647, -0.0099, -0.0328,\n",
      "        -0.0361, -0.1085, -0.0121, -0.0948,  0.0095, -0.0053, -0.1561,  0.0304,\n",
      "         0.1408, -0.0091,  0.1302,  0.0871, -0.0493, -0.0203,  0.1120, -0.0558,\n",
      "        -0.0629,  0.1430,  0.0259,  0.1392, -0.0475,  0.0217])\n",
      "gru_cells.0.bias_hh\n",
      "tensor([-0.0332,  0.0442, -0.1472, -0.0456, -0.1438,  0.1043,  0.0451,  0.1555,\n",
      "         0.1268, -0.1691,  0.0426, -0.1560, -0.1143, -0.0966, -0.0023,  0.1359,\n",
      "        -0.0607, -0.1227, -0.0863, -0.0531, -0.0958,  0.0333, -0.0704,  0.0937,\n",
      "        -0.0370,  0.0593, -0.1396,  0.1397,  0.0136,  0.0929,  0.0389,  0.1503,\n",
      "         0.0905, -0.1106,  0.1559, -0.0772,  0.0270,  0.1678, -0.1394, -0.1400,\n",
      "        -0.0626,  0.0926, -0.1114,  0.1665, -0.1708,  0.0881,  0.1589,  0.0757,\n",
      "         0.1052,  0.0133, -0.0148,  0.1212, -0.0758,  0.0402,  0.1526,  0.0132,\n",
      "        -0.1624, -0.1158,  0.1666, -0.1401,  0.1222, -0.0086, -0.0232, -0.0315,\n",
      "        -0.1643,  0.1242, -0.0542,  0.1045, -0.0093, -0.0621,  0.1381,  0.1522,\n",
      "        -0.1240, -0.1321,  0.1714, -0.1469, -0.0439, -0.0988,  0.1347, -0.1438,\n",
      "         0.0290,  0.0139,  0.0760, -0.1633,  0.0066,  0.0641, -0.1302, -0.0282,\n",
      "        -0.1271, -0.1249, -0.0584,  0.1551,  0.0984, -0.1548,  0.1204,  0.1488,\n",
      "         0.1022,  0.0271, -0.1225,  0.0938, -0.0121,  0.0651])\n"
     ]
    }
   ],
   "source": [
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "print(len(gru_cell_weights))\n",
    "# Print or access the weights as needed\n",
    "for key, value in gru_cell_weights.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "weight_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "weight_hh = gru_cell_weights['gru_cells.0.weight_hh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_ih[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ih_np = weight_ih.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['nose_x_minus_80', 'nose_y_minus_80', 'nose_z_minus_80',\n",
    "       'headTop_x_minus_80', 'headTop_y_minus_80', 'headTop_z_minus_80',\n",
    "       'neck_x_minus_80', 'neck_y_minus_80', 'neck_z_minus_80',\n",
    "       'tailBase_x_minus_80', 'tailBase_y_minus_80', 'tailBase_z_minus_80',\n",
    "       'lEar_x_minus_80', 'lEar_y_minus_80', 'lEar_z_minus_80',\n",
    "       'lShoulder_x_minus_80', 'lShoulder_y_minus_80', 'lShoulder_z_minus_80',\n",
    "       'lElbow_x_minus_80', 'lElbow_y_minus_80', 'lElbow_z_minus_80',\n",
    "       'lWrist_x_minus_80', 'lWrist_y_minus_80', 'lWrist_z_minus_80',\n",
    "       'lHip_x_minus_80', 'lHip_y_minus_80', 'lHip_z_minus_80',\n",
    "       'rEar_x_minus_80', 'rEar_y_minus_80', 'rEar_z_minus_80',\n",
    "       'rShoulder_x_minus_80', 'rShoulder_y_minus_80', 'rShoulder_z_minus_80',\n",
    "       'rElbow_x_minus_80', 'rElbow_y_minus_80', 'rElbow_z_minus_80',\n",
    "       'rWrist_x_minus_80', 'rWrist_y_minus_80', 'rWrist_z_minus_80',\n",
    "       'rHip_x_minus_80', 'rHip_y_minus_80', 'rHip_z_minus_80', 's_1_minus_80',\n",
    "       's_2_minus_80', 's_3_minus_80', 's_4_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 46)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_ih_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.DataFrame(weight_ih_np, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.118293\n",
       "1      0.057691\n",
       "2     -0.099260\n",
       "3      0.020036\n",
       "4      0.046773\n",
       "         ...   \n",
       "97    -0.108127\n",
       "98    -0.147265\n",
       "99     0.120991\n",
       "100   -0.109019\n",
       "101    0.127330\n",
       "Name: rWrist_x_minus_80, Length: 102, dtype: float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['rWrist_x_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.073618\n",
       "1      0.129745\n",
       "2     -0.052349\n",
       "3      0.136418\n",
       "4      0.078713\n",
       "         ...   \n",
       "97    -0.137238\n",
       "98    -0.041288\n",
       "99    -0.127240\n",
       "100   -0.085322\n",
       "101   -0.029799\n",
       "Name: lWrist_x_minus_80, Length: 102, dtype: float32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['lWrist_x_minus_80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 1\n",
    "hidden_dim = 1\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwrist_df = scaled_df[['rWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "lwrist_df = scaled_df[['lWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "labels = ['s_1','s_2','s_3','s_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       ...,\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8470, dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rWrist_loss.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 's_m80_val13_gates.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5328, 4])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtered_df = scaled_df[scaled_df['trial'] != 14]\n",
    "#target = filtered_df[['s_1','s_2','s_3','s_4']] \n",
    "target = torch.from_numpy(y_test.to_numpy())\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 102\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y223sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mgrad\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'grad'"
     ]
    }
   ],
   "source": [
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = loss_fn(output, target)\n",
    "model_loss.requires_grad_(True)\n",
    "model_loss.backward()\n",
    "\n",
    "\n",
    "# Access gradients for specific parameters\n",
    "gru_weight_ih_gradients = model.gru.weight_ih.grad  # Gradients for input-hidden weights of GRU\n",
    "gru_weight_hh_gradients = model.gru.weight_hh.grad  # Gradients for hidden-hidden weights of GRU\n",
    "linear_weight_gradients = model.fc.weight.grad  # Gradients for the weight of the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6458, dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rWrist_x_minus_80</th>\n",
       "      <th>lWrist_x_minus_80</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rWrist_x_minus_80  lWrist_x_minus_80\n",
       "0                  0.0                0.0\n",
       "1                  0.0                0.0\n",
       "2                  0.0                0.0\n",
       "3                  0.0                0.0\n",
       "4                  0.0                0.0\n",
       "..                 ...                ...\n",
       "97                 0.0                0.0\n",
       "98                 0.0                0.0\n",
       "99                 0.0                0.0\n",
       "100                0.0                0.0\n",
       "101                0.0                0.0\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_array = gru_weight_ih_gradients.numpy()\n",
    "gradients = pd.DataFrame(gradients_array, columns=features)\n",
    "gradients[['rWrist_x_minus_80','lWrist_x_minus_80']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.392157618e-06"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean(np.round(gradients['rWrist_x_minus_80'] - gradients['lWrist_x_minus_80'],5)),15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# Perform the backward pass on the combined loss\n",
    "combined_loss = rWrist_loss + lWrist_loss\n",
    "combined_loss.backward()\n",
    "gradients = combined_loss.grad\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lWrist_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GRUCellNet:\n\tsize mismatch for gru.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([4, 1]) from checkpoint, the shape in current model is torch.Size([4, 34]).\n\tsize mismatch for gru_cells.0.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru_cells.0.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru_cells.0.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru_cells.0.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for batch_norm.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 110\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(load_folder, model_filename))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set the model in evaluation mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y210sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GRUCellNet:\n\tsize mismatch for gru.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([4, 1]) from checkpoint, the shape in current model is torch.Size([4, 34]).\n\tsize mismatch for gru_cells.0.weight_ih: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 46]).\n\tsize mismatch for gru_cells.0.weight_hh: copying a param with shape torch.Size([3, 1]) from checkpoint, the shape in current model is torch.Size([102, 34]).\n\tsize mismatch for gru_cells.0.bias_ih: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for gru_cells.0.bias_hh: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([102]).\n\tsize mismatch for batch_norm.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_mean: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34]).\n\tsize mismatch for batch_norm.running_var: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([34])."
     ]
    }
   ],
   "source": [
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 'lWrist_model.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "lWrist_weights_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "lWrist_weights_hh = gru_cell_weights['gru_cells.0.weight_hh']\n",
    "\n",
    "\n",
    "        # Define the folder and model filename\n",
    "load_folder = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models'\n",
    "model_filename = 'rWrist_model.pth'\n",
    "\n",
    "# Load the model\n",
    "checkpoint = torch.load(os.path.join(load_folder, model_filename))\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "\n",
    "# Access the model's state dictionary\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# Now you can access the weights of the nn.GRUCell layers\n",
    "gru_cell_weights = {\n",
    "    key: value\n",
    "    for key, value in model_state_dict.items()\n",
    "    if key.startswith('gru_cells.')\n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Extract and assign the weights to separate variables\n",
    "rWrist_weights_ih = gru_cell_weights['gru_cells.0.weight_ih']\n",
    "rWrist_weights_hh = gru_cell_weights['gru_cells.0.weight_hh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6961],\n",
      "        [ 0.6693],\n",
      "        [-0.4421]])\n",
      "tensor([[ 0.6961],\n",
      "        [ 0.6693],\n",
      "        [-0.4421]])\n"
     ]
    }
   ],
   "source": [
    "print(lWrist_weights_ih)\n",
    "print(rWrist_weights_ih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['rWrist_x'], dtype='object')\n",
      "Index(['rWrist_x'], dtype='object')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 1 expected 46",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 124\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mX_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#print(X_train.shape[0])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states \u001b[39m=\u001b[39m run_training(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     train_dataloader, val_dataloader\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, loss_fn\u001b[39m=\u001b[39;49mloss_fn, num_epochs\u001b[39m=\u001b[39;49mn_epochs, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m folder_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models\u001b[39m\u001b[39m'\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 124\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#train_hidden_states, val_hidden_states = [], []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epoch_train_loss, epoch_train_acc, train_preds, train_probs \u001b[39m=\u001b[39m train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(epoch_train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accs\u001b[39m.\u001b[39mappend(epoch_train_acc)\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 124\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out, last_hidden_states \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Append the predicted probabilities to the list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 124\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m new_hidden_states \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, gru_cell \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru_cells):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     h[layer_idx] \u001b[39m=\u001b[39m gru_cell(input_t, h[layer_idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     new_hidden_states\u001b[39m.\u001b[39mappend(h[layer_idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y233sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     input_t \u001b[39m=\u001b[39m h[layer_idx]\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1279\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[1;32m-> 1279\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru_cell(\n\u001b[0;32m   1280\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[0;32m   1281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[0;32m   1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[0;32m   1283\u001b[0m )\n\u001b[0;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1286\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 1 expected 46"
     ]
    }
   ],
   "source": [
    "rwrist_df = scaled_df[['rWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "lwrist_df = scaled_df[['lWrist_x','trial','id','s_1','s_2','s_3','s_4']]\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "\n",
    "#rwrist_df = lwrist_df.copy()\n",
    "set_values = 13 \n",
    "train_set = rwrist_df[rwrist_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "test_set = rwrist_df[rwrist_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = rwrist_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "print(X_train.columns)\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "print(X_train.columns)\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "#print(X_train.shape[0])\n",
    "\n",
    "\n",
    "train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "    train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/gradient_models' \n",
    "model_filename = 'rWrist_model.pth'  \n",
    "\n",
    "# Combine the folder path and model filename\n",
    "full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "torch.save(state_dict, full_model)\n",
    "\n",
    "# Iterate through the test data batches\n",
    "for inputs, _ in train_dataloader:\n",
    "    inputs = inputs.float()\n",
    "    inputs.requires_grad = True  # Set requires_grad to True\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions, h, test_hidden_states = model(inputs)\n",
    "        probabilities = torch.sigmoid(predictions)\n",
    "        rWrist_output = torch.round(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]  # Update input_t with the new hidden state for the next layer\n",
    "            hidden_states.append(new_hidden_states)\n",
    "        \n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        # Apply BatchNorm to the last hidden state\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "        \n",
    "        return out, last_hidden_states "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step(loss)\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['H_headFront_x', 'H_headFront_y', 'H_headFront_z', 'H_neck_x',\n",
       "       'H_neck_y', 'H_neck_z', 'H_lowerBack_x', 'H_lowerBack_y',\n",
       "       'H_lowerBack_z', 'H_leftWrist_x', 'H_leftWrist_y', 'H_leftWrist_z',\n",
       "       'H_leftShoulder_x', 'H_leftShoulder_y', 'H_leftShoulder_z',\n",
       "       'H_leftElbow_x', 'H_leftElbow_y', 'H_leftElbow_z', 'nose_x', 'nose_y',\n",
       "       'nose_z', 'headTop_x', 'headTop_y', 'headTop_z', 'neck_x', 'neck_y',\n",
       "       'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z', 'lEar_x', 'lEar_y',\n",
       "       'lEar_z', 'lShoulder_x', 'lShoulder_y', 'lShoulder_z', 'lElbow_x',\n",
       "       'lElbow_y', 'lElbow_z', 'lWrist_x', 'lWrist_y', 'lWrist_z', 'lHip_x',\n",
       "       'lHip_y', 'lHip_z', 'rEar_x', 'rEar_y', 'rEar_z', 'rShoulder_x',\n",
       "       'rShoulder_y', 'rShoulder_z', 'rElbow_x', 'rElbow_y', 'rElbow_z',\n",
       "       'rWrist_x', 'rWrist_y', 'rWrist_z', 'rHip_x', 'rHip_y', 'rHip_z', 's_1',\n",
       "       's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_headFront_x</th>\n",
       "      <th>H_headFront_y</th>\n",
       "      <th>H_headFront_z</th>\n",
       "      <th>H_neck_x</th>\n",
       "      <th>H_neck_y</th>\n",
       "      <th>H_neck_z</th>\n",
       "      <th>H_lowerBack_x</th>\n",
       "      <th>H_lowerBack_y</th>\n",
       "      <th>H_lowerBack_z</th>\n",
       "      <th>H_leftWrist_x</th>\n",
       "      <th>...</th>\n",
       "      <th>rWrist_z</th>\n",
       "      <th>rHip_x</th>\n",
       "      <th>rHip_y</th>\n",
       "      <th>rHip_z</th>\n",
       "      <th>s_1</th>\n",
       "      <th>s_2</th>\n",
       "      <th>s_3</th>\n",
       "      <th>s_4</th>\n",
       "      <th>id</th>\n",
       "      <th>trial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.368540</td>\n",
       "      <td>0.913164</td>\n",
       "      <td>0.486069</td>\n",
       "      <td>0.388278</td>\n",
       "      <td>0.729763</td>\n",
       "      <td>0.267526</td>\n",
       "      <td>0.408348</td>\n",
       "      <td>0.953196</td>\n",
       "      <td>0.737211</td>\n",
       "      <td>0.343755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019150</td>\n",
       "      <td>0.874006</td>\n",
       "      <td>0.303179</td>\n",
       "      <td>0.514884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.359652</td>\n",
       "      <td>0.909720</td>\n",
       "      <td>0.492463</td>\n",
       "      <td>0.395405</td>\n",
       "      <td>0.730680</td>\n",
       "      <td>0.268290</td>\n",
       "      <td>0.392213</td>\n",
       "      <td>0.951550</td>\n",
       "      <td>0.738959</td>\n",
       "      <td>0.341679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033237</td>\n",
       "      <td>0.876957</td>\n",
       "      <td>0.298164</td>\n",
       "      <td>0.533691</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.347923</td>\n",
       "      <td>0.901555</td>\n",
       "      <td>0.488367</td>\n",
       "      <td>0.406773</td>\n",
       "      <td>0.731274</td>\n",
       "      <td>0.272798</td>\n",
       "      <td>0.394118</td>\n",
       "      <td>0.960908</td>\n",
       "      <td>0.727405</td>\n",
       "      <td>0.337201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020840</td>\n",
       "      <td>0.877541</td>\n",
       "      <td>0.293674</td>\n",
       "      <td>0.546340</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.339938</td>\n",
       "      <td>0.898339</td>\n",
       "      <td>0.490705</td>\n",
       "      <td>0.408593</td>\n",
       "      <td>0.730717</td>\n",
       "      <td>0.274102</td>\n",
       "      <td>0.393517</td>\n",
       "      <td>0.961208</td>\n",
       "      <td>0.718410</td>\n",
       "      <td>0.332870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>0.878950</td>\n",
       "      <td>0.290393</td>\n",
       "      <td>0.561498</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24477</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.350412</td>\n",
       "      <td>0.897700</td>\n",
       "      <td>0.457687</td>\n",
       "      <td>0.411017</td>\n",
       "      <td>0.728061</td>\n",
       "      <td>0.265325</td>\n",
       "      <td>0.394291</td>\n",
       "      <td>0.959973</td>\n",
       "      <td>0.727458</td>\n",
       "      <td>0.339210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063765</td>\n",
       "      <td>0.878661</td>\n",
       "      <td>0.287158</td>\n",
       "      <td>0.566016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24478</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6881</th>\n",
       "      <td>0.424921</td>\n",
       "      <td>0.617265</td>\n",
       "      <td>0.301569</td>\n",
       "      <td>0.553744</td>\n",
       "      <td>0.411033</td>\n",
       "      <td>0.106477</td>\n",
       "      <td>0.184756</td>\n",
       "      <td>0.701764</td>\n",
       "      <td>0.503297</td>\n",
       "      <td>0.413698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200201</td>\n",
       "      <td>0.868922</td>\n",
       "      <td>0.289443</td>\n",
       "      <td>0.247519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94261</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>0.419726</td>\n",
       "      <td>0.618263</td>\n",
       "      <td>0.351344</td>\n",
       "      <td>0.544942</td>\n",
       "      <td>0.423803</td>\n",
       "      <td>0.108464</td>\n",
       "      <td>0.176467</td>\n",
       "      <td>0.707939</td>\n",
       "      <td>0.530420</td>\n",
       "      <td>0.413039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200201</td>\n",
       "      <td>0.769215</td>\n",
       "      <td>0.257287</td>\n",
       "      <td>0.254733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94262</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>0.405272</td>\n",
       "      <td>0.614114</td>\n",
       "      <td>0.248829</td>\n",
       "      <td>0.545891</td>\n",
       "      <td>0.439484</td>\n",
       "      <td>0.087123</td>\n",
       "      <td>0.177473</td>\n",
       "      <td>0.709672</td>\n",
       "      <td>0.446293</td>\n",
       "      <td>0.404919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035104</td>\n",
       "      <td>0.774344</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>0.431135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94263</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6884</th>\n",
       "      <td>0.402649</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.244677</td>\n",
       "      <td>0.536706</td>\n",
       "      <td>0.437462</td>\n",
       "      <td>0.086819</td>\n",
       "      <td>0.142198</td>\n",
       "      <td>0.706432</td>\n",
       "      <td>0.456654</td>\n",
       "      <td>0.400817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057599</td>\n",
       "      <td>0.776229</td>\n",
       "      <td>0.279005</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94264</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>0.396262</td>\n",
       "      <td>0.632908</td>\n",
       "      <td>0.240631</td>\n",
       "      <td>0.523997</td>\n",
       "      <td>0.436066</td>\n",
       "      <td>0.083707</td>\n",
       "      <td>0.121503</td>\n",
       "      <td>0.699916</td>\n",
       "      <td>0.462010</td>\n",
       "      <td>0.392979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147189</td>\n",
       "      <td>0.765717</td>\n",
       "      <td>0.283865</td>\n",
       "      <td>0.025204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94265</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6873 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      H_headFront_x  H_headFront_y  H_headFront_z  H_neck_x  H_neck_y  \\\n",
       "0          0.368540       0.913164       0.486069  0.388278  0.729763   \n",
       "1          0.359652       0.909720       0.492463  0.395405  0.730680   \n",
       "2          0.347923       0.901555       0.488367  0.406773  0.731274   \n",
       "3          0.339938       0.898339       0.490705  0.408593  0.730717   \n",
       "4          0.350412       0.897700       0.457687  0.411017  0.728061   \n",
       "...             ...            ...            ...       ...       ...   \n",
       "6881       0.424921       0.617265       0.301569  0.553744  0.411033   \n",
       "6882       0.419726       0.618263       0.351344  0.544942  0.423803   \n",
       "6883       0.405272       0.614114       0.248829  0.545891  0.439484   \n",
       "6884       0.402649       0.627825       0.244677  0.536706  0.437462   \n",
       "6885       0.396262       0.632908       0.240631  0.523997  0.436066   \n",
       "\n",
       "      H_neck_z  H_lowerBack_x  H_lowerBack_y  H_lowerBack_z  H_leftWrist_x  \\\n",
       "0     0.267526       0.408348       0.953196       0.737211       0.343755   \n",
       "1     0.268290       0.392213       0.951550       0.738959       0.341679   \n",
       "2     0.272798       0.394118       0.960908       0.727405       0.337201   \n",
       "3     0.274102       0.393517       0.961208       0.718410       0.332870   \n",
       "4     0.265325       0.394291       0.959973       0.727458       0.339210   \n",
       "...        ...            ...            ...            ...            ...   \n",
       "6881  0.106477       0.184756       0.701764       0.503297       0.413698   \n",
       "6882  0.108464       0.176467       0.707939       0.530420       0.413039   \n",
       "6883  0.087123       0.177473       0.709672       0.446293       0.404919   \n",
       "6884  0.086819       0.142198       0.706432       0.456654       0.400817   \n",
       "6885  0.083707       0.121503       0.699916       0.462010       0.392979   \n",
       "\n",
       "      ...  rWrist_z    rHip_x    rHip_y    rHip_z  s_1  s_2  s_3  s_4     id  \\\n",
       "0     ...  0.019150  0.874006  0.303179  0.514884  1.0  1.0  1.0  1.0  24474   \n",
       "1     ...  0.033237  0.876957  0.298164  0.533691  1.0  1.0  1.0  1.0  24475   \n",
       "2     ...  0.020840  0.877541  0.293674  0.546340  1.0  1.0  1.0  1.0  24476   \n",
       "3     ...  0.032460  0.878950  0.290393  0.561498  1.0  1.0  1.0  1.0  24477   \n",
       "4     ...  0.063765  0.878661  0.287158  0.566016  1.0  1.0  1.0  1.0  24478   \n",
       "...   ...       ...       ...       ...       ...  ...  ...  ...  ...    ...   \n",
       "6881  ...  0.200201  0.868922  0.289443  0.247519  1.0  1.0  0.0  0.0  94261   \n",
       "6882  ...  0.200201  0.769215  0.257287  0.254733  1.0  1.0  0.0  0.0  94262   \n",
       "6883  ...  0.035104  0.774344  0.268751  0.431135  1.0  1.0  0.0  0.0  94263   \n",
       "6884  ...  0.057599  0.776229  0.279005  0.051032  1.0  1.0  0.0  0.0  94264   \n",
       "6885  ...  0.147189  0.765717  0.283865  0.025204  1.0  1.0  0.0  0.0  94265   \n",
       "\n",
       "      trial  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "6881     15  \n",
       "6882     15  \n",
       "6883     15  \n",
       "6884     15  \n",
       "6885     15  \n",
       "\n",
       "[6873 rows x 66 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['H_headFront_x', 'H_headFront_y', 'H_headFront_z', 'H_neck_x',\n",
       "       'H_neck_y', 'H_neck_z', 'H_lowerBack_x', 'H_lowerBack_y',\n",
       "       'H_lowerBack_z', 'H_leftWrist_x', 'H_leftWrist_y', 'H_leftWrist_z',\n",
       "       'H_leftShoulder_x', 'H_leftShoulder_y', 'H_leftShoulder_z',\n",
       "       'H_leftElbow_x', 'H_leftElbow_y', 'H_leftElbow_z', 'nose_x', 'nose_y',\n",
       "       'nose_z', 'headTop_x', 'headTop_y', 'headTop_z', 'neck_x', 'neck_y',\n",
       "       'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z', 'lEar_x', 'lEar_y',\n",
       "       'lEar_z', 'lShoulder_x', 'lShoulder_y', 'lShoulder_z', 'lElbow_x',\n",
       "       'lElbow_y', 'lElbow_z', 'lWrist_x', 'lWrist_y', 'lWrist_z', 'lHip_x',\n",
       "       'lHip_y', 'lHip_z', 'rEar_x', 'rEar_y', 'rEar_z', 'rShoulder_x',\n",
       "       'rShoulder_y', 'rShoulder_z', 'rElbow_x', 'rElbow_y', 'rElbow_z',\n",
       "       'rWrist_x', 'rWrist_y', 'rWrist_z', 'rHip_x', 'rHip_y', 'rHip_z', 's_1',\n",
       "       's_2', 's_3', 's_4', 'id', 'trial'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 46 expected 64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 133\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mX_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final \u001b[39m=\u001b[39m run_training(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     train_dataloader, val_dataloader\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, loss_fn\u001b[39m=\u001b[39;49mloss_fn, num_epochs\u001b[39m=\u001b[39;49mn_epochs, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Specify the folder path and the model filename\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 133\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#train_hidden_states, val_hidden_states = [], []\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     epoch_train_loss, epoch_train_acc, train_preds, train_probs \u001b[39m=\u001b[39m train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(epoch_train_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accs\u001b[39m.\u001b[39mappend(epoch_train_acc)\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 133\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m out, last_hidden_states \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_prob \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Append the predicted probabilities to the list\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 133\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m new_hidden_states \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer_idx, gru_cell \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgru_cells):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     h[layer_idx] \u001b[39m=\u001b[39m gru_cell(input_t, h[layer_idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     new_hidden_states\u001b[39m.\u001b[39mappend(h[layer_idx])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y246sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     input_t \u001b[39m=\u001b[39m h[layer_idx]  \u001b[39m# Update input_t with the new hidden state for the next layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1279\u001b[0m, in \u001b[0;36mGRUCell.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     hx \u001b[39m=\u001b[39m hx\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched \u001b[39melse\u001b[39;00m hx\n\u001b[1;32m-> 1279\u001b[0m ret \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru_cell(\n\u001b[0;32m   1280\u001b[0m     \u001b[39minput\u001b[39;49m, hx,\n\u001b[0;32m   1281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_hh,\n\u001b[0;32m   1282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_ih, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_hh,\n\u001b[0;32m   1283\u001b[0m )\n\u001b[0;32m   1285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[0;32m   1286\u001b[0m     ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input has inconsistent input_size: got 46 expected 64"
     ]
    }
   ],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(1,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['H_headFront_x', 'H_headFront_y', 'H_headFront_z', 'H_neck_x',\n",
       "       'H_neck_y', 'H_neck_z', 'H_lowerBack_x', 'H_lowerBack_y',\n",
       "       'H_lowerBack_z', 'H_leftWrist_x', 'H_leftWrist_y', 'H_leftWrist_z',\n",
       "       'H_leftShoulder_x', 'H_leftShoulder_y', 'H_leftShoulder_z',\n",
       "       'H_leftElbow_x', 'H_leftElbow_y', 'H_leftElbow_z', 's_1', 's_2', 's_3',\n",
       "       's_4', 'nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y',\n",
       "       'headTop_z', 'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y',\n",
       "       'tailBase_z', 'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x',\n",
       "       'lShoulder_y', 'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z',\n",
       "       'lWrist_x', 'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z',\n",
       "       'rEar_x', 'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y',\n",
       "       'rShoulder_z', 'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x',\n",
       "       'rWrist_y', 'rWrist_z', 'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2',\n",
       "       's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns='H_id')\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 2, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3080\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3080\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3081\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:96\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:126\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:133\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._maybe_get_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\index.pyx:150\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine._unpack_bool_indexer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 's_1_minus_10'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3826\u001b[0m, in \u001b[0;36mNDFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3825\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3826\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3827\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   3828\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3082\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3082\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[39mif\u001b[39;00m tolerance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 's_1_minus_10'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\GRUCell.ipynb Cell 135\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y256sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m columns_to_shift:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y256sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     new_col_name \u001b[39m=\u001b[39m col \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_minus_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(shift)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y256sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     trial_df[new_col_name] \u001b[39m=\u001b[39m trial_df[col]\u001b[39m.\u001b[39mshift(shift)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y256sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Drop the last 'i' records for each trial\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kacpe/Desktop/study/research%20lab/lab_rotation_git/GRUCell.ipynb#Y256sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m trial_df \u001b[39m=\u001b[39m trial_df\u001b[39m.\u001b[39mdropna()\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3163\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3160\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array(key, value)\n\u001b[0;32m   3161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3162\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[1;32m-> 3163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3243\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_valid_index(value)\n\u001b[0;32m   3242\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_column(key, value)\n\u001b[1;32m-> 3243\u001b[0m NDFrame\u001b[39m.\u001b[39;49m_set_item(\u001b[39mself\u001b[39;49m, key, value)\n\u001b[0;32m   3245\u001b[0m \u001b[39m# check if we are modifying a copy\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[39m# try to set first as we want an invalid\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m \u001b[39m# value exception to occur first\u001b[39;00m\n\u001b[0;32m   3248\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3829\u001b[0m, in \u001b[0;36mNDFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3826\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39mget_loc(key)\n\u001b[0;32m   3827\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m   3828\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[1;32m-> 3829\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minsert(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis), key, value)\n\u001b[0;32m   3830\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   3832\u001b[0m NDFrame\u001b[39m.\u001b[39m_iset_item(\u001b[39mself\u001b[39m, loc, value)\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1203\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   1200\u001b[0m     \u001b[39m# TODO(EA2D): special case not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m     value \u001b[39m=\u001b[39m safe_reshape(value, (\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m value\u001b[39m.\u001b[39mshape)\n\u001b[1;32m-> 1203\u001b[0m block \u001b[39m=\u001b[39m make_block(values\u001b[39m=\u001b[39;49mvalue, ndim\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mndim, placement\u001b[39m=\u001b[39;49m\u001b[39mslice\u001b[39;49m(loc, loc \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[0;32m   1205\u001b[0m \u001b[39mfor\u001b[39;00m blkno, count \u001b[39min\u001b[39;00m _fast_count_smallints(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblknos[loc:]):\n\u001b[0;32m   1206\u001b[0m     blk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks[blkno]\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2742\u001b[0m, in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   2737\u001b[0m \u001b[39melif\u001b[39;00m klass \u001b[39mis\u001b[39;00m DatetimeTZBlock \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_datetime64tz_dtype(values\u001b[39m.\u001b[39mdtype):\n\u001b[0;32m   2738\u001b[0m     \u001b[39m# TODO: This is no longer hit internally; does it need to be retained\u001b[39;00m\n\u001b[0;32m   2739\u001b[0m     \u001b[39m#  for e.g. pyarrow?\u001b[39;00m\n\u001b[0;32m   2740\u001b[0m     values \u001b[39m=\u001b[39m DatetimeArray\u001b[39m.\u001b[39m_simple_new(values, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m-> 2742\u001b[0m \u001b[39mreturn\u001b[39;00m klass(values, ndim\u001b[39m=\u001b[39;49mndim, placement\u001b[39m=\u001b[39;49mplacement)\n",
      "File \u001b[1;32mc:\\Users\\kacpe\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:142\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_values(values)\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_ndim \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    143\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrong number of items passed \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplacement implies \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmgr_locs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 2, placement implies 1"
     ]
    }
   ],
   "source": [
    "# finished smh, left set 14,15 to train \n",
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['H_headFront_x', 'H_headFront_y', 'H_headFront_z', 'H_neck_x',\n",
       "       'H_neck_y', 'H_neck_z', 'H_lowerBack_x', 'H_lowerBack_y',\n",
       "       'H_lowerBack_z', 'H_leftWrist_x', 'H_leftWrist_y', 'H_leftWrist_z',\n",
       "       'H_leftShoulder_x', 'H_leftShoulder_y', 'H_leftShoulder_z',\n",
       "       'H_leftElbow_x', 'H_leftElbow_y', 'H_leftElbow_z', 'nose_x', 'nose_y',\n",
       "       'nose_z', 'headTop_x', 'headTop_y', 'headTop_z', 'neck_x', 'neck_y',\n",
       "       'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z', 'lEar_x', 'lEar_y',\n",
       "       'lEar_z', 'lShoulder_x', 'lShoulder_y', 'lShoulder_z', 'lElbow_x',\n",
       "       'lElbow_y', 'lElbow_z', 'lWrist_x', 'lWrist_y', 'lWrist_z', 'lHip_x',\n",
       "       'lHip_y', 'lHip_z', 'rEar_x', 'rEar_y', 'rEar_z', 'rShoulder_x',\n",
       "       'rShoulder_y', 'rShoulder_z', 'rElbow_x', 'rElbow_y', 'rElbow_z',\n",
       "       'rWrist_x', 'rWrist_y', 'rWrist_z', 'rHip_x', 'rHip_y', 'rHip_z', 's_1',\n",
       "       's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.11917006224393845, train acc: 0.9746272063298843\n",
      "Epoch 50, train loss: 0.11917006224393845, train acc: 0.9746272063298843\n",
      "Epoch 100, train loss: 0.11917006224393845, train acc: 0.9746272063298843\n",
      "Epoch 150, train loss: 0.11917006224393845, train acc: 0.9746272063298843\n",
      "Epoch 200, train loss: 0.11917006224393845, train acc: 0.9746272063298843\n",
      "Accuracy for each output state:\n",
      "0.8311258278145695\n",
      "Value 's_m_h10_val1: 0.8311258278145695' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07436089962720871, train acc: 0.9831700870646766\n",
      "Epoch 50, train loss: 0.07436089962720871, train acc: 0.9831700870646766\n",
      "Epoch 100, train loss: 0.07436089962720871, train acc: 0.9831700870646766\n",
      "Epoch 150, train loss: 0.07436089962720871, train acc: 0.9831700870646766\n",
      "Epoch 200, train loss: 0.07436089962720871, train acc: 0.9831700870646766\n",
      "Accuracy for each output state:\n",
      "0.8315602836879432\n",
      "Value 's_m_h20_val1: 0.8315602836879432' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06241896376013756, train acc: 0.9840670692943421\n",
      "Epoch 50, train loss: 0.06241896376013756, train acc: 0.9840670692943421\n",
      "Epoch 100, train loss: 0.06241896376013756, train acc: 0.9840670692943421\n",
      "Epoch 150, train loss: 0.06241896376013756, train acc: 0.9840670692943421\n",
      "Epoch 200, train loss: 0.06241896376013756, train acc: 0.9840670692943421\n",
      "Accuracy for each output state:\n",
      "0.8358778625954199\n",
      "Value 's_m_h30_val1: 0.8358778625954199' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07707393169403076, train acc: 0.9756176853055917\n",
      "Epoch 50, train loss: 0.07707393169403076, train acc: 0.9756176853055917\n",
      "Epoch 100, train loss: 0.07707393169403076, train acc: 0.9756176853055917\n",
      "Epoch 150, train loss: 0.07707393169403076, train acc: 0.9756176853055917\n",
      "Epoch 200, train loss: 0.07707393169403076, train acc: 0.9756176853055917\n",
      "Accuracy for each output state:\n",
      "0.8533057851239669\n",
      "Value 's_m_h40_val1: 0.8533057851239669' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10546642541885376, train acc: 0.9631154357950765\n",
      "Epoch 50, train loss: 0.10546642541885376, train acc: 0.9631154357950765\n",
      "Epoch 100, train loss: 0.10546642541885376, train acc: 0.9631154357950765\n",
      "Epoch 150, train loss: 0.10546642541885376, train acc: 0.9631154357950765\n",
      "Epoch 200, train loss: 0.10546642541885376, train acc: 0.9631154357950765\n",
      "Accuracy for each output state:\n",
      "0.8693693693693694\n",
      "Value 's_m_h50_val1: 0.8693693693693694' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14536361396312714, train acc: 0.948824931880109\n",
      "Epoch 50, train loss: 0.14536361396312714, train acc: 0.948824931880109\n",
      "Epoch 100, train loss: 0.14536361396312714, train acc: 0.948824931880109\n",
      "Epoch 150, train loss: 0.14536361396312714, train acc: 0.948824931880109\n",
      "Epoch 200, train loss: 0.14536361396312714, train acc: 0.948824931880109\n",
      "Accuracy for each output state:\n",
      "0.8811881188118812\n",
      "Value 's_m_h60_val1: 0.8811881188118812' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19373422861099243, train acc: 0.9342288904396371\n",
      "Epoch 50, train loss: 0.19373422861099243, train acc: 0.9342288904396371\n",
      "Epoch 100, train loss: 0.19373422861099243, train acc: 0.9342288904396371\n",
      "Epoch 150, train loss: 0.19373422861099243, train acc: 0.9342288904396371\n",
      "Epoch 200, train loss: 0.19373422861099243, train acc: 0.9342288904396371\n",
      "Accuracy for each output state:\n",
      "0.8956043956043956\n",
      "Value 's_m_h70_val1: 0.8956043956043956' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2494223415851593, train acc: 0.9202432045779685\n",
      "Epoch 50, train loss: 0.2494223415851593, train acc: 0.9202432045779685\n",
      "Epoch 100, train loss: 0.2494223415851593, train acc: 0.9202432045779685\n",
      "Epoch 150, train loss: 0.2494223415851593, train acc: 0.9202432045779685\n",
      "Epoch 200, train loss: 0.2494223415851593, train acc: 0.9202432045779685\n",
      "Accuracy for each output state:\n",
      "0.9074074074074074\n",
      "Value 's_m_h80_val1: 0.9074074074074074' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3139488101005554, train acc: 0.905493396918562\n",
      "Epoch 50, train loss: 0.3139488101005554, train acc: 0.905493396918562\n",
      "Epoch 100, train loss: 0.3139488101005554, train acc: 0.905493396918562\n",
      "Epoch 150, train loss: 0.3139488101005554, train acc: 0.905493396918562\n",
      "Epoch 200, train loss: 0.3139488101005554, train acc: 0.905493396918562\n",
      "Accuracy for each output state:\n",
      "0.926056338028169\n",
      "Value 's_m_h90_val1: 0.926056338028169' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3837681710720062, train acc: 0.890625\n",
      "Epoch 50, train loss: 0.3837681710720062, train acc: 0.890625\n",
      "Epoch 100, train loss: 0.3837681710720062, train acc: 0.890625\n",
      "Epoch 150, train loss: 0.3837681710720062, train acc: 0.890625\n",
      "Epoch 200, train loss: 0.3837681710720062, train acc: 0.890625\n",
      "Accuracy for each output state:\n",
      "0.9467213114754098\n",
      "Value 's_m_h100_val1: 0.9467213114754098' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.45013394951820374, train acc: 0.8766918020108275\n",
      "Epoch 50, train loss: 0.45013394951820374, train acc: 0.8766918020108275\n",
      "Epoch 100, train loss: 0.45013394951820374, train acc: 0.8766918020108275\n",
      "Epoch 150, train loss: 0.45013394951820374, train acc: 0.8766918020108275\n",
      "Epoch 200, train loss: 0.45013394951820374, train acc: 0.8766918020108275\n",
      "Accuracy for each output state:\n",
      "0.9852941176470589\n",
      "Value 's_m_h110_val1: 0.9852941176470589' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5128182172775269, train acc: 0.8634737678855325\n",
      "Epoch 50, train loss: 0.5128182172775269, train acc: 0.8634737678855325\n",
      "Epoch 100, train loss: 0.5128182172775269, train acc: 0.8634737678855325\n",
      "Epoch 150, train loss: 0.5128182172775269, train acc: 0.8634737678855325\n",
      "Epoch 200, train loss: 0.5128182172775269, train acc: 0.8634737678855325\n",
      "Accuracy for each output state:\n",
      "0.9878048780487805\n",
      "Value 's_m_h120_val1: 0.9878048780487805' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5742144584655762, train acc: 0.8508278822567457\n",
      "Epoch 50, train loss: 0.5742144584655762, train acc: 0.8508278822567457\n",
      "Epoch 100, train loss: 0.5742144584655762, train acc: 0.8508278822567457\n",
      "Epoch 150, train loss: 0.5742144584655762, train acc: 0.8508278822567457\n",
      "Epoch 200, train loss: 0.5742144584655762, train acc: 0.8508278822567457\n",
      "Accuracy for each output state:\n",
      "0.9838709677419355\n",
      "Value 's_m_h130_val1: 0.9838709677419355' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.630157470703125, train acc: 0.8397516835016835\n",
      "Epoch 50, train loss: 0.630157470703125, train acc: 0.8397516835016835\n",
      "Epoch 100, train loss: 0.630157470703125, train acc: 0.8397516835016835\n",
      "Epoch 150, train loss: 0.630157470703125, train acc: 0.8397516835016835\n",
      "Epoch 200, train loss: 0.630157470703125, train acc: 0.8397516835016835\n",
      "Accuracy for each output state:\n",
      "0.9761904761904762\n",
      "Value 's_m_h140_val1: 0.9761904761904762' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6788204312324524, train acc: 0.8283824804856895\n",
      "Epoch 50, train loss: 0.6788204312324524, train acc: 0.8283824804856895\n",
      "Epoch 100, train loss: 0.6788204312324524, train acc: 0.8283824804856895\n",
      "Epoch 150, train loss: 0.6788204312324524, train acc: 0.8283824804856895\n",
      "Epoch 200, train loss: 0.6788204312324524, train acc: 0.8283824804856895\n",
      "Accuracy for each output state:\n",
      "0.9545454545454546\n",
      "Value 's_m_h150_val1: 0.9545454545454546' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1370386779308319, train acc: 0.9711839288654701\n",
      "Epoch 50, train loss: 0.1370386779308319, train acc: 0.9711839288654701\n",
      "Epoch 100, train loss: 0.1370386779308319, train acc: 0.9711839288654701\n",
      "Epoch 150, train loss: 0.1370386779308319, train acc: 0.9711839288654701\n",
      "Epoch 200, train loss: 0.1370386779308319, train acc: 0.9711839288654701\n",
      "Accuracy for each output state:\n",
      "0.9657692307692308\n",
      "Value 's_m_h10_val2: 0.9657692307692308' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08946117013692856, train acc: 0.9787628518456093\n",
      "Epoch 50, train loss: 0.08946117013692856, train acc: 0.9787628518456093\n",
      "Epoch 100, train loss: 0.08946117013692856, train acc: 0.9787628518456093\n",
      "Epoch 150, train loss: 0.08946117013692856, train acc: 0.9787628518456093\n",
      "Epoch 200, train loss: 0.08946117013692856, train acc: 0.9787628518456093\n",
      "Accuracy for each output state:\n",
      "0.9765625\n",
      "Value 's_m_h20_val2: 0.9765625' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07593190670013428, train acc: 0.9794579665113068\n",
      "Epoch 50, train loss: 0.07593190670013428, train acc: 0.9794579665113068\n",
      "Epoch 100, train loss: 0.07593190670013428, train acc: 0.9794579665113068\n",
      "Epoch 150, train loss: 0.07593190670013428, train acc: 0.9794579665113068\n",
      "Epoch 200, train loss: 0.07593190670013428, train acc: 0.9794579665113068\n",
      "Accuracy for each output state:\n",
      "0.9873015873015873\n",
      "Value 's_m_h30_val2: 0.9873015873015873' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08902640640735626, train acc: 0.9702812665841146\n",
      "Epoch 50, train loss: 0.08902640640735626, train acc: 0.9702812665841146\n",
      "Epoch 100, train loss: 0.08902640640735626, train acc: 0.9702812665841146\n",
      "Epoch 150, train loss: 0.08902640640735626, train acc: 0.9702812665841146\n",
      "Epoch 200, train loss: 0.08902640640735626, train acc: 0.9702812665841146\n",
      "Accuracy for each output state:\n",
      "0.9879032258064515\n",
      "Value 's_m_h40_val2: 0.9879032258064515' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11880499124526978, train acc: 0.9575548703065482\n",
      "Epoch 50, train loss: 0.11880499124526978, train acc: 0.9575548703065482\n",
      "Epoch 100, train loss: 0.11880499124526978, train acc: 0.9575548703065482\n",
      "Epoch 150, train loss: 0.11880499124526978, train acc: 0.9575548703065482\n",
      "Epoch 200, train loss: 0.11880499124526978, train acc: 0.9575548703065482\n",
      "Accuracy for each output state:\n",
      "0.9885245901639345\n",
      "Value 's_m_h50_val2: 0.9885245901639345' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.16102764010429382, train acc: 0.9429089893914014\n",
      "Epoch 50, train loss: 0.16102764010429382, train acc: 0.9429089893914014\n",
      "Epoch 100, train loss: 0.16102764010429382, train acc: 0.9429089893914014\n",
      "Epoch 150, train loss: 0.16102764010429382, train acc: 0.9429089893914014\n",
      "Epoch 200, train loss: 0.16102764010429382, train acc: 0.9429089893914014\n",
      "Accuracy for each output state:\n",
      "0.99\n",
      "Value 's_m_h60_val2: 0.99' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2118711918592453, train acc: 0.9283871584177336\n",
      "Epoch 50, train loss: 0.2118711918592453, train acc: 0.9283871584177336\n",
      "Epoch 100, train loss: 0.2118711918592453, train acc: 0.9283871584177336\n",
      "Epoch 150, train loss: 0.2118711918592453, train acc: 0.9283871584177336\n",
      "Epoch 200, train loss: 0.2118711918592453, train acc: 0.9283871584177336\n",
      "Accuracy for each output state:\n",
      "0.9851694915254237\n",
      "Value 's_m_h70_val2: 0.9851694915254237' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.27003881335258484, train acc: 0.9136069114470843\n",
      "Epoch 50, train loss: 0.27003881335258484, train acc: 0.9136069114470843\n",
      "Epoch 100, train loss: 0.27003881335258484, train acc: 0.9136069114470843\n",
      "Epoch 150, train loss: 0.27003881335258484, train acc: 0.9136069114470843\n",
      "Epoch 200, train loss: 0.27003881335258484, train acc: 0.9136069114470843\n",
      "Accuracy for each output state:\n",
      "0.9767241379310345\n",
      "Value 's_m_h80_val2: 0.9767241379310345' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.33704203367233276, train acc: 0.898798707853826\n",
      "Epoch 50, train loss: 0.33704203367233276, train acc: 0.898798707853826\n",
      "Epoch 100, train loss: 0.33704203367233276, train acc: 0.898798707853826\n",
      "Epoch 150, train loss: 0.33704203367233276, train acc: 0.898798707853826\n",
      "Epoch 200, train loss: 0.33704203367233276, train acc: 0.898798707853826\n",
      "Accuracy for each output state:\n",
      "0.9697368421052631\n",
      "Value 's_m_h90_val2: 0.9697368421052631' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.40854182839393616, train acc: 0.883856222730106\n",
      "Epoch 50, train loss: 0.40854182839393616, train acc: 0.883856222730106\n",
      "Epoch 100, train loss: 0.40854182839393616, train acc: 0.883856222730106\n",
      "Epoch 150, train loss: 0.40854182839393616, train acc: 0.883856222730106\n",
      "Epoch 200, train loss: 0.40854182839393616, train acc: 0.883856222730106\n",
      "Accuracy for each output state:\n",
      "0.9611607142857143\n",
      "Value 's_m_h100_val2: 0.9611607142857143' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.47637948393821716, train acc: 0.8696768671089236\n",
      "Epoch 50, train loss: 0.47637948393821716, train acc: 0.8696768671089236\n",
      "Epoch 100, train loss: 0.47637948393821716, train acc: 0.8696768671089236\n",
      "Epoch 150, train loss: 0.47637948393821716, train acc: 0.8696768671089236\n",
      "Epoch 200, train loss: 0.47637948393821716, train acc: 0.8696768671089236\n",
      "Accuracy for each output state:\n",
      "0.9513636363636363\n",
      "Value 's_m_h110_val2: 0.9513636363636363' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5406999588012695, train acc: 0.8567174056915949\n",
      "Epoch 50, train loss: 0.5406999588012695, train acc: 0.8567174056915949\n",
      "Epoch 100, train loss: 0.5406999588012695, train acc: 0.8567174056915949\n",
      "Epoch 150, train loss: 0.5406999588012695, train acc: 0.8567174056915949\n",
      "Epoch 200, train loss: 0.5406999588012695, train acc: 0.8567174056915949\n",
      "Accuracy for each output state:\n",
      "0.9425925925925925\n",
      "Value 's_m_h120_val2: 0.9425925925925925' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6047819256782532, train acc: 0.8437286592305941\n",
      "Epoch 50, train loss: 0.6047819256782532, train acc: 0.8437286592305941\n",
      "Epoch 100, train loss: 0.6047819256782532, train acc: 0.8437286592305941\n",
      "Epoch 150, train loss: 0.6047819256782532, train acc: 0.8437286592305941\n",
      "Epoch 200, train loss: 0.6047819256782532, train acc: 0.8437286592305941\n",
      "Accuracy for each output state:\n",
      "0.9330188679245284\n",
      "Value 's_m_h130_val2: 0.9330188679245284' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6638168692588806, train acc: 0.8315306842229014\n",
      "Epoch 50, train loss: 0.6638168692588806, train acc: 0.8315306842229014\n",
      "Epoch 100, train loss: 0.6638168692588806, train acc: 0.8315306842229014\n",
      "Epoch 150, train loss: 0.6638168692588806, train acc: 0.8315306842229014\n",
      "Epoch 200, train loss: 0.6638168692588806, train acc: 0.8315306842229014\n",
      "Accuracy for each output state:\n",
      "0.9225961538461538\n",
      "Value 's_m_h140_val2: 0.9225961538461538' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.7165622711181641, train acc: 0.8202042304886944\n",
      "Epoch 50, train loss: 0.7165622711181641, train acc: 0.8202042304886944\n",
      "Epoch 100, train loss: 0.7165622711181641, train acc: 0.8202042304886944\n",
      "Epoch 150, train loss: 0.7165622711181641, train acc: 0.8202042304886944\n",
      "Epoch 200, train loss: 0.7165622711181641, train acc: 0.8202042304886944\n",
      "Accuracy for each output state:\n",
      "0.911764705882353\n",
      "Value 's_m_h150_val2: 0.911764705882353' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1312083750963211, train acc: 0.9712133227597145\n",
      "Epoch 50, train loss: 0.1312083750963211, train acc: 0.9712133227597145\n",
      "Epoch 100, train loss: 0.1312083750963211, train acc: 0.9712133227597145\n",
      "Epoch 150, train loss: 0.1312083750963211, train acc: 0.9712133227597145\n",
      "Epoch 200, train loss: 0.1312083750963211, train acc: 0.9712133227597145\n",
      "Accuracy for each output state:\n",
      "0.97188995215311\n",
      "Value 's_m_h10_val3: 0.97188995215311' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08594396710395813, train acc: 0.9796431467964315\n",
      "Epoch 50, train loss: 0.08594396710395813, train acc: 0.9796431467964315\n",
      "Epoch 100, train loss: 0.08594396710395813, train acc: 0.9796431467964315\n",
      "Epoch 150, train loss: 0.08594396710395813, train acc: 0.9796431467964315\n",
      "Epoch 200, train loss: 0.08594396710395813, train acc: 0.9796431467964315\n",
      "Accuracy for each output state:\n",
      "0.9840686274509804\n",
      "Value 's_m_h20_val3: 0.9840686274509804' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07276853919029236, train acc: 0.9807883817427386\n",
      "Epoch 50, train loss: 0.07276853919029236, train acc: 0.9807883817427386\n",
      "Epoch 100, train loss: 0.07276853919029236, train acc: 0.9807883817427386\n",
      "Epoch 150, train loss: 0.07276853919029236, train acc: 0.9807883817427386\n",
      "Epoch 200, train loss: 0.07276853919029236, train acc: 0.9807883817427386\n",
      "Accuracy for each output state:\n",
      "0.9830402010050252\n",
      "Value 's_m_h30_val3: 0.9830402010050252' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08491681516170502, train acc: 0.9718776550552252\n",
      "Epoch 50, train loss: 0.08491681516170502, train acc: 0.9718776550552252\n",
      "Epoch 100, train loss: 0.08491681516170502, train acc: 0.9718776550552252\n",
      "Epoch 150, train loss: 0.08491681516170502, train acc: 0.9718776550552252\n",
      "Epoch 200, train loss: 0.08491681516170502, train acc: 0.9718776550552252\n",
      "Accuracy for each output state:\n",
      "0.9742268041237113\n",
      "Value 's_m_h40_val3: 0.9742268041237113' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11143846809864044, train acc: 0.960313315926893\n",
      "Epoch 50, train loss: 0.11143846809864044, train acc: 0.960313315926893\n",
      "Epoch 100, train loss: 0.11143846809864044, train acc: 0.960313315926893\n",
      "Epoch 150, train loss: 0.11143846809864044, train acc: 0.960313315926893\n",
      "Epoch 200, train loss: 0.11143846809864044, train acc: 0.960313315926893\n",
      "Accuracy for each output state:\n",
      "0.9556878306878307\n",
      "Value 's_m_h50_val3: 0.9556878306878307' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1495562046766281, train acc: 0.9476360392506691\n",
      "Epoch 50, train loss: 0.1495562046766281, train acc: 0.9476360392506691\n",
      "Epoch 100, train loss: 0.1495562046766281, train acc: 0.9476360392506691\n",
      "Epoch 150, train loss: 0.1495562046766281, train acc: 0.9476360392506691\n",
      "Epoch 200, train loss: 0.1495562046766281, train acc: 0.9476360392506691\n",
      "Accuracy for each output state:\n",
      "0.936141304347826\n",
      "Value 's_m_h60_val3: 0.936141304347826' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1951696276664734, train acc: 0.9346752058554437\n",
      "Epoch 50, train loss: 0.1951696276664734, train acc: 0.9346752058554437\n",
      "Epoch 100, train loss: 0.1951696276664734, train acc: 0.9346752058554437\n",
      "Epoch 150, train loss: 0.1951696276664734, train acc: 0.9346752058554437\n",
      "Epoch 200, train loss: 0.1951696276664734, train acc: 0.9346752058554437\n",
      "Accuracy for each output state:\n",
      "0.9162011173184358\n",
      "Value 's_m_h70_val3: 0.9162011173184358' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24703611433506012, train acc: 0.9205633802816902\n",
      "Epoch 50, train loss: 0.24703611433506012, train acc: 0.9205633802816902\n",
      "Epoch 100, train loss: 0.24703611433506012, train acc: 0.9205633802816902\n",
      "Epoch 150, train loss: 0.24703611433506012, train acc: 0.9205633802816902\n",
      "Epoch 200, train loss: 0.24703611433506012, train acc: 0.9205633802816902\n",
      "Accuracy for each output state:\n",
      "0.9001436781609196\n",
      "Value 's_m_h80_val3: 0.9001436781609196' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.30759909749031067, train acc: 0.9067502410800385\n",
      "Epoch 50, train loss: 0.30759909749031067, train acc: 0.9067502410800385\n",
      "Epoch 100, train loss: 0.30759909749031067, train acc: 0.9067502410800385\n",
      "Epoch 150, train loss: 0.30759909749031067, train acc: 0.9067502410800385\n",
      "Epoch 200, train loss: 0.30759909749031067, train acc: 0.9067502410800385\n",
      "Accuracy for each output state:\n",
      "0.8846153846153846\n",
      "Value 's_m_h90_val3: 0.8846153846153846' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3738124966621399, train acc: 0.8925668979187315\n",
      "Epoch 50, train loss: 0.3738124966621399, train acc: 0.8925668979187315\n",
      "Epoch 100, train loss: 0.3738124966621399, train acc: 0.8925668979187315\n",
      "Epoch 150, train loss: 0.3738124966621399, train acc: 0.8925668979187315\n",
      "Epoch 200, train loss: 0.3738124966621399, train acc: 0.8925668979187315\n",
      "Accuracy for each output state:\n",
      "0.8666158536585366\n",
      "Value 's_m_h100_val3: 0.8666158536585366' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.44032660126686096, train acc: 0.8797655453618757\n",
      "Epoch 50, train loss: 0.44032660126686096, train acc: 0.8797655453618757\n",
      "Epoch 100, train loss: 0.44032660126686096, train acc: 0.8797655453618757\n",
      "Epoch 150, train loss: 0.44032660126686096, train acc: 0.8797655453618757\n",
      "Epoch 200, train loss: 0.44032660126686096, train acc: 0.8797655453618757\n",
      "Accuracy for each output state:\n",
      "0.8514150943396227\n",
      "Value 's_m_h110_val3: 0.8514150943396227' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5059806108474731, train acc: 0.8675236096537251\n",
      "Epoch 50, train loss: 0.5059806108474731, train acc: 0.8675236096537251\n",
      "Epoch 100, train loss: 0.5059806108474731, train acc: 0.8675236096537251\n",
      "Epoch 150, train loss: 0.5059806108474731, train acc: 0.8675236096537251\n",
      "Epoch 200, train loss: 0.5059806108474731, train acc: 0.8675236096537251\n",
      "Accuracy for each output state:\n",
      "0.8425324675324675\n",
      "Value 's_m_h120_val3: 0.8425324675324675' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5703888535499573, train acc: 0.8552432432432432\n",
      "Epoch 50, train loss: 0.5703888535499573, train acc: 0.8552432432432432\n",
      "Epoch 100, train loss: 0.5703888535499573, train acc: 0.8552432432432432\n",
      "Epoch 150, train loss: 0.5703888535499573, train acc: 0.8552432432432432\n",
      "Epoch 200, train loss: 0.5703888535499573, train acc: 0.8552432432432432\n",
      "Accuracy for each output state:\n",
      "0.8313758389261745\n",
      "Value 's_m_h130_val3: 0.8313758389261745' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6293841600418091, train acc: 0.8436454849498328\n",
      "Epoch 50, train loss: 0.6293841600418091, train acc: 0.8436454849498328\n",
      "Epoch 100, train loss: 0.6293841600418091, train acc: 0.8436454849498328\n",
      "Epoch 150, train loss: 0.6293841600418091, train acc: 0.8436454849498328\n",
      "Epoch 200, train loss: 0.6293841600418091, train acc: 0.8436454849498328\n",
      "Accuracy for each output state:\n",
      "0.8194444444444444\n",
      "Value 's_m_h140_val3: 0.8194444444444444' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6817476749420166, train acc: 0.8315304948216341\n",
      "Epoch 50, train loss: 0.6817476749420166, train acc: 0.8315304948216341\n",
      "Epoch 100, train loss: 0.6817476749420166, train acc: 0.8315304948216341\n",
      "Epoch 150, train loss: 0.6817476749420166, train acc: 0.8315304948216341\n",
      "Epoch 200, train loss: 0.6817476749420166, train acc: 0.8315304948216341\n",
      "Accuracy for each output state:\n",
      "0.8093525179856116\n",
      "Value 's_m_h150_val3: 0.8093525179856116' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13663388788700104, train acc: 0.9716306420851876\n",
      "Epoch 50, train loss: 0.13663388788700104, train acc: 0.9716306420851876\n",
      "Epoch 100, train loss: 0.13663388788700104, train acc: 0.9716306420851876\n",
      "Epoch 150, train loss: 0.13663388788700104, train acc: 0.9716306420851876\n",
      "Epoch 200, train loss: 0.13663388788700104, train acc: 0.9716306420851876\n",
      "Accuracy for each output state:\n",
      "0.9698375870069605\n",
      "Value 's_m_h10_val4: 0.9698375870069605' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08989044278860092, train acc: 0.9792750325097529\n",
      "Epoch 50, train loss: 0.08989044278860092, train acc: 0.9792750325097529\n",
      "Epoch 100, train loss: 0.08989044278860092, train acc: 0.9792750325097529\n",
      "Epoch 150, train loss: 0.08989044278860092, train acc: 0.9792750325097529\n",
      "Epoch 200, train loss: 0.08989044278860092, train acc: 0.9792750325097529\n",
      "Accuracy for each output state:\n",
      "0.9863420427553444\n",
      "Value 's_m_h20_val4: 0.9863420427553444' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07478608936071396, train acc: 0.9802062541583499\n",
      "Epoch 50, train loss: 0.07478608936071396, train acc: 0.9802062541583499\n",
      "Epoch 100, train loss: 0.07478608936071396, train acc: 0.9802062541583499\n",
      "Epoch 150, train loss: 0.07478608936071396, train acc: 0.9802062541583499\n",
      "Epoch 200, train loss: 0.07478608936071396, train acc: 0.9802062541583499\n",
      "Accuracy for each output state:\n",
      "0.9969586374695864\n",
      "Value 's_m_h30_val4: 0.9969586374695864' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08591293543577194, train acc: 0.9713470708446866\n",
      "Epoch 50, train loss: 0.08591293543577194, train acc: 0.9713470708446866\n",
      "Epoch 100, train loss: 0.08591293543577194, train acc: 0.9713470708446866\n",
      "Epoch 150, train loss: 0.08591293543577194, train acc: 0.9713470708446866\n",
      "Epoch 200, train loss: 0.08591293543577194, train acc: 0.9713470708446866\n",
      "Accuracy for each output state:\n",
      "0.9850374064837906\n",
      "Value 's_m_h40_val4: 0.9850374064837906' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1132185086607933, train acc: 0.9593946266573622\n",
      "Epoch 50, train loss: 0.1132185086607933, train acc: 0.9593946266573622\n",
      "Epoch 100, train loss: 0.1132185086607933, train acc: 0.9593946266573622\n",
      "Epoch 150, train loss: 0.1132185086607933, train acc: 0.9593946266573622\n",
      "Epoch 200, train loss: 0.1132185086607933, train acc: 0.9593946266573622\n",
      "Accuracy for each output state:\n",
      "0.9731457800511509\n",
      "Value 's_m_h50_val4: 0.9731457800511509' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1530851274728775, train acc: 0.9465307582260372\n",
      "Epoch 50, train loss: 0.1530851274728775, train acc: 0.9465307582260372\n",
      "Epoch 100, train loss: 0.1530851274728775, train acc: 0.9465307582260372\n",
      "Epoch 150, train loss: 0.1530851274728775, train acc: 0.9465307582260372\n",
      "Epoch 200, train loss: 0.1530851274728775, train acc: 0.9465307582260372\n",
      "Accuracy for each output state:\n",
      "0.9599737532808399\n",
      "Value 's_m_h60_val4: 0.9599737532808399' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.20104940235614777, train acc: 0.9327311078503302\n",
      "Epoch 50, train loss: 0.20104940235614777, train acc: 0.9327311078503302\n",
      "Epoch 100, train loss: 0.20104940235614777, train acc: 0.9327311078503302\n",
      "Epoch 150, train loss: 0.20104940235614777, train acc: 0.9327311078503302\n",
      "Epoch 200, train loss: 0.20104940235614777, train acc: 0.9327311078503302\n",
      "Accuracy for each output state:\n",
      "0.9460916442048517\n",
      "Value 's_m_h70_val4: 0.9460916442048517' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2551339566707611, train acc: 0.9192394578313253\n",
      "Epoch 50, train loss: 0.2551339566707611, train acc: 0.9192394578313253\n",
      "Epoch 100, train loss: 0.2551339566707611, train acc: 0.9192394578313253\n",
      "Epoch 150, train loss: 0.2551339566707611, train acc: 0.9192394578313253\n",
      "Epoch 200, train loss: 0.2551339566707611, train acc: 0.9192394578313253\n",
      "Accuracy for each output state:\n",
      "0.9307479224376731\n",
      "Value 's_m_h80_val4: 0.9307479224376731' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3178956210613251, train acc: 0.9048723897911833\n",
      "Epoch 50, train loss: 0.3178956210613251, train acc: 0.9048723897911833\n",
      "Epoch 100, train loss: 0.3178956210613251, train acc: 0.9048723897911833\n",
      "Epoch 150, train loss: 0.3178956210613251, train acc: 0.9048723897911833\n",
      "Epoch 200, train loss: 0.3178956210613251, train acc: 0.9048723897911833\n",
      "Accuracy for each output state:\n",
      "0.9166666666666666\n",
      "Value 's_m_h90_val4: 0.9166666666666666' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.38547971844673157, train acc: 0.8902523847376789\n",
      "Epoch 50, train loss: 0.38547971844673157, train acc: 0.8902523847376789\n",
      "Epoch 100, train loss: 0.38547971844673157, train acc: 0.8902523847376789\n",
      "Epoch 150, train loss: 0.38547971844673157, train acc: 0.8902523847376789\n",
      "Epoch 200, train loss: 0.38547971844673157, train acc: 0.8902523847376789\n",
      "Accuracy for each output state:\n",
      "0.8995601173020528\n",
      "Value 's_m_h100_val4: 0.8995601173020528' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4501360356807709, train acc: 0.8758687653311529\n",
      "Epoch 50, train loss: 0.4501360356807709, train acc: 0.8758687653311529\n",
      "Epoch 100, train loss: 0.4501360356807709, train acc: 0.8758687653311529\n",
      "Epoch 150, train loss: 0.4501360356807709, train acc: 0.8758687653311529\n",
      "Epoch 200, train loss: 0.4501360356807709, train acc: 0.8758687653311529\n",
      "Accuracy for each output state:\n",
      "0.8889728096676737\n",
      "Value 's_m_h110_val4: 0.8889728096676737' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5115603804588318, train acc: 0.8626893939393939\n",
      "Epoch 50, train loss: 0.5115603804588318, train acc: 0.8626893939393939\n",
      "Epoch 100, train loss: 0.5115603804588318, train acc: 0.8626893939393939\n",
      "Epoch 150, train loss: 0.5115603804588318, train acc: 0.8626893939393939\n",
      "Epoch 200, train loss: 0.5115603804588318, train acc: 0.8626893939393939\n",
      "Accuracy for each output state:\n",
      "0.8785046728971962\n",
      "Value 's_m_h120_val4: 0.8785046728971962' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5722014904022217, train acc: 0.8502276669557676\n",
      "Epoch 50, train loss: 0.5722014904022217, train acc: 0.8502276669557676\n",
      "Epoch 100, train loss: 0.5722014904022217, train acc: 0.8502276669557676\n",
      "Epoch 150, train loss: 0.5722014904022217, train acc: 0.8502276669557676\n",
      "Epoch 200, train loss: 0.5722014904022217, train acc: 0.8502276669557676\n",
      "Accuracy for each output state:\n",
      "0.8665594855305466\n",
      "Value 's_m_h130_val4: 0.8665594855305466' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6291467547416687, train acc: 0.8393336314847942\n",
      "Epoch 50, train loss: 0.6291467547416687, train acc: 0.8393336314847942\n",
      "Epoch 100, train loss: 0.6291467547416687, train acc: 0.8393336314847942\n",
      "Epoch 150, train loss: 0.6291467547416687, train acc: 0.8393336314847942\n",
      "Epoch 200, train loss: 0.6291467547416687, train acc: 0.8393336314847942\n",
      "Accuracy for each output state:\n",
      "0.8538205980066444\n",
      "Value 's_m_h140_val4: 0.8538205980066444' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6801110506057739, train acc: 0.827966297322253\n",
      "Epoch 50, train loss: 0.6801110506057739, train acc: 0.827966297322253\n",
      "Epoch 100, train loss: 0.6801110506057739, train acc: 0.827966297322253\n",
      "Epoch 150, train loss: 0.6801110506057739, train acc: 0.827966297322253\n",
      "Epoch 200, train loss: 0.6801110506057739, train acc: 0.827966297322253\n",
      "Accuracy for each output state:\n",
      "0.8402061855670103\n",
      "Value 's_m_h150_val4: 0.8402061855670103' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14342501759529114, train acc: 0.9697129419396692\n",
      "Epoch 50, train loss: 0.14342501759529114, train acc: 0.9697129419396692\n",
      "Epoch 100, train loss: 0.14342501759529114, train acc: 0.9697129419396692\n",
      "Epoch 150, train loss: 0.14342501759529114, train acc: 0.9697129419396692\n",
      "Epoch 200, train loss: 0.14342501759529114, train acc: 0.9697129419396692\n",
      "Accuracy for each output state:\n",
      "0.9672351885098743\n",
      "Value 's_m_h10_val5: 0.9672351885098743' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09521935880184174, train acc: 0.9783023564553601\n",
      "Epoch 50, train loss: 0.09521935880184174, train acc: 0.9783023564553601\n",
      "Epoch 100, train loss: 0.09521935880184174, train acc: 0.9783023564553601\n",
      "Epoch 150, train loss: 0.09521935880184174, train acc: 0.9783023564553601\n",
      "Epoch 200, train loss: 0.09521935880184174, train acc: 0.9783023564553601\n",
      "Accuracy for each output state:\n",
      "0.9798903107861061\n",
      "Value 's_m_h20_val5: 0.9798903107861061' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07906828820705414, train acc: 0.9799524294937139\n",
      "Epoch 50, train loss: 0.07906828820705414, train acc: 0.9799524294937139\n",
      "Epoch 100, train loss: 0.07906828820705414, train acc: 0.9799524294937139\n",
      "Epoch 150, train loss: 0.07906828820705414, train acc: 0.9799524294937139\n",
      "Epoch 200, train loss: 0.07906828820705414, train acc: 0.9799524294937139\n",
      "Accuracy for each output state:\n",
      "0.9925512104283053\n",
      "Value 's_m_h30_val5: 0.9925512104283053' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08970243483781815, train acc: 0.9718934911242604\n",
      "Epoch 50, train loss: 0.08970243483781815, train acc: 0.9718934911242604\n",
      "Epoch 100, train loss: 0.08970243483781815, train acc: 0.9718934911242604\n",
      "Epoch 150, train loss: 0.08970243483781815, train acc: 0.9718934911242604\n",
      "Epoch 200, train loss: 0.08970243483781815, train acc: 0.9718934911242604\n",
      "Accuracy for each output state:\n",
      "0.9952561669829222\n",
      "Value 's_m_h40_val5: 0.9952561669829222' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11599241197109222, train acc: 0.9594630752764894\n",
      "Epoch 50, train loss: 0.11599241197109222, train acc: 0.9594630752764894\n",
      "Epoch 100, train loss: 0.11599241197109222, train acc: 0.9594630752764894\n",
      "Epoch 150, train loss: 0.11599241197109222, train acc: 0.9594630752764894\n",
      "Epoch 200, train loss: 0.11599241197109222, train acc: 0.9594630752764894\n",
      "Accuracy for each output state:\n",
      "0.9874274661508704\n",
      "Value 's_m_h50_val5: 0.9874274661508704' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15546955168247223, train acc: 0.9460300036589828\n",
      "Epoch 50, train loss: 0.15546955168247223, train acc: 0.9460300036589828\n",
      "Epoch 100, train loss: 0.15546955168247223, train acc: 0.9460300036589828\n",
      "Epoch 150, train loss: 0.15546955168247223, train acc: 0.9460300036589828\n",
      "Epoch 200, train loss: 0.15546955168247223, train acc: 0.9460300036589828\n",
      "Accuracy for each output state:\n",
      "0.9812623274161736\n",
      "Value 's_m_h60_val5: 0.9812623274161736' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2034374475479126, train acc: 0.9325478783327075\n",
      "Epoch 50, train loss: 0.2034374475479126, train acc: 0.9325478783327075\n",
      "Epoch 100, train loss: 0.2034374475479126, train acc: 0.9325478783327075\n",
      "Epoch 150, train loss: 0.2034374475479126, train acc: 0.9325478783327075\n",
      "Epoch 200, train loss: 0.2034374475479126, train acc: 0.9325478783327075\n",
      "Accuracy for each output state:\n",
      "0.971327967806841\n",
      "Value 's_m_h70_val5: 0.971327967806841' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.25818535685539246, train acc: 0.9179039722329349\n",
      "Epoch 50, train loss: 0.25818535685539246, train acc: 0.9179039722329349\n",
      "Epoch 100, train loss: 0.25818535685539246, train acc: 0.9179039722329349\n",
      "Epoch 150, train loss: 0.25818535685539246, train acc: 0.9179039722329349\n",
      "Epoch 200, train loss: 0.25818535685539246, train acc: 0.9179039722329349\n",
      "Accuracy for each output state:\n",
      "0.9620123203285421\n",
      "Value 's_m_h80_val5: 0.9620123203285421' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.321819931268692, train acc: 0.9038842647641696\n",
      "Epoch 50, train loss: 0.321819931268692, train acc: 0.9038842647641696\n",
      "Epoch 100, train loss: 0.321819931268692, train acc: 0.9038842647641696\n",
      "Epoch 150, train loss: 0.321819931268692, train acc: 0.9038842647641696\n",
      "Epoch 200, train loss: 0.321819931268692, train acc: 0.9038842647641696\n",
      "Accuracy for each output state:\n",
      "0.9517819706498951\n",
      "Value 's_m_h90_val5: 0.9517819706498951' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3898908793926239, train acc: 0.8894211169995924\n",
      "Epoch 50, train loss: 0.3898908793926239, train acc: 0.8894211169995924\n",
      "Epoch 100, train loss: 0.3898908793926239, train acc: 0.8894211169995924\n",
      "Epoch 150, train loss: 0.3898908793926239, train acc: 0.8894211169995924\n",
      "Epoch 200, train loss: 0.3898908793926239, train acc: 0.8894211169995924\n",
      "Accuracy for each output state:\n",
      "0.9405781584582441\n",
      "Value 's_m_h100_val5: 0.9405781584582441' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.45486268401145935, train acc: 0.8760490977759127\n",
      "Epoch 50, train loss: 0.45486268401145935, train acc: 0.8760490977759127\n",
      "Epoch 100, train loss: 0.45486268401145935, train acc: 0.8760490977759127\n",
      "Epoch 150, train loss: 0.45486268401145935, train acc: 0.8760490977759127\n",
      "Epoch 200, train loss: 0.45486268401145935, train acc: 0.8760490977759127\n",
      "Accuracy for each output state:\n",
      "0.9294310722100656\n",
      "Value 's_m_h110_val5: 0.9294310722100656' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5172655582427979, train acc: 0.862948551664505\n",
      "Epoch 50, train loss: 0.5172655582427979, train acc: 0.862948551664505\n",
      "Epoch 100, train loss: 0.5172655582427979, train acc: 0.862948551664505\n",
      "Epoch 150, train loss: 0.5172655582427979, train acc: 0.862948551664505\n",
      "Epoch 200, train loss: 0.5172655582427979, train acc: 0.862948551664505\n",
      "Accuracy for each output state:\n",
      "0.9200223713646533\n",
      "Value 's_m_h120_val5: 0.9200223713646533' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5816261768341064, train acc: 0.8485287561301827\n",
      "Epoch 50, train loss: 0.5816261768341064, train acc: 0.8485287561301827\n",
      "Epoch 100, train loss: 0.5816261768341064, train acc: 0.8485287561301827\n",
      "Epoch 150, train loss: 0.5816261768341064, train acc: 0.8485287561301827\n",
      "Epoch 200, train loss: 0.5816261768341064, train acc: 0.8485287561301827\n",
      "Accuracy for each output state:\n",
      "0.9147597254004577\n",
      "Value 's_m_h130_val5: 0.9147597254004577' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6408299803733826, train acc: 0.8344454670961804\n",
      "Epoch 50, train loss: 0.6408299803733826, train acc: 0.8344454670961804\n",
      "Epoch 100, train loss: 0.6408299803733826, train acc: 0.8344454670961804\n",
      "Epoch 150, train loss: 0.6408299803733826, train acc: 0.8344454670961804\n",
      "Epoch 200, train loss: 0.6408299803733826, train acc: 0.8344454670961804\n",
      "Accuracy for each output state:\n",
      "0.9098360655737705\n",
      "Value 's_m_h140_val5: 0.9098360655737705' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6935784220695496, train acc: 0.8219210651450309\n",
      "Epoch 50, train loss: 0.6935784220695496, train acc: 0.8219210651450309\n",
      "Epoch 100, train loss: 0.6935784220695496, train acc: 0.8219210651450309\n",
      "Epoch 150, train loss: 0.6935784220695496, train acc: 0.8219210651450309\n",
      "Epoch 200, train loss: 0.6935784220695496, train acc: 0.8219210651450309\n",
      "Accuracy for each output state:\n",
      "0.9040767386091128\n",
      "Value 's_m_h150_val5: 0.9040767386091128' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13644203543663025, train acc: 0.9684987694831829\n",
      "Epoch 50, train loss: 0.13644203543663025, train acc: 0.9684987694831829\n",
      "Epoch 100, train loss: 0.13644203543663025, train acc: 0.9684987694831829\n",
      "Epoch 150, train loss: 0.13644203543663025, train acc: 0.9684987694831829\n",
      "Epoch 200, train loss: 0.13644203543663025, train acc: 0.9684987694831829\n",
      "Accuracy for each output state:\n",
      "0.9880573248407643\n",
      "Value 's_m_h10_val6: 0.9880573248407643' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.088963083922863, train acc: 0.9790931989924433\n",
      "Epoch 50, train loss: 0.088963083922863, train acc: 0.9790931989924433\n",
      "Epoch 100, train loss: 0.088963083922863, train acc: 0.9790931989924433\n",
      "Epoch 150, train loss: 0.088963083922863, train acc: 0.9790931989924433\n",
      "Epoch 200, train loss: 0.088963083922863, train acc: 0.9790931989924433\n",
      "Accuracy for each output state:\n",
      "0.9886731391585761\n",
      "Value 's_m_h20_val6: 0.9886731391585761' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0746813490986824, train acc: 0.98134135855546\n",
      "Epoch 50, train loss: 0.0746813490986824, train acc: 0.98134135855546\n",
      "Epoch 100, train loss: 0.0746813490986824, train acc: 0.98134135855546\n",
      "Epoch 150, train loss: 0.0746813490986824, train acc: 0.98134135855546\n",
      "Epoch 200, train loss: 0.0746813490986824, train acc: 0.98134135855546\n",
      "Accuracy for each output state:\n",
      "0.9847861842105263\n",
      "Value 's_m_h30_val6: 0.9847861842105263' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08615338057279587, train acc: 0.9738766519823788\n",
      "Epoch 50, train loss: 0.08615338057279587, train acc: 0.9738766519823788\n",
      "Epoch 100, train loss: 0.08615338057279587, train acc: 0.9738766519823788\n",
      "Epoch 150, train loss: 0.08615338057279587, train acc: 0.9738766519823788\n",
      "Epoch 200, train loss: 0.08615338057279587, train acc: 0.9738766519823788\n",
      "Accuracy for each output state:\n",
      "0.9778428093645485\n",
      "Value 's_m_h40_val6: 0.9778428093645485' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1113068237900734, train acc: 0.9625564588979223\n",
      "Epoch 50, train loss: 0.1113068237900734, train acc: 0.9625564588979223\n",
      "Epoch 100, train loss: 0.1113068237900734, train acc: 0.9625564588979223\n",
      "Epoch 150, train loss: 0.1113068237900734, train acc: 0.9625564588979223\n",
      "Epoch 200, train loss: 0.1113068237900734, train acc: 0.9625564588979223\n",
      "Accuracy for each output state:\n",
      "0.9651360544217688\n",
      "Value 's_m_h50_val6: 0.9651360544217688' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14844219386577606, train acc: 0.949212233549583\n",
      "Epoch 50, train loss: 0.14844219386577606, train acc: 0.949212233549583\n",
      "Epoch 100, train loss: 0.14844219386577606, train acc: 0.949212233549583\n",
      "Epoch 150, train loss: 0.14844219386577606, train acc: 0.949212233549583\n",
      "Epoch 200, train loss: 0.14844219386577606, train acc: 0.949212233549583\n",
      "Accuracy for each output state:\n",
      "0.9528546712802768\n",
      "Value 's_m_h60_val6: 0.9528546712802768' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19399477541446686, train acc: 0.9365842055185537\n",
      "Epoch 50, train loss: 0.19399477541446686, train acc: 0.9365842055185537\n",
      "Epoch 100, train loss: 0.19399477541446686, train acc: 0.9365842055185537\n",
      "Epoch 150, train loss: 0.19399477541446686, train acc: 0.9365842055185537\n",
      "Epoch 200, train loss: 0.19399477541446686, train acc: 0.9365842055185537\n",
      "Accuracy for each output state:\n",
      "0.9414612676056338\n",
      "Value 's_m_h70_val6: 0.9414612676056338' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24776773154735565, train acc: 0.92316715542522\n",
      "Epoch 50, train loss: 0.24776773154735565, train acc: 0.92316715542522\n",
      "Epoch 100, train loss: 0.24776773154735565, train acc: 0.92316715542522\n",
      "Epoch 150, train loss: 0.24776773154735565, train acc: 0.92316715542522\n",
      "Epoch 200, train loss: 0.24776773154735565, train acc: 0.92316715542522\n",
      "Accuracy for each output state:\n",
      "0.9274193548387097\n",
      "Value 's_m_h80_val6: 0.9274193548387097' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.31134796142578125, train acc: 0.9083417085427136\n",
      "Epoch 50, train loss: 0.31134796142578125, train acc: 0.9083417085427136\n",
      "Epoch 100, train loss: 0.31134796142578125, train acc: 0.9083417085427136\n",
      "Epoch 150, train loss: 0.31134796142578125, train acc: 0.9083417085427136\n",
      "Epoch 200, train loss: 0.31134796142578125, train acc: 0.9083417085427136\n",
      "Accuracy for each output state:\n",
      "0.9124087591240876\n",
      "Value 's_m_h90_val6: 0.9124087591240876' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.38103246688842773, train acc: 0.8940020682523268\n",
      "Epoch 50, train loss: 0.38103246688842773, train acc: 0.8940020682523268\n",
      "Epoch 100, train loss: 0.38103246688842773, train acc: 0.8940020682523268\n",
      "Epoch 150, train loss: 0.38103246688842773, train acc: 0.8940020682523268\n",
      "Epoch 200, train loss: 0.38103246688842773, train acc: 0.8940020682523268\n",
      "Accuracy for each output state:\n",
      "0.8982342007434945\n",
      "Value 's_m_h100_val6: 0.8982342007434945' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4477011263370514, train acc: 0.8803514376996805\n",
      "Epoch 50, train loss: 0.4477011263370514, train acc: 0.8803514376996805\n",
      "Epoch 100, train loss: 0.4477011263370514, train acc: 0.8803514376996805\n",
      "Epoch 150, train loss: 0.4477011263370514, train acc: 0.8803514376996805\n",
      "Epoch 200, train loss: 0.4477011263370514, train acc: 0.8803514376996805\n",
      "Accuracy for each output state:\n",
      "0.8821022727272727\n",
      "Value 's_m_h110_val6: 0.8821022727272727' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5106966495513916, train acc: 0.8678924259055982\n",
      "Epoch 50, train loss: 0.5106966495513916, train acc: 0.8678924259055982\n",
      "Epoch 100, train loss: 0.5106966495513916, train acc: 0.8678924259055982\n",
      "Epoch 150, train loss: 0.5106966495513916, train acc: 0.8678924259055982\n",
      "Epoch 200, train loss: 0.5106966495513916, train acc: 0.8678924259055982\n",
      "Accuracy for each output state:\n",
      "0.8672779922779923\n",
      "Value 's_m_h120_val6: 0.8672779922779923' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5718761086463928, train acc: 0.8546998867497169\n",
      "Epoch 50, train loss: 0.5718761086463928, train acc: 0.8546998867497169\n",
      "Epoch 100, train loss: 0.5718761086463928, train acc: 0.8546998867497169\n",
      "Epoch 150, train loss: 0.5718761086463928, train acc: 0.8546998867497169\n",
      "Epoch 200, train loss: 0.5718761086463928, train acc: 0.8546998867497169\n",
      "Accuracy for each output state:\n",
      "0.8528543307086615\n",
      "Value 's_m_h130_val6: 0.8528543307086615' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.627205491065979, train acc: 0.8419883040935673\n",
      "Epoch 50, train loss: 0.627205491065979, train acc: 0.8419883040935673\n",
      "Epoch 100, train loss: 0.627205491065979, train acc: 0.8419883040935673\n",
      "Epoch 150, train loss: 0.627205491065979, train acc: 0.8419883040935673\n",
      "Epoch 200, train loss: 0.627205491065979, train acc: 0.8419883040935673\n",
      "Accuracy for each output state:\n",
      "0.8368473895582329\n",
      "Value 's_m_h140_val6: 0.8368473895582329' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6791941523551941, train acc: 0.8295042321644498\n",
      "Epoch 50, train loss: 0.6791941523551941, train acc: 0.8295042321644498\n",
      "Epoch 100, train loss: 0.6791941523551941, train acc: 0.8295042321644498\n",
      "Epoch 150, train loss: 0.6791941523551941, train acc: 0.8295042321644498\n",
      "Epoch 200, train loss: 0.6791941523551941, train acc: 0.8295042321644498\n",
      "Accuracy for each output state:\n",
      "0.8232581967213115\n",
      "Value 's_m_h150_val6: 0.8232581967213115' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14212201535701752, train acc: 0.9704698537867587\n",
      "Epoch 50, train loss: 0.14212201535701752, train acc: 0.9704698537867587\n",
      "Epoch 100, train loss: 0.14212201535701752, train acc: 0.9704698537867587\n",
      "Epoch 150, train loss: 0.14212201535701752, train acc: 0.9704698537867587\n",
      "Epoch 200, train loss: 0.14212201535701752, train acc: 0.9704698537867587\n",
      "Accuracy for each output state:\n",
      "0.9779874213836478\n",
      "Value 's_m_h10_val7: 0.9779874213836478' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.09387411177158356, train acc: 0.9779720867664369\n",
      "Epoch 50, train loss: 0.09387411177158356, train acc: 0.9779720867664369\n",
      "Epoch 100, train loss: 0.09387411177158356, train acc: 0.9779720867664369\n",
      "Epoch 150, train loss: 0.09387411177158356, train acc: 0.9779720867664369\n",
      "Epoch 200, train loss: 0.09387411177158356, train acc: 0.9779720867664369\n",
      "Accuracy for each output state:\n",
      "0.9896166134185304\n",
      "Value 's_m_h20_val7: 0.9896166134185304' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07800699025392532, train acc: 0.9789478215946271\n",
      "Epoch 50, train loss: 0.07800699025392532, train acc: 0.9789478215946271\n",
      "Epoch 100, train loss: 0.07800699025392532, train acc: 0.9789478215946271\n",
      "Epoch 150, train loss: 0.07800699025392532, train acc: 0.9789478215946271\n",
      "Epoch 200, train loss: 0.07800699025392532, train acc: 0.9789478215946271\n",
      "Accuracy for each output state:\n",
      "0.9939123376623377\n",
      "Value 's_m_h30_val7: 0.9939123376623377' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08973007649183273, train acc: 0.9702664549144168\n",
      "Epoch 50, train loss: 0.08973007649183273, train acc: 0.9702664549144168\n",
      "Epoch 100, train loss: 0.08973007649183273, train acc: 0.9702664549144168\n",
      "Epoch 150, train loss: 0.08973007649183273, train acc: 0.9702664549144168\n",
      "Epoch 200, train loss: 0.08973007649183273, train acc: 0.9702664549144168\n",
      "Accuracy for each output state:\n",
      "0.9867986798679867\n",
      "Value 's_m_h40_val7: 0.9867986798679867' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11752229183912277, train acc: 0.9582504070924552\n",
      "Epoch 50, train loss: 0.11752229183912277, train acc: 0.9582504070924552\n",
      "Epoch 100, train loss: 0.11752229183912277, train acc: 0.9582504070924552\n",
      "Epoch 150, train loss: 0.11752229183912277, train acc: 0.9582504070924552\n",
      "Epoch 200, train loss: 0.11752229183912277, train acc: 0.9582504070924552\n",
      "Accuracy for each output state:\n",
      "0.977768456375839\n",
      "Value 's_m_h50_val7: 0.977768456375839' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15827037394046783, train acc: 0.9447280490068684\n",
      "Epoch 50, train loss: 0.15827037394046783, train acc: 0.9447280490068684\n",
      "Epoch 100, train loss: 0.15827037394046783, train acc: 0.9447280490068684\n",
      "Epoch 150, train loss: 0.15827037394046783, train acc: 0.9447280490068684\n",
      "Epoch 200, train loss: 0.15827037394046783, train acc: 0.9447280490068684\n",
      "Accuracy for each output state:\n",
      "0.9692832764505119\n",
      "Value 's_m_h60_val7: 0.9692832764505119' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2075573205947876, train acc: 0.9308652563369545\n",
      "Epoch 50, train loss: 0.2075573205947876, train acc: 0.9308652563369545\n",
      "Epoch 100, train loss: 0.2075573205947876, train acc: 0.9308652563369545\n",
      "Epoch 150, train loss: 0.2075573205947876, train acc: 0.9308652563369545\n",
      "Epoch 200, train loss: 0.2075573205947876, train acc: 0.9308652563369545\n",
      "Accuracy for each output state:\n",
      "0.9605034722222221\n",
      "Value 's_m_h70_val7: 0.9605034722222221' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.26435860991477966, train acc: 0.9161445075386724\n",
      "Epoch 50, train loss: 0.26435860991477966, train acc: 0.9161445075386724\n",
      "Epoch 100, train loss: 0.26435860991477966, train acc: 0.9161445075386724\n",
      "Epoch 150, train loss: 0.26435860991477966, train acc: 0.9161445075386724\n",
      "Epoch 200, train loss: 0.26435860991477966, train acc: 0.9161445075386724\n",
      "Accuracy for each output state:\n",
      "0.9518551236749117\n",
      "Value 's_m_h80_val7: 0.9518551236749117' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.33107680082321167, train acc: 0.9013489027582041\n",
      "Epoch 50, train loss: 0.33107680082321167, train acc: 0.9013489027582041\n",
      "Epoch 100, train loss: 0.33107680082321167, train acc: 0.9013489027582041\n",
      "Epoch 150, train loss: 0.33107680082321167, train acc: 0.9013489027582041\n",
      "Epoch 200, train loss: 0.33107680082321167, train acc: 0.9013489027582041\n",
      "Accuracy for each output state:\n",
      "0.9424460431654675\n",
      "Value 's_m_h90_val7: 0.9424460431654675' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4032265245914459, train acc: 0.8866790967474621\n",
      "Epoch 50, train loss: 0.4032265245914459, train acc: 0.8866790967474621\n",
      "Epoch 100, train loss: 0.4032265245914459, train acc: 0.8866790967474621\n",
      "Epoch 150, train loss: 0.4032265245914459, train acc: 0.8866790967474621\n",
      "Epoch 200, train loss: 0.4032265245914459, train acc: 0.8866790967474621\n",
      "Accuracy for each output state:\n",
      "0.9326923076923077\n",
      "Value 's_m_h100_val7: 0.9326923076923077' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4726811349391937, train acc: 0.8721463622786431\n",
      "Epoch 50, train loss: 0.4726811349391937, train acc: 0.8721463622786431\n",
      "Epoch 100, train loss: 0.4726811349391937, train acc: 0.8721463622786431\n",
      "Epoch 150, train loss: 0.4726811349391937, train acc: 0.8721463622786431\n",
      "Epoch 200, train loss: 0.4726811349391937, train acc: 0.8721463622786431\n",
      "Accuracy for each output state:\n",
      "0.9221082089552238\n",
      "Value 's_m_h110_val7: 0.9221082089552238' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5388759970664978, train acc: 0.8585880800527821\n",
      "Epoch 50, train loss: 0.5388759970664978, train acc: 0.8585880800527821\n",
      "Epoch 100, train loss: 0.5388759970664978, train acc: 0.8585880800527821\n",
      "Epoch 150, train loss: 0.5388759970664978, train acc: 0.8585880800527821\n",
      "Epoch 200, train loss: 0.5388759970664978, train acc: 0.8585880800527821\n",
      "Accuracy for each output state:\n",
      "0.9125475285171102\n",
      "Value 's_m_h120_val7: 0.9125475285171102' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6043439507484436, train acc: 0.8449625595643295\n",
      "Epoch 50, train loss: 0.6043439507484436, train acc: 0.8449625595643295\n",
      "Epoch 100, train loss: 0.6043439507484436, train acc: 0.8449625595643295\n",
      "Epoch 150, train loss: 0.6043439507484436, train acc: 0.8449625595643295\n",
      "Epoch 200, train loss: 0.6043439507484436, train acc: 0.8449625595643295\n",
      "Accuracy for each output state:\n",
      "0.9060077519379846\n",
      "Value 's_m_h130_val7: 0.9060077519379846' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6648086309432983, train acc: 0.8332552144363722\n",
      "Epoch 50, train loss: 0.6648086309432983, train acc: 0.8332552144363722\n",
      "Epoch 100, train loss: 0.6648086309432983, train acc: 0.8332552144363722\n",
      "Epoch 150, train loss: 0.6648086309432983, train acc: 0.8332552144363722\n",
      "Epoch 200, train loss: 0.6648086309432983, train acc: 0.8332552144363722\n",
      "Accuracy for each output state:\n",
      "0.8992094861660078\n",
      "Value 's_m_h140_val7: 0.8992094861660078' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.7186407446861267, train acc: 0.8211170341652532\n",
      "Epoch 50, train loss: 0.7186407446861267, train acc: 0.8211170341652532\n",
      "Epoch 100, train loss: 0.7186407446861267, train acc: 0.8211170341652532\n",
      "Epoch 150, train loss: 0.7186407446861267, train acc: 0.8211170341652532\n",
      "Epoch 200, train loss: 0.7186407446861267, train acc: 0.8211170341652532\n",
      "Accuracy for each output state:\n",
      "0.8931451612903225\n",
      "Value 's_m_h150_val7: 0.8931451612903225' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.12854048609733582, train acc: 0.9717672074342416\n",
      "Epoch 50, train loss: 0.12854048609733582, train acc: 0.9717672074342416\n",
      "Epoch 100, train loss: 0.12854048609733582, train acc: 0.9717672074342416\n",
      "Epoch 150, train loss: 0.12854048609733582, train acc: 0.9717672074342416\n",
      "Epoch 200, train loss: 0.12854048609733582, train acc: 0.9717672074342416\n",
      "Accuracy for each output state:\n",
      "0.9665775401069518\n",
      "Value 's_m_h10_val8: 0.9665775401069518' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08161813020706177, train acc: 0.9815187630858432\n",
      "Epoch 50, train loss: 0.08161813020706177, train acc: 0.9815187630858432\n",
      "Epoch 100, train loss: 0.08161813020706177, train acc: 0.9815187630858432\n",
      "Epoch 150, train loss: 0.08161813020706177, train acc: 0.9815187630858432\n",
      "Epoch 200, train loss: 0.08161813020706177, train acc: 0.9815187630858432\n",
      "Accuracy for each output state:\n",
      "0.9642857142857143\n",
      "Value 's_m_h20_val8: 0.9642857142857143' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06760192662477493, train acc: 0.9829461196243203\n",
      "Epoch 50, train loss: 0.06760192662477493, train acc: 0.9829461196243203\n",
      "Epoch 100, train loss: 0.06760192662477493, train acc: 0.9829461196243203\n",
      "Epoch 150, train loss: 0.06760192662477493, train acc: 0.9829461196243203\n",
      "Epoch 200, train loss: 0.06760192662477493, train acc: 0.9829461196243203\n",
      "Accuracy for each output state:\n",
      "0.9597457627118645\n",
      "Value 's_m_h30_val8: 0.9597457627118645' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07953424751758575, train acc: 0.9738573115196492\n",
      "Epoch 50, train loss: 0.07953424751758575, train acc: 0.9738573115196492\n",
      "Epoch 100, train loss: 0.07953424751758575, train acc: 0.9738573115196492\n",
      "Epoch 150, train loss: 0.07953424751758575, train acc: 0.9738573115196492\n",
      "Epoch 200, train loss: 0.07953424751758575, train acc: 0.9738573115196492\n",
      "Accuracy for each output state:\n",
      "0.9513081395348837\n",
      "Value 's_m_h40_val8: 0.9513081395348837' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10561024397611618, train acc: 0.9622991881153913\n",
      "Epoch 50, train loss: 0.10561024397611618, train acc: 0.9622991881153913\n",
      "Epoch 100, train loss: 0.10561024397611618, train acc: 0.9622991881153913\n",
      "Epoch 150, train loss: 0.10561024397611618, train acc: 0.9622991881153913\n",
      "Epoch 200, train loss: 0.10561024397611618, train acc: 0.9622991881153913\n",
      "Accuracy for each output state:\n",
      "0.9348802395209581\n",
      "Value 's_m_h50_val8: 0.9348802395209581' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1432449221611023, train acc: 0.9494158258098778\n",
      "Epoch 50, train loss: 0.1432449221611023, train acc: 0.9494158258098778\n",
      "Epoch 100, train loss: 0.1432449221611023, train acc: 0.9494158258098778\n",
      "Epoch 150, train loss: 0.1432449221611023, train acc: 0.9494158258098778\n",
      "Epoch 200, train loss: 0.1432449221611023, train acc: 0.9494158258098778\n",
      "Accuracy for each output state:\n",
      "0.9112654320987654\n",
      "Value 's_m_h60_val8: 0.9112654320987654' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18838630616664886, train acc: 0.9363768379016155\n",
      "Epoch 50, train loss: 0.18838630616664886, train acc: 0.9363768379016155\n",
      "Epoch 100, train loss: 0.18838630616664886, train acc: 0.9363768379016155\n",
      "Epoch 150, train loss: 0.18838630616664886, train acc: 0.9363768379016155\n",
      "Epoch 200, train loss: 0.18838630616664886, train acc: 0.9363768379016155\n",
      "Accuracy for each output state:\n",
      "0.8853503184713376\n",
      "Value 's_m_h70_val8: 0.8853503184713376' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2408498376607895, train acc: 0.9230769230769231\n",
      "Epoch 50, train loss: 0.2408498376607895, train acc: 0.9230769230769231\n",
      "Epoch 100, train loss: 0.2408498376607895, train acc: 0.9230769230769231\n",
      "Epoch 150, train loss: 0.2408498376607895, train acc: 0.9230769230769231\n",
      "Epoch 200, train loss: 0.2408498376607895, train acc: 0.9230769230769231\n",
      "Accuracy for each output state:\n",
      "0.8585526315789475\n",
      "Value 's_m_h80_val8: 0.8585526315789475' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.30148735642433167, train acc: 0.9096385542168675\n",
      "Epoch 50, train loss: 0.30148735642433167, train acc: 0.9096385542168675\n",
      "Epoch 100, train loss: 0.30148735642433167, train acc: 0.9096385542168675\n",
      "Epoch 150, train loss: 0.30148735642433167, train acc: 0.9096385542168675\n",
      "Epoch 200, train loss: 0.30148735642433167, train acc: 0.9096385542168675\n",
      "Accuracy for each output state:\n",
      "0.8282312925170068\n",
      "Value 's_m_h90_val8: 0.8282312925170068' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.365884929895401, train acc: 0.8957555511888387\n",
      "Epoch 50, train loss: 0.365884929895401, train acc: 0.8957555511888387\n",
      "Epoch 100, train loss: 0.365884929895401, train acc: 0.8957555511888387\n",
      "Epoch 150, train loss: 0.365884929895401, train acc: 0.8957555511888387\n",
      "Epoch 200, train loss: 0.365884929895401, train acc: 0.8957555511888387\n",
      "Accuracy for each output state:\n",
      "0.7957746478873239\n",
      "Value 's_m_h100_val8: 0.7957746478873239' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4261959195137024, train acc: 0.8836633663366337\n",
      "Epoch 50, train loss: 0.4261959195137024, train acc: 0.8836633663366337\n",
      "Epoch 100, train loss: 0.4261959195137024, train acc: 0.8836633663366337\n",
      "Epoch 150, train loss: 0.4261959195137024, train acc: 0.8836633663366337\n",
      "Epoch 200, train loss: 0.4261959195137024, train acc: 0.8836633663366337\n",
      "Accuracy for each output state:\n",
      "0.7627737226277372\n",
      "Value 's_m_h110_val8: 0.7627737226277372' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4839707314968109, train acc: 0.872010813058848\n",
      "Epoch 50, train loss: 0.4839707314968109, train acc: 0.872010813058848\n",
      "Epoch 100, train loss: 0.4839707314968109, train acc: 0.872010813058848\n",
      "Epoch 150, train loss: 0.4839707314968109, train acc: 0.872010813058848\n",
      "Epoch 200, train loss: 0.4839707314968109, train acc: 0.872010813058848\n",
      "Accuracy for each output state:\n",
      "0.7367424242424242\n",
      "Value 's_m_h120_val8: 0.7367424242424242' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5421417355537415, train acc: 0.860944527736132\n",
      "Epoch 50, train loss: 0.5421417355537415, train acc: 0.860944527736132\n",
      "Epoch 100, train loss: 0.5421417355537415, train acc: 0.860944527736132\n",
      "Epoch 150, train loss: 0.5421417355537415, train acc: 0.860944527736132\n",
      "Epoch 200, train loss: 0.5421417355537415, train acc: 0.860944527736132\n",
      "Accuracy for each output state:\n",
      "0.7106299212598425\n",
      "Value 's_m_h130_val8: 0.7106299212598425' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5999771356582642, train acc: 0.8498564804592625\n",
      "Epoch 50, train loss: 0.5999771356582642, train acc: 0.8498564804592625\n",
      "Epoch 100, train loss: 0.5999771356582642, train acc: 0.8498564804592625\n",
      "Epoch 150, train loss: 0.5999771356582642, train acc: 0.8498564804592625\n",
      "Epoch 200, train loss: 0.5999771356582642, train acc: 0.8498564804592625\n",
      "Accuracy for each output state:\n",
      "0.6885245901639343\n",
      "Value 's_m_h140_val8: 0.6885245901639343' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6532652974128723, train acc: 0.8383458646616542\n",
      "Epoch 50, train loss: 0.6532652974128723, train acc: 0.8383458646616542\n",
      "Epoch 100, train loss: 0.6532652974128723, train acc: 0.8383458646616542\n",
      "Epoch 150, train loss: 0.6532652974128723, train acc: 0.8383458646616542\n",
      "Epoch 200, train loss: 0.6532652974128723, train acc: 0.8383458646616542\n",
      "Accuracy for each output state:\n",
      "0.6666666666666666\n",
      "Value 's_m_h150_val8: 0.6666666666666666' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13262280821800232, train acc: 0.9728384991843393\n",
      "Epoch 50, train loss: 0.13262280821800232, train acc: 0.9728384991843393\n",
      "Epoch 100, train loss: 0.13262280821800232, train acc: 0.9728384991843393\n",
      "Epoch 150, train loss: 0.13262280821800232, train acc: 0.9728384991843393\n",
      "Epoch 200, train loss: 0.13262280821800232, train acc: 0.9728384991843393\n",
      "Accuracy for each output state:\n",
      "0.9717537942664418\n",
      "Value 's_m_h10_val9: 0.9717537942664418' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08751290291547775, train acc: 0.9805926544240401\n",
      "Epoch 50, train loss: 0.08751290291547775, train acc: 0.9805926544240401\n",
      "Epoch 100, train loss: 0.08751290291547775, train acc: 0.9805926544240401\n",
      "Epoch 150, train loss: 0.08751290291547775, train acc: 0.9805926544240401\n",
      "Epoch 200, train loss: 0.08751290291547775, train acc: 0.9805926544240401\n",
      "Accuracy for each output state:\n",
      "0.983704974271012\n",
      "Value 's_m_h20_val9: 0.983704974271012' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07433486729860306, train acc: 0.9800427350427351\n",
      "Epoch 50, train loss: 0.07433486729860306, train acc: 0.9800427350427351\n",
      "Epoch 100, train loss: 0.07433486729860306, train acc: 0.9800427350427351\n",
      "Epoch 150, train loss: 0.07433486729860306, train acc: 0.9800427350427351\n",
      "Epoch 200, train loss: 0.07433486729860306, train acc: 0.9800427350427351\n",
      "Accuracy for each output state:\n",
      "0.9917102966841187\n",
      "Value 's_m_h30_val9: 0.9917102966841187' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08776995539665222, train acc: 0.9707092819614711\n",
      "Epoch 50, train loss: 0.08776995539665222, train acc: 0.9707092819614711\n",
      "Epoch 100, train loss: 0.08776995539665222, train acc: 0.9707092819614711\n",
      "Epoch 150, train loss: 0.08776995539665222, train acc: 0.9707092819614711\n",
      "Epoch 200, train loss: 0.08776995539665222, train acc: 0.9707092819614711\n",
      "Accuracy for each output state:\n",
      "0.9911190053285968\n",
      "Value 's_m_h40_val9: 0.9911190053285968' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11654525995254517, train acc: 0.9585727109515261\n",
      "Epoch 50, train loss: 0.11654525995254517, train acc: 0.9585727109515261\n",
      "Epoch 100, train loss: 0.11654525995254517, train acc: 0.9585727109515261\n",
      "Epoch 150, train loss: 0.11654525995254517, train acc: 0.9585727109515261\n",
      "Epoch 200, train loss: 0.11654525995254517, train acc: 0.9585727109515261\n",
      "Accuracy for each output state:\n",
      "0.9778481012658228\n",
      "Value 's_m_h50_val9: 0.9778481012658228' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.15826807916164398, train acc: 0.94548802946593\n",
      "Epoch 50, train loss: 0.15826807916164398, train acc: 0.94548802946593\n",
      "Epoch 100, train loss: 0.15826807916164398, train acc: 0.94548802946593\n",
      "Epoch 150, train loss: 0.15826807916164398, train acc: 0.94548802946593\n",
      "Epoch 200, train loss: 0.15826807916164398, train acc: 0.94548802946593\n",
      "Accuracy for each output state:\n",
      "0.9650092081031307\n",
      "Value 's_m_h60_val9: 0.9650092081031307' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.20962971448898315, train acc: 0.9316162570888469\n",
      "Epoch 50, train loss: 0.20962971448898315, train acc: 0.9316162570888469\n",
      "Epoch 100, train loss: 0.20962971448898315, train acc: 0.9316162570888469\n",
      "Epoch 150, train loss: 0.20962971448898315, train acc: 0.9316162570888469\n",
      "Epoch 200, train loss: 0.20962971448898315, train acc: 0.9316162570888469\n",
      "Accuracy for each output state:\n",
      "0.9554409005628518\n",
      "Value 's_m_h70_val9: 0.9554409005628518' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2681564390659332, train acc: 0.9174757281553398\n",
      "Epoch 50, train loss: 0.2681564390659332, train acc: 0.9174757281553398\n",
      "Epoch 100, train loss: 0.2681564390659332, train acc: 0.9174757281553398\n",
      "Epoch 150, train loss: 0.2681564390659332, train acc: 0.9174757281553398\n",
      "Epoch 200, train loss: 0.2681564390659332, train acc: 0.9174757281553398\n",
      "Accuracy for each output state:\n",
      "0.9459847036328871\n",
      "Value 's_m_h80_val9: 0.9459847036328871' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.33559340238571167, train acc: 0.9026447105788423\n",
      "Epoch 50, train loss: 0.33559340238571167, train acc: 0.9026447105788423\n",
      "Epoch 100, train loss: 0.33559340238571167, train acc: 0.9026447105788423\n",
      "Epoch 150, train loss: 0.33559340238571167, train acc: 0.9026447105788423\n",
      "Epoch 200, train loss: 0.33559340238571167, train acc: 0.9026447105788423\n",
      "Accuracy for each output state:\n",
      "0.9361598440545809\n",
      "Value 's_m_h90_val9: 0.9361598440545809' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.40812429785728455, train acc: 0.8873716632443532\n",
      "Epoch 50, train loss: 0.40812429785728455, train acc: 0.8873716632443532\n",
      "Epoch 100, train loss: 0.40812429785728455, train acc: 0.8873716632443532\n",
      "Epoch 150, train loss: 0.40812429785728455, train acc: 0.8873716632443532\n",
      "Epoch 200, train loss: 0.40812429785728455, train acc: 0.8873716632443532\n",
      "Accuracy for each output state:\n",
      "0.9249502982107356\n",
      "Value 's_m_h100_val9: 0.9249502982107356' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4788561761379242, train acc: 0.8728329809725158\n",
      "Epoch 50, train loss: 0.4788561761379242, train acc: 0.8728329809725158\n",
      "Epoch 100, train loss: 0.4788561761379242, train acc: 0.8728329809725158\n",
      "Epoch 150, train loss: 0.4788561761379242, train acc: 0.8728329809725158\n",
      "Epoch 200, train loss: 0.4788561761379242, train acc: 0.8728329809725158\n",
      "Accuracy for each output state:\n",
      "0.9132860040567952\n",
      "Value 's_m_h110_val9: 0.9132860040567952' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5453515648841858, train acc: 0.859041394335512\n",
      "Epoch 50, train loss: 0.5453515648841858, train acc: 0.859041394335512\n",
      "Epoch 100, train loss: 0.5453515648841858, train acc: 0.859041394335512\n",
      "Epoch 150, train loss: 0.5453515648841858, train acc: 0.859041394335512\n",
      "Epoch 200, train loss: 0.5453515648841858, train acc: 0.859041394335512\n",
      "Accuracy for each output state:\n",
      "0.9016563146997929\n",
      "Value 's_m_h120_val9: 0.9016563146997929' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6096391081809998, train acc: 0.8456179775280899\n",
      "Epoch 50, train loss: 0.6096391081809998, train acc: 0.8456179775280899\n",
      "Epoch 100, train loss: 0.6096391081809998, train acc: 0.8456179775280899\n",
      "Epoch 150, train loss: 0.6096391081809998, train acc: 0.8456179775280899\n",
      "Epoch 200, train loss: 0.6096391081809998, train acc: 0.8456179775280899\n",
      "Accuracy for each output state:\n",
      "0.8895348837209303\n",
      "Value 's_m_h130_val9: 0.8895348837209303' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.668190062046051, train acc: 0.8336426914153132\n",
      "Epoch 50, train loss: 0.668190062046051, train acc: 0.8336426914153132\n",
      "Epoch 100, train loss: 0.668190062046051, train acc: 0.8336426914153132\n",
      "Epoch 150, train loss: 0.668190062046051, train acc: 0.8336426914153132\n",
      "Epoch 200, train loss: 0.668190062046051, train acc: 0.8336426914153132\n",
      "Accuracy for each output state:\n",
      "0.8768898488120951\n",
      "Value 's_m_h140_val9: 0.8768898488120951' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.7194205522537231, train acc: 0.8218824940047962\n",
      "Epoch 50, train loss: 0.7194205522537231, train acc: 0.8218824940047962\n",
      "Epoch 100, train loss: 0.7194205522537231, train acc: 0.8218824940047962\n",
      "Epoch 150, train loss: 0.7194205522537231, train acc: 0.8218824940047962\n",
      "Epoch 200, train loss: 0.7194205522537231, train acc: 0.8218824940047962\n",
      "Accuracy for each output state:\n",
      "0.8647902869757175\n",
      "Value 's_m_h150_val9: 0.8647902869757175' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1360592544078827, train acc: 0.9705675970684547\n",
      "Epoch 50, train loss: 0.1360592544078827, train acc: 0.9705675970684547\n",
      "Epoch 100, train loss: 0.1360592544078827, train acc: 0.9705675970684547\n",
      "Epoch 150, train loss: 0.1360592544078827, train acc: 0.9705675970684547\n",
      "Epoch 200, train loss: 0.1360592544078827, train acc: 0.9705675970684547\n",
      "Accuracy for each output state:\n",
      "0.9685483870967742\n",
      "Value 's_m_h10_val10: 0.9685483870967742' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08995483815670013, train acc: 0.9796349434082576\n",
      "Epoch 50, train loss: 0.08995483815670013, train acc: 0.9796349434082576\n",
      "Epoch 100, train loss: 0.08995483815670013, train acc: 0.9796349434082576\n",
      "Epoch 150, train loss: 0.08995483815670013, train acc: 0.9796349434082576\n",
      "Epoch 200, train loss: 0.08995483815670013, train acc: 0.9796349434082576\n",
      "Accuracy for each output state:\n",
      "0.975\n",
      "Value 's_m_h20_val10: 0.975' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07264718413352966, train acc: 0.9817381379422795\n",
      "Epoch 50, train loss: 0.07264718413352966, train acc: 0.9817381379422795\n",
      "Epoch 100, train loss: 0.07264718413352966, train acc: 0.9817381379422795\n",
      "Epoch 150, train loss: 0.07264718413352966, train acc: 0.9817381379422795\n",
      "Epoch 200, train loss: 0.07264718413352966, train acc: 0.9817381379422795\n",
      "Accuracy for each output state:\n",
      "0.9655172413793103\n",
      "Value 's_m_h30_val10: 0.9655172413793103' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0818968191742897, train acc: 0.9753879526113799\n",
      "Epoch 50, train loss: 0.0818968191742897, train acc: 0.9753879526113799\n",
      "Epoch 100, train loss: 0.0818968191742897, train acc: 0.9753879526113799\n",
      "Epoch 150, train loss: 0.0818968191742897, train acc: 0.9753879526113799\n",
      "Epoch 200, train loss: 0.0818968191742897, train acc: 0.9753879526113799\n",
      "Accuracy for each output state:\n",
      "0.9375\n",
      "Value 's_m_h40_val10: 0.9375' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10697866231203079, train acc: 0.9638646847770375\n",
      "Epoch 50, train loss: 0.10697866231203079, train acc: 0.9638646847770375\n",
      "Epoch 100, train loss: 0.10697866231203079, train acc: 0.9638646847770375\n",
      "Epoch 150, train loss: 0.10697866231203079, train acc: 0.9638646847770375\n",
      "Epoch 200, train loss: 0.10697866231203079, train acc: 0.9638646847770375\n",
      "Accuracy for each output state:\n",
      "0.9166666666666666\n",
      "Value 's_m_h50_val10: 0.9166666666666666' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14419089257717133, train acc: 0.9507264134430247\n",
      "Epoch 50, train loss: 0.14419089257717133, train acc: 0.9507264134430247\n",
      "Epoch 100, train loss: 0.14419089257717133, train acc: 0.9507264134430247\n",
      "Epoch 150, train loss: 0.14419089257717133, train acc: 0.9507264134430247\n",
      "Epoch 200, train loss: 0.14419089257717133, train acc: 0.9507264134430247\n",
      "Accuracy for each output state:\n",
      "0.8961538461538461\n",
      "Value 's_m_h60_val10: 0.8961538461538461' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18919718265533447, train acc: 0.9371074825049345\n",
      "Epoch 50, train loss: 0.18919718265533447, train acc: 0.9371074825049345\n",
      "Epoch 100, train loss: 0.18919718265533447, train acc: 0.9371074825049345\n",
      "Epoch 150, train loss: 0.18919718265533447, train acc: 0.9371074825049345\n",
      "Epoch 200, train loss: 0.18919718265533447, train acc: 0.9371074825049345\n",
      "Accuracy for each output state:\n",
      "0.873\n",
      "Value 's_m_h70_val10: 0.873' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2403031289577484, train acc: 0.9234769004233389\n",
      "Epoch 50, train loss: 0.2403031289577484, train acc: 0.9234769004233389\n",
      "Epoch 100, train loss: 0.2403031289577484, train acc: 0.9234769004233389\n",
      "Epoch 150, train loss: 0.2403031289577484, train acc: 0.9234769004233389\n",
      "Epoch 200, train loss: 0.2403031289577484, train acc: 0.9234769004233389\n",
      "Accuracy for each output state:\n",
      "0.8541666666666667\n",
      "Value 's_m_h80_val10: 0.8541666666666667' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.29989007115364075, train acc: 0.9097865104855469\n",
      "Epoch 50, train loss: 0.29989007115364075, train acc: 0.9097865104855469\n",
      "Epoch 100, train loss: 0.29989007115364075, train acc: 0.9097865104855469\n",
      "Epoch 150, train loss: 0.29989007115364075, train acc: 0.9097865104855469\n",
      "Epoch 200, train loss: 0.29989007115364075, train acc: 0.9097865104855469\n",
      "Accuracy for each output state:\n",
      "0.8369565217391305\n",
      "Value 's_m_h90_val10: 0.8369565217391305' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3656262159347534, train acc: 0.8952066757228799\n",
      "Epoch 50, train loss: 0.3656262159347534, train acc: 0.8952066757228799\n",
      "Epoch 100, train loss: 0.3656262159347534, train acc: 0.8952066757228799\n",
      "Epoch 150, train loss: 0.3656262159347534, train acc: 0.8952066757228799\n",
      "Epoch 200, train loss: 0.3656262159347534, train acc: 0.8952066757228799\n",
      "Accuracy for each output state:\n",
      "0.8261363636363637\n",
      "Value 's_m_h100_val10: 0.8261363636363637' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.43062207102775574, train acc: 0.881508078994614\n",
      "Epoch 50, train loss: 0.43062207102775574, train acc: 0.881508078994614\n",
      "Epoch 100, train loss: 0.43062207102775574, train acc: 0.881508078994614\n",
      "Epoch 150, train loss: 0.43062207102775574, train acc: 0.881508078994614\n",
      "Epoch 200, train loss: 0.43062207102775574, train acc: 0.881508078994614\n",
      "Accuracy for each output state:\n",
      "0.8202380952380952\n",
      "Value 's_m_h110_val10: 0.8202380952380952' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.49307242035865784, train acc: 0.8680997332238867\n",
      "Epoch 50, train loss: 0.49307242035865784, train acc: 0.8680997332238867\n",
      "Epoch 100, train loss: 0.49307242035865784, train acc: 0.8680997332238867\n",
      "Epoch 150, train loss: 0.49307242035865784, train acc: 0.8680997332238867\n",
      "Epoch 200, train loss: 0.49307242035865784, train acc: 0.8680997332238867\n",
      "Accuracy for each output state:\n",
      "0.8125\n",
      "Value 's_m_h120_val10: 0.8125' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5547478199005127, train acc: 0.8543735474329178\n",
      "Epoch 50, train loss: 0.5547478199005127, train acc: 0.8543735474329178\n",
      "Epoch 100, train loss: 0.5547478199005127, train acc: 0.8543735474329178\n",
      "Epoch 150, train loss: 0.5547478199005127, train acc: 0.8543735474329178\n",
      "Epoch 200, train loss: 0.5547478199005127, train acc: 0.8543735474329178\n",
      "Accuracy for each output state:\n",
      "0.8039473684210526\n",
      "Value 's_m_h130_val10: 0.8039473684210526' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6114788055419922, train acc: 0.8417156542564772\n",
      "Epoch 50, train loss: 0.6114788055419922, train acc: 0.8417156542564772\n",
      "Epoch 100, train loss: 0.6114788055419922, train acc: 0.8417156542564772\n",
      "Epoch 150, train loss: 0.6114788055419922, train acc: 0.8417156542564772\n",
      "Epoch 200, train loss: 0.6114788055419922, train acc: 0.8417156542564772\n",
      "Accuracy for each output state:\n",
      "0.7958333333333334\n",
      "Value 's_m_h140_val10: 0.7958333333333334' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6625216603279114, train acc: 0.8300583876038625\n",
      "Epoch 50, train loss: 0.6625216603279114, train acc: 0.8300583876038625\n",
      "Epoch 100, train loss: 0.6625216603279114, train acc: 0.8300583876038625\n",
      "Epoch 150, train loss: 0.6625216603279114, train acc: 0.8300583876038625\n",
      "Epoch 200, train loss: 0.6625216603279114, train acc: 0.8300583876038625\n",
      "Accuracy for each output state:\n",
      "0.7838235294117647\n",
      "Value 's_m_h150_val10: 0.7838235294117647' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13660039007663727, train acc: 0.9704814522494081\n",
      "Epoch 50, train loss: 0.13660039007663727, train acc: 0.9704814522494081\n",
      "Epoch 100, train loss: 0.13660039007663727, train acc: 0.9704814522494081\n",
      "Epoch 150, train loss: 0.13660039007663727, train acc: 0.9704814522494081\n",
      "Epoch 200, train loss: 0.13660039007663727, train acc: 0.9704814522494081\n",
      "Accuracy for each output state:\n",
      "0.9755154639175259\n",
      "Value 's_m_h10_val11: 0.9755154639175259' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08925466239452362, train acc: 0.9793785310734463\n",
      "Epoch 50, train loss: 0.08925466239452362, train acc: 0.9793785310734463\n",
      "Epoch 100, train loss: 0.08925466239452362, train acc: 0.9793785310734463\n",
      "Epoch 150, train loss: 0.08925466239452362, train acc: 0.9793785310734463\n",
      "Epoch 200, train loss: 0.08925466239452362, train acc: 0.9793785310734463\n",
      "Accuracy for each output state:\n",
      "0.9841269841269841\n",
      "Value 's_m_h20_val11: 0.9841269841269841' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07253892719745636, train acc: 0.9810074318744839\n",
      "Epoch 50, train loss: 0.07253892719745636, train acc: 0.9810074318744839\n",
      "Epoch 100, train loss: 0.07253892719745636, train acc: 0.9810074318744839\n",
      "Epoch 150, train loss: 0.07253892719745636, train acc: 0.9810074318744839\n",
      "Epoch 200, train loss: 0.07253892719745636, train acc: 0.9810074318744839\n",
      "Accuracy for each output state:\n",
      "0.9802989130434783\n",
      "Value 's_m_h30_val11: 0.9802989130434783' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0823541209101677, train acc: 0.9740912933220626\n",
      "Epoch 50, train loss: 0.0823541209101677, train acc: 0.9740912933220626\n",
      "Epoch 100, train loss: 0.0823541209101677, train acc: 0.9740912933220626\n",
      "Epoch 150, train loss: 0.0823541209101677, train acc: 0.9740912933220626\n",
      "Epoch 200, train loss: 0.0823541209101677, train acc: 0.9740912933220626\n",
      "Accuracy for each output state:\n",
      "0.9657821229050279\n",
      "Value 's_m_h40_val11: 0.9657821229050279' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10691872239112854, train acc: 0.9626839826839827\n",
      "Epoch 50, train loss: 0.10691872239112854, train acc: 0.9626839826839827\n",
      "Epoch 100, train loss: 0.10691872239112854, train acc: 0.9626839826839827\n",
      "Epoch 150, train loss: 0.10691872239112854, train acc: 0.9626839826839827\n",
      "Epoch 200, train loss: 0.10691872239112854, train acc: 0.9626839826839827\n",
      "Accuracy for each output state:\n",
      "0.944683908045977\n",
      "Value 's_m_h50_val11: 0.944683908045977' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14319588243961334, train acc: 0.9492014196983141\n",
      "Epoch 50, train loss: 0.14319588243961334, train acc: 0.9492014196983141\n",
      "Epoch 100, train loss: 0.14319588243961334, train acc: 0.9492014196983141\n",
      "Epoch 150, train loss: 0.14319588243961334, train acc: 0.9492014196983141\n",
      "Epoch 200, train loss: 0.14319588243961334, train acc: 0.9492014196983141\n",
      "Accuracy for each output state:\n",
      "0.9267751479289941\n",
      "Value 's_m_h60_val11: 0.9267751479289941' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18945302069187164, train acc: 0.935304822565969\n",
      "Epoch 50, train loss: 0.18945302069187164, train acc: 0.935304822565969\n",
      "Epoch 100, train loss: 0.18945302069187164, train acc: 0.935304822565969\n",
      "Epoch 150, train loss: 0.18945302069187164, train acc: 0.935304822565969\n",
      "Epoch 200, train loss: 0.18945302069187164, train acc: 0.935304822565969\n",
      "Accuracy for each output state:\n",
      "0.9108231707317073\n",
      "Value 's_m_h70_val11: 0.9108231707317073' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24393245577812195, train acc: 0.9213818860877684\n",
      "Epoch 50, train loss: 0.24393245577812195, train acc: 0.9213818860877684\n",
      "Epoch 100, train loss: 0.24393245577812195, train acc: 0.9213818860877684\n",
      "Epoch 150, train loss: 0.24393245577812195, train acc: 0.9213818860877684\n",
      "Epoch 200, train loss: 0.24393245577812195, train acc: 0.9213818860877684\n",
      "Accuracy for each output state:\n",
      "0.8962264150943398\n",
      "Value 's_m_h80_val11: 0.8962264150943398' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3069097399711609, train acc: 0.9075743048897411\n",
      "Epoch 50, train loss: 0.3069097399711609, train acc: 0.9075743048897411\n",
      "Epoch 100, train loss: 0.3069097399711609, train acc: 0.9075743048897411\n",
      "Epoch 150, train loss: 0.3069097399711609, train acc: 0.9075743048897411\n",
      "Epoch 200, train loss: 0.3069097399711609, train acc: 0.9075743048897411\n",
      "Accuracy for each output state:\n",
      "0.8790584415584415\n",
      "Value 's_m_h90_val11: 0.8790584415584415' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.37396639585494995, train acc: 0.893743842364532\n",
      "Epoch 50, train loss: 0.37396639585494995, train acc: 0.893743842364532\n",
      "Epoch 100, train loss: 0.37396639585494995, train acc: 0.893743842364532\n",
      "Epoch 150, train loss: 0.37396639585494995, train acc: 0.893743842364532\n",
      "Epoch 200, train loss: 0.37396639585494995, train acc: 0.893743842364532\n",
      "Accuracy for each output state:\n",
      "0.8607382550335569\n",
      "Value 's_m_h100_val11: 0.8607382550335569' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4380970597267151, train acc: 0.8805471124620061\n",
      "Epoch 50, train loss: 0.4380970597267151, train acc: 0.8805471124620061\n",
      "Epoch 100, train loss: 0.4380970597267151, train acc: 0.8805471124620061\n",
      "Epoch 150, train loss: 0.4380970597267151, train acc: 0.8805471124620061\n",
      "Epoch 200, train loss: 0.4380970597267151, train acc: 0.8805471124620061\n",
      "Accuracy for each output state:\n",
      "0.8402777777777778\n",
      "Value 's_m_h110_val11: 0.8402777777777778' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.49848195910453796, train acc: 0.8674139728884255\n",
      "Epoch 50, train loss: 0.49848195910453796, train acc: 0.8674139728884255\n",
      "Epoch 100, train loss: 0.49848195910453796, train acc: 0.8674139728884255\n",
      "Epoch 150, train loss: 0.49848195910453796, train acc: 0.8674139728884255\n",
      "Epoch 200, train loss: 0.49848195910453796, train acc: 0.8674139728884255\n",
      "Accuracy for each output state:\n",
      "0.8246402877697843\n",
      "Value 's_m_h120_val11: 0.8246402877697843' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5582365393638611, train acc: 0.8541353383458646\n",
      "Epoch 50, train loss: 0.5582365393638611, train acc: 0.8541353383458646\n",
      "Epoch 100, train loss: 0.5582365393638611, train acc: 0.8541353383458646\n",
      "Epoch 150, train loss: 0.5582365393638611, train acc: 0.8541353383458646\n",
      "Epoch 200, train loss: 0.5582365393638611, train acc: 0.8541353383458646\n",
      "Accuracy for each output state:\n",
      "0.8106343283582089\n",
      "Value 's_m_h130_val11: 0.8106343283582089' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6121494770050049, train acc: 0.8426910299003322\n",
      "Epoch 50, train loss: 0.6121494770050049, train acc: 0.8426910299003322\n",
      "Epoch 100, train loss: 0.6121494770050049, train acc: 0.8426910299003322\n",
      "Epoch 150, train loss: 0.6121494770050049, train acc: 0.8426910299003322\n",
      "Epoch 200, train loss: 0.6121494770050049, train acc: 0.8426910299003322\n",
      "Accuracy for each output state:\n",
      "0.7926356589147288\n",
      "Value 's_m_h140_val11: 0.7926356589147288' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6590017080307007, train acc: 0.8313142857142857\n",
      "Epoch 50, train loss: 0.6590017080307007, train acc: 0.8313142857142857\n",
      "Epoch 100, train loss: 0.6590017080307007, train acc: 0.8313142857142857\n",
      "Epoch 150, train loss: 0.6590017080307007, train acc: 0.8313142857142857\n",
      "Epoch 200, train loss: 0.6590017080307007, train acc: 0.8313142857142857\n",
      "Accuracy for each output state:\n",
      "0.7741935483870968\n",
      "Value 's_m_h150_val11: 0.7741935483870968' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13676424324512482, train acc: 0.9707817640607864\n",
      "Epoch 50, train loss: 0.13676424324512482, train acc: 0.9707817640607864\n",
      "Epoch 100, train loss: 0.13676424324512482, train acc: 0.9707817640607864\n",
      "Epoch 150, train loss: 0.13676424324512482, train acc: 0.9707817640607864\n",
      "Epoch 200, train loss: 0.13676424324512482, train acc: 0.9707817640607864\n",
      "Accuracy for each output state:\n",
      "0.9669117647058825\n",
      "Value 's_m_h10_val12: 0.9669117647058825' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0877641960978508, train acc: 0.9805382027871216\n",
      "Epoch 50, train loss: 0.0877641960978508, train acc: 0.9805382027871216\n",
      "Epoch 100, train loss: 0.0877641960978508, train acc: 0.9805382027871216\n",
      "Epoch 150, train loss: 0.0877641960978508, train acc: 0.9805382027871216\n",
      "Epoch 200, train loss: 0.0877641960978508, train acc: 0.9805382027871216\n",
      "Accuracy for each output state:\n",
      "0.9727272727272728\n",
      "Value 's_m_h20_val12: 0.9727272727272728' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07041414827108383, train acc: 0.9821808946419793\n",
      "Epoch 50, train loss: 0.07041414827108383, train acc: 0.9821808946419793\n",
      "Epoch 100, train loss: 0.07041414827108383, train acc: 0.9821808946419793\n",
      "Epoch 150, train loss: 0.07041414827108383, train acc: 0.9821808946419793\n",
      "Epoch 200, train loss: 0.07041414827108383, train acc: 0.9821808946419793\n",
      "Accuracy for each output state:\n",
      "0.96875\n",
      "Value 's_m_h30_val12: 0.96875' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07972244918346405, train acc: 0.9755156800268321\n",
      "Epoch 50, train loss: 0.07972244918346405, train acc: 0.9755156800268321\n",
      "Epoch 100, train loss: 0.07972244918346405, train acc: 0.9755156800268321\n",
      "Epoch 150, train loss: 0.07972244918346405, train acc: 0.9755156800268321\n",
      "Epoch 200, train loss: 0.07972244918346405, train acc: 0.9755156800268321\n",
      "Accuracy for each output state:\n",
      "0.95\n",
      "Value 's_m_h40_val12: 0.95' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10533110052347183, train acc: 0.9641078481882192\n",
      "Epoch 50, train loss: 0.10533110052347183, train acc: 0.9641078481882192\n",
      "Epoch 100, train loss: 0.10533110052347183, train acc: 0.9641078481882192\n",
      "Epoch 150, train loss: 0.10533110052347183, train acc: 0.9641078481882192\n",
      "Epoch 200, train loss: 0.10533110052347183, train acc: 0.9641078481882192\n",
      "Accuracy for each output state:\n",
      "0.9316666666666668\n",
      "Value 's_m_h50_val12: 0.9316666666666668' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1434670239686966, train acc: 0.9508182298081999\n",
      "Epoch 50, train loss: 0.1434670239686966, train acc: 0.9508182298081999\n",
      "Epoch 100, train loss: 0.1434670239686966, train acc: 0.9508182298081999\n",
      "Epoch 150, train loss: 0.1434670239686966, train acc: 0.9508182298081999\n",
      "Epoch 200, train loss: 0.1434670239686966, train acc: 0.9508182298081999\n",
      "Accuracy for each output state:\n",
      "0.9129310344827586\n",
      "Value 's_m_h60_val12: 0.9129310344827586' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1894688904285431, train acc: 0.9371279090745084\n",
      "Epoch 50, train loss: 0.1894688904285431, train acc: 0.9371279090745084\n",
      "Epoch 100, train loss: 0.1894688904285431, train acc: 0.9371279090745084\n",
      "Epoch 150, train loss: 0.1894688904285431, train acc: 0.9371279090745084\n",
      "Epoch 200, train loss: 0.1894688904285431, train acc: 0.9371279090745084\n",
      "Accuracy for each output state:\n",
      "0.9\n",
      "Value 's_m_h70_val12: 0.9' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.2424667924642563, train acc: 0.9231445493244493\n",
      "Epoch 50, train loss: 0.2424667924642563, train acc: 0.9231445493244493\n",
      "Epoch 100, train loss: 0.2424667924642563, train acc: 0.9231445493244493\n",
      "Epoch 150, train loss: 0.2424667924642563, train acc: 0.9231445493244493\n",
      "Epoch 200, train loss: 0.2424667924642563, train acc: 0.9231445493244493\n",
      "Accuracy for each output state:\n",
      "0.8805555555555555\n",
      "Value 's_m_h80_val12: 0.8805555555555555' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3042677342891693, train acc: 0.9087972639179175\n",
      "Epoch 50, train loss: 0.3042677342891693, train acc: 0.9087972639179175\n",
      "Epoch 100, train loss: 0.3042677342891693, train acc: 0.9087972639179175\n",
      "Epoch 150, train loss: 0.3042677342891693, train acc: 0.9087972639179175\n",
      "Epoch 200, train loss: 0.3042677342891693, train acc: 0.9087972639179175\n",
      "Accuracy for each output state:\n",
      "0.8576923076923078\n",
      "Value 's_m_h90_val12: 0.8576923076923078' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.37071871757507324, train acc: 0.8944954128440367\n",
      "Epoch 50, train loss: 0.37071871757507324, train acc: 0.8944954128440367\n",
      "Epoch 100, train loss: 0.37071871757507324, train acc: 0.8944954128440367\n",
      "Epoch 150, train loss: 0.37071871757507324, train acc: 0.8944954128440367\n",
      "Epoch 200, train loss: 0.37071871757507324, train acc: 0.8944954128440367\n",
      "Accuracy for each output state:\n",
      "0.833\n",
      "Value 's_m_h100_val12: 0.833' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.43427351117134094, train acc: 0.8814970901063617\n",
      "Epoch 50, train loss: 0.43427351117134094, train acc: 0.8814970901063617\n",
      "Epoch 100, train loss: 0.43427351117134094, train acc: 0.8814970901063617\n",
      "Epoch 150, train loss: 0.43427351117134094, train acc: 0.8814970901063617\n",
      "Epoch 200, train loss: 0.43427351117134094, train acc: 0.8814970901063617\n",
      "Accuracy for each output state:\n",
      "0.8083333333333333\n",
      "Value 's_m_h110_val12: 0.8083333333333333' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4949175715446472, train acc: 0.8691926491843899\n",
      "Epoch 50, train loss: 0.4949175715446472, train acc: 0.8691926491843899\n",
      "Epoch 100, train loss: 0.4949175715446472, train acc: 0.8691926491843899\n",
      "Epoch 150, train loss: 0.4949175715446472, train acc: 0.8691926491843899\n",
      "Epoch 200, train loss: 0.4949175715446472, train acc: 0.8691926491843899\n",
      "Accuracy for each output state:\n",
      "0.7782608695652173\n",
      "Value 's_m_h120_val12: 0.7782608695652173' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5542353987693787, train acc: 0.8566340633638103\n",
      "Epoch 50, train loss: 0.5542353987693787, train acc: 0.8566340633638103\n",
      "Epoch 100, train loss: 0.5542353987693787, train acc: 0.8566340633638103\n",
      "Epoch 150, train loss: 0.5542353987693787, train acc: 0.8566340633638103\n",
      "Epoch 200, train loss: 0.5542353987693787, train acc: 0.8566340633638103\n",
      "Accuracy for each output state:\n",
      "0.7454545454545455\n",
      "Value 's_m_h130_val12: 0.7454545454545455' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6089351773262024, train acc: 0.8446745562130178\n",
      "Epoch 50, train loss: 0.6089351773262024, train acc: 0.8446745562130178\n",
      "Epoch 100, train loss: 0.6089351773262024, train acc: 0.8446745562130178\n",
      "Epoch 150, train loss: 0.6089351773262024, train acc: 0.8446745562130178\n",
      "Epoch 200, train loss: 0.6089351773262024, train acc: 0.8446745562130178\n",
      "Accuracy for each output state:\n",
      "0.7142857142857143\n",
      "Value 's_m_h140_val12: 0.7142857142857143' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6589521169662476, train acc: 0.8334275378702238\n",
      "Epoch 50, train loss: 0.6589521169662476, train acc: 0.8334275378702238\n",
      "Epoch 100, train loss: 0.6589521169662476, train acc: 0.8334275378702238\n",
      "Epoch 150, train loss: 0.6589521169662476, train acc: 0.8334275378702238\n",
      "Epoch 200, train loss: 0.6589521169662476, train acc: 0.8334275378702238\n",
      "Accuracy for each output state:\n",
      "0.68875\n",
      "Value 's_m_h150_val12: 0.68875' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.13406828045845032, train acc: 0.9712073073868149\n",
      "Epoch 50, train loss: 0.13406828045845032, train acc: 0.9712073073868149\n",
      "Epoch 100, train loss: 0.13406828045845032, train acc: 0.9712073073868149\n",
      "Epoch 150, train loss: 0.13406828045845032, train acc: 0.9712073073868149\n",
      "Epoch 200, train loss: 0.13406828045845032, train acc: 0.9712073073868149\n",
      "Accuracy for each output state:\n",
      "0.9643691588785047\n",
      "Value 's_m_h10_val13: 0.9643691588785047' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08827846497297287, train acc: 0.9793257514216085\n",
      "Epoch 50, train loss: 0.08827846497297287, train acc: 0.9793257514216085\n",
      "Epoch 100, train loss: 0.08827846497297287, train acc: 0.9793257514216085\n",
      "Epoch 150, train loss: 0.08827846497297287, train acc: 0.9793257514216085\n",
      "Epoch 200, train loss: 0.08827846497297287, train acc: 0.9793257514216085\n",
      "Accuracy for each output state:\n",
      "0.9808612440191389\n",
      "Value 's_m_h20_val13: 0.9808612440191389' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0729321613907814, train acc: 0.9806733167082294\n",
      "Epoch 50, train loss: 0.0729321613907814, train acc: 0.9806733167082294\n",
      "Epoch 100, train loss: 0.0729321613907814, train acc: 0.9806733167082294\n",
      "Epoch 150, train loss: 0.0729321613907814, train acc: 0.9806733167082294\n",
      "Epoch 200, train loss: 0.0729321613907814, train acc: 0.9806733167082294\n",
      "Accuracy for each output state:\n",
      "0.9859068627450981\n",
      "Value 's_m_h30_val13: 0.9859068627450981' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.0842706486582756, train acc: 0.9733191489361702\n",
      "Epoch 50, train loss: 0.0842706486582756, train acc: 0.9733191489361702\n",
      "Epoch 100, train loss: 0.0842706486582756, train acc: 0.9733191489361702\n",
      "Epoch 150, train loss: 0.0842706486582756, train acc: 0.9733191489361702\n",
      "Epoch 200, train loss: 0.0842706486582756, train acc: 0.9733191489361702\n",
      "Accuracy for each output state:\n",
      "0.9767587939698492\n",
      "Value 's_m_h40_val13: 0.9767587939698492' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.11020389944314957, train acc: 0.9613775065387968\n",
      "Epoch 50, train loss: 0.11020389944314957, train acc: 0.9613775065387968\n",
      "Epoch 100, train loss: 0.11020389944314957, train acc: 0.9613775065387968\n",
      "Epoch 150, train loss: 0.11020389944314957, train acc: 0.9613775065387968\n",
      "Epoch 200, train loss: 0.11020389944314957, train acc: 0.9613775065387968\n",
      "Accuracy for each output state:\n",
      "0.9639175257731959\n",
      "Value 's_m_h50_val13: 0.9639175257731959' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14821191132068634, train acc: 0.9479445933869526\n",
      "Epoch 50, train loss: 0.14821191132068634, train acc: 0.9479445933869526\n",
      "Epoch 100, train loss: 0.14821191132068634, train acc: 0.9479445933869526\n",
      "Epoch 150, train loss: 0.14821191132068634, train acc: 0.9479445933869526\n",
      "Epoch 200, train loss: 0.14821191132068634, train acc: 0.9479445933869526\n",
      "Accuracy for each output state:\n",
      "0.9437830687830687\n",
      "Value 's_m_h60_val13: 0.9437830687830687' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19403505325317383, train acc: 0.9346012832263978\n",
      "Epoch 50, train loss: 0.19403505325317383, train acc: 0.9346012832263978\n",
      "Epoch 100, train loss: 0.19403505325317383, train acc: 0.9346012832263978\n",
      "Epoch 150, train loss: 0.19403505325317383, train acc: 0.9346012832263978\n",
      "Epoch 200, train loss: 0.19403505325317383, train acc: 0.9346012832263978\n",
      "Accuracy for each output state:\n",
      "0.923913043478261\n",
      "Value 's_m_h70_val13: 0.923913043478261' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.24656017124652863, train acc: 0.9216839134524929\n",
      "Epoch 50, train loss: 0.24656017124652863, train acc: 0.9216839134524929\n",
      "Epoch 100, train loss: 0.24656017124652863, train acc: 0.9216839134524929\n",
      "Epoch 150, train loss: 0.24656017124652863, train acc: 0.9216839134524929\n",
      "Epoch 200, train loss: 0.24656017124652863, train acc: 0.9216839134524929\n",
      "Accuracy for each output state:\n",
      "0.9036312849162011\n",
      "Value 's_m_h80_val13: 0.9036312849162011' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3063851296901703, train acc: 0.9081159420289855\n",
      "Epoch 50, train loss: 0.3063851296901703, train acc: 0.9081159420289855\n",
      "Epoch 100, train loss: 0.3063851296901703, train acc: 0.9081159420289855\n",
      "Epoch 150, train loss: 0.3063851296901703, train acc: 0.9081159420289855\n",
      "Epoch 200, train loss: 0.3063851296901703, train acc: 0.9081159420289855\n",
      "Accuracy for each output state:\n",
      "0.8829022988505746\n",
      "Value 's_m_h90_val13: 0.8829022988505746' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.37029919028282166, train acc: 0.894240317775571\n",
      "Epoch 50, train loss: 0.37029919028282166, train acc: 0.894240317775571\n",
      "Epoch 100, train loss: 0.37029919028282166, train acc: 0.894240317775571\n",
      "Epoch 150, train loss: 0.37029919028282166, train acc: 0.894240317775571\n",
      "Epoch 200, train loss: 0.37029919028282166, train acc: 0.894240317775571\n",
      "Accuracy for each output state:\n",
      "0.8609467455621301\n",
      "Value 's_m_h100_val13: 0.8609467455621301' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4331130385398865, train acc: 0.8806945863125638\n",
      "Epoch 50, train loss: 0.4331130385398865, train acc: 0.8806945863125638\n",
      "Epoch 100, train loss: 0.4331130385398865, train acc: 0.8806945863125638\n",
      "Epoch 150, train loss: 0.4331130385398865, train acc: 0.8806945863125638\n",
      "Epoch 200, train loss: 0.4331130385398865, train acc: 0.8806945863125638\n",
      "Accuracy for each output state:\n",
      "0.8452743902439025\n",
      "Value 's_m_h110_val13: 0.8452743902439025' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.49551957845687866, train acc: 0.8678233438485804\n",
      "Epoch 50, train loss: 0.49551957845687866, train acc: 0.8678233438485804\n",
      "Epoch 100, train loss: 0.49551957845687866, train acc: 0.8678233438485804\n",
      "Epoch 150, train loss: 0.49551957845687866, train acc: 0.8678233438485804\n",
      "Epoch 200, train loss: 0.49551957845687866, train acc: 0.8678233438485804\n",
      "Accuracy for each output state:\n",
      "0.8325471698113207\n",
      "Value 's_m_h120_val13: 0.8325471698113207' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5579556226730347, train acc: 0.8547670639219935\n",
      "Epoch 50, train loss: 0.5579556226730347, train acc: 0.8547670639219935\n",
      "Epoch 100, train loss: 0.5579556226730347, train acc: 0.8547670639219935\n",
      "Epoch 150, train loss: 0.5579556226730347, train acc: 0.8547670639219935\n",
      "Epoch 200, train loss: 0.5579556226730347, train acc: 0.8547670639219935\n",
      "Accuracy for each output state:\n",
      "0.8157467532467533\n",
      "Value 's_m_h130_val13: 0.8157467532467533' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6161829233169556, train acc: 0.8416759776536313\n",
      "Epoch 50, train loss: 0.6161829233169556, train acc: 0.8416759776536313\n",
      "Epoch 100, train loss: 0.6161829233169556, train acc: 0.8416759776536313\n",
      "Epoch 150, train loss: 0.6161829233169556, train acc: 0.8416759776536313\n",
      "Epoch 200, train loss: 0.6161829233169556, train acc: 0.8416759776536313\n",
      "Accuracy for each output state:\n",
      "0.8053691275167785\n",
      "Value 's_m_h140_val13: 0.8053691275167785' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6668766140937805, train acc: 0.8299307958477509\n",
      "Epoch 50, train loss: 0.6668766140937805, train acc: 0.8299307958477509\n",
      "Epoch 100, train loss: 0.6668766140937805, train acc: 0.8299307958477509\n",
      "Epoch 150, train loss: 0.6668766140937805, train acc: 0.8299307958477509\n",
      "Epoch 200, train loss: 0.6668766140937805, train acc: 0.8299307958477509\n",
      "Accuracy for each output state:\n",
      "0.7899305555555556\n",
      "Value 's_m_h150_val13: 0.7899305555555556' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.133048415184021, train acc: 0.9705008829667684\n",
      "Epoch 50, train loss: 0.133048415184021, train acc: 0.9705008829667684\n",
      "Epoch 100, train loss: 0.133048415184021, train acc: 0.9705008829667684\n",
      "Epoch 150, train loss: 0.133048415184021, train acc: 0.9705008829667684\n",
      "Epoch 200, train loss: 0.133048415184021, train acc: 0.9705008829667684\n",
      "Accuracy for each output state:\n",
      "0.9731781376518219\n",
      "Value 's_m_h10_val14: 0.9731781376518219' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08789630979299545, train acc: 0.9790195434389883\n",
      "Epoch 50, train loss: 0.08789630979299545, train acc: 0.9790195434389883\n",
      "Epoch 100, train loss: 0.08789630979299545, train acc: 0.9790195434389883\n",
      "Epoch 150, train loss: 0.08789630979299545, train acc: 0.9790195434389883\n",
      "Epoch 200, train loss: 0.08789630979299545, train acc: 0.9790195434389883\n",
      "Accuracy for each output state:\n",
      "0.9870867768595042\n",
      "Value 's_m_h20_val14: 0.9870867768595042' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.07382229715585709, train acc: 0.9800386619599932\n",
      "Epoch 50, train loss: 0.07382229715585709, train acc: 0.9800386619599932\n",
      "Epoch 100, train loss: 0.07382229715585709, train acc: 0.9800386619599932\n",
      "Epoch 150, train loss: 0.07382229715585709, train acc: 0.9800386619599932\n",
      "Epoch 200, train loss: 0.07382229715585709, train acc: 0.9800386619599932\n",
      "Accuracy for each output state:\n",
      "0.9915611814345991\n",
      "Value 's_m_h30_val14: 0.9915611814345991' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08468007296323776, train acc: 0.9720261662936822\n",
      "Epoch 50, train loss: 0.08468007296323776, train acc: 0.9720261662936822\n",
      "Epoch 100, train loss: 0.08468007296323776, train acc: 0.9720261662936822\n",
      "Epoch 150, train loss: 0.08468007296323776, train acc: 0.9720261662936822\n",
      "Epoch 200, train loss: 0.08468007296323776, train acc: 0.9720261662936822\n",
      "Accuracy for each output state:\n",
      "0.9832974137931034\n",
      "Value 's_m_h40_val14: 0.9832974137931034' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10946297645568848, train acc: 0.9606632563062268\n",
      "Epoch 50, train loss: 0.10946297645568848, train acc: 0.9606632563062268\n",
      "Epoch 100, train loss: 0.10946297645568848, train acc: 0.9606632563062268\n",
      "Epoch 150, train loss: 0.10946297645568848, train acc: 0.9606632563062268\n",
      "Epoch 200, train loss: 0.10946297645568848, train acc: 0.9606632563062268\n",
      "Accuracy for each output state:\n",
      "0.9664096916299559\n",
      "Value 's_m_h50_val14: 0.9664096916299559' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.14493557810783386, train acc: 0.9477301501175619\n",
      "Epoch 50, train loss: 0.14493557810783386, train acc: 0.9477301501175619\n",
      "Epoch 100, train loss: 0.14493557810783386, train acc: 0.9477301501175619\n",
      "Epoch 150, train loss: 0.14493557810783386, train acc: 0.9477301501175619\n",
      "Epoch 200, train loss: 0.14493557810783386, train acc: 0.9477301501175619\n",
      "Accuracy for each output state:\n",
      "0.9498873873873873\n",
      "Value 's_m_h60_val14: 0.9498873873873873' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.18780097365379333, train acc: 0.9345889775468547\n",
      "Epoch 50, train loss: 0.18780097365379333, train acc: 0.9345889775468547\n",
      "Epoch 100, train loss: 0.18780097365379333, train acc: 0.9345889775468547\n",
      "Epoch 150, train loss: 0.18780097365379333, train acc: 0.9345889775468547\n",
      "Epoch 200, train loss: 0.18780097365379333, train acc: 0.9345889775468547\n",
      "Accuracy for each output state:\n",
      "0.9337557603686636\n",
      "Value 's_m_h70_val14: 0.9337557603686636' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.23693782091140747, train acc: 0.9211754619927606\n",
      "Epoch 50, train loss: 0.23693782091140747, train acc: 0.9211754619927606\n",
      "Epoch 100, train loss: 0.23693782091140747, train acc: 0.9211754619927606\n",
      "Epoch 150, train loss: 0.23693782091140747, train acc: 0.9211754619927606\n",
      "Epoch 200, train loss: 0.23693782091140747, train acc: 0.9211754619927606\n",
      "Accuracy for each output state:\n",
      "0.9150943396226415\n",
      "Value 's_m_h80_val14: 0.9150943396226415' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.29443344473838806, train acc: 0.9079076140144843\n",
      "Epoch 50, train loss: 0.29443344473838806, train acc: 0.9079076140144843\n",
      "Epoch 100, train loss: 0.29443344473838806, train acc: 0.9079076140144843\n",
      "Epoch 150, train loss: 0.29443344473838806, train acc: 0.9079076140144843\n",
      "Epoch 200, train loss: 0.29443344473838806, train acc: 0.9079076140144843\n",
      "Accuracy for each output state:\n",
      "0.8967391304347825\n",
      "Value 's_m_h90_val14: 0.8967391304347825' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3561933636665344, train acc: 0.8952505534312739\n",
      "Epoch 50, train loss: 0.3561933636665344, train acc: 0.8952505534312739\n",
      "Epoch 100, train loss: 0.3561933636665344, train acc: 0.8952505534312739\n",
      "Epoch 150, train loss: 0.3561933636665344, train acc: 0.8952505534312739\n",
      "Epoch 200, train loss: 0.3561933636665344, train acc: 0.8952505534312739\n",
      "Accuracy for each output state:\n",
      "0.8756188118811881\n",
      "Value 's_m_h100_val14: 0.8756188118811881' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4173458218574524, train acc: 0.8828950093186996\n",
      "Epoch 50, train loss: 0.4173458218574524, train acc: 0.8828950093186996\n",
      "Epoch 100, train loss: 0.4173458218574524, train acc: 0.8828950093186996\n",
      "Epoch 150, train loss: 0.4173458218574524, train acc: 0.8828950093186996\n",
      "Epoch 200, train loss: 0.4173458218574524, train acc: 0.8828950093186996\n",
      "Accuracy for each output state:\n",
      "0.8591370558375635\n",
      "Value 's_m_h110_val14: 0.8591370558375635' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4772448241710663, train acc: 0.8717210492642354\n",
      "Epoch 50, train loss: 0.4772448241710663, train acc: 0.8717210492642354\n",
      "Epoch 100, train loss: 0.4772448241710663, train acc: 0.8717210492642354\n",
      "Epoch 150, train loss: 0.4772448241710663, train acc: 0.8717210492642354\n",
      "Epoch 200, train loss: 0.4772448241710663, train acc: 0.8717210492642354\n",
      "Accuracy for each output state:\n",
      "0.8437500000000001\n",
      "Value 's_m_h120_val14: 0.8437500000000001' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5362172722816467, train acc: 0.8595845240712244\n",
      "Epoch 50, train loss: 0.5362172722816467, train acc: 0.8595845240712244\n",
      "Epoch 100, train loss: 0.5362172722816467, train acc: 0.8595845240712244\n",
      "Epoch 150, train loss: 0.5362172722816467, train acc: 0.8595845240712244\n",
      "Epoch 200, train loss: 0.5362172722816467, train acc: 0.8595845240712244\n",
      "Accuracy for each output state:\n",
      "0.8308823529411765\n",
      "Value 's_m_h130_val14: 0.8308823529411765' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5911128520965576, train acc: 0.8466205488772964\n",
      "Epoch 50, train loss: 0.5911128520965576, train acc: 0.8466205488772964\n",
      "Epoch 100, train loss: 0.5911128520965576, train acc: 0.8466205488772964\n",
      "Epoch 150, train loss: 0.5911128520965576, train acc: 0.8466205488772964\n",
      "Epoch 200, train loss: 0.5911128520965576, train acc: 0.8466205488772964\n",
      "Accuracy for each output state:\n",
      "0.8214285714285714\n",
      "Value 's_m_h140_val14: 0.8214285714285714' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.63963782787323, train acc: 0.8337432654017334\n",
      "Epoch 50, train loss: 0.63963782787323, train acc: 0.8337432654017334\n",
      "Epoch 100, train loss: 0.63963782787323, train acc: 0.8337432654017334\n",
      "Epoch 150, train loss: 0.63963782787323, train acc: 0.8337432654017334\n",
      "Epoch 200, train loss: 0.63963782787323, train acc: 0.8337432654017334\n",
      "Accuracy for each output state:\n",
      "0.807909604519774\n",
      "Value 's_m_h150_val14: 0.807909604519774' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1291152536869049, train acc: 0.9716317599249765\n",
      "Epoch 50, train loss: 0.1291152536869049, train acc: 0.9716317599249765\n",
      "Epoch 100, train loss: 0.1291152536869049, train acc: 0.9716317599249765\n",
      "Epoch 150, train loss: 0.1291152536869049, train acc: 0.9716317599249765\n",
      "Epoch 200, train loss: 0.1291152536869049, train acc: 0.9716317599249765\n",
      "Accuracy for each output state:\n",
      "0.9815384615384615\n",
      "Value 's_m_h10_val15: 0.9815384615384615' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08292754739522934, train acc: 0.9808245445829339\n",
      "Epoch 50, train loss: 0.08292754739522934, train acc: 0.9808245445829339\n",
      "Epoch 100, train loss: 0.08292754739522934, train acc: 0.9808245445829339\n",
      "Epoch 150, train loss: 0.08292754739522934, train acc: 0.9808245445829339\n",
      "Epoch 200, train loss: 0.08292754739522934, train acc: 0.9808245445829339\n",
      "Accuracy for each output state:\n",
      "0.980952380952381\n",
      "Value 's_m_h20_val15: 0.980952380952381' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.06963218003511429, train acc: 0.9814073226544623\n",
      "Epoch 50, train loss: 0.06963218003511429, train acc: 0.9814073226544623\n",
      "Epoch 100, train loss: 0.06963218003511429, train acc: 0.9814073226544623\n",
      "Epoch 150, train loss: 0.06963218003511429, train acc: 0.9814073226544623\n",
      "Epoch 200, train loss: 0.06963218003511429, train acc: 0.9814073226544623\n",
      "Accuracy for each output state:\n",
      "0.9754098360655739\n",
      "Value 's_m_h30_val15: 0.9754098360655739' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.08247174322605133, train acc: 0.9731515557042489\n",
      "Epoch 50, train loss: 0.08247174322605133, train acc: 0.9731515557042489\n",
      "Epoch 100, train loss: 0.08247174322605133, train acc: 0.9731515557042489\n",
      "Epoch 150, train loss: 0.08247174322605133, train acc: 0.9731515557042489\n",
      "Epoch 200, train loss: 0.08247174322605133, train acc: 0.9731515557042489\n",
      "Accuracy for each output state:\n",
      "0.9576271186440678\n",
      "Value 's_m_h40_val15: 0.9576271186440678' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.10995224118232727, train acc: 0.9610739979445015\n",
      "Epoch 50, train loss: 0.10995224118232727, train acc: 0.9610739979445015\n",
      "Epoch 100, train loss: 0.10995224118232727, train acc: 0.9610739979445015\n",
      "Epoch 150, train loss: 0.10995224118232727, train acc: 0.9610739979445015\n",
      "Epoch 200, train loss: 0.10995224118232727, train acc: 0.9610739979445015\n",
      "Accuracy for each output state:\n",
      "0.9394736842105263\n",
      "Value 's_m_h50_val15: 0.9394736842105263' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.1492781788110733, train acc: 0.9477009477009477\n",
      "Epoch 50, train loss: 0.1492781788110733, train acc: 0.9477009477009477\n",
      "Epoch 100, train loss: 0.1492781788110733, train acc: 0.9477009477009477\n",
      "Epoch 150, train loss: 0.1492781788110733, train acc: 0.9477009477009477\n",
      "Epoch 200, train loss: 0.1492781788110733, train acc: 0.9477009477009477\n",
      "Accuracy for each output state:\n",
      "0.9190909090909091\n",
      "Value 's_m_h60_val15: 0.9190909090909091' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.19686953723430634, train acc: 0.9343288952860741\n",
      "Epoch 50, train loss: 0.19686953723430634, train acc: 0.9343288952860741\n",
      "Epoch 100, train loss: 0.19686953723430634, train acc: 0.9343288952860741\n",
      "Epoch 150, train loss: 0.19686953723430634, train acc: 0.9343288952860741\n",
      "Epoch 200, train loss: 0.19686953723430634, train acc: 0.9343288952860741\n",
      "Accuracy for each output state:\n",
      "0.9066037735849057\n",
      "Value 's_m_h70_val15: 0.9066037735849057' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.251828134059906, train acc: 0.9204042081949059\n",
      "Epoch 50, train loss: 0.251828134059906, train acc: 0.9204042081949059\n",
      "Epoch 100, train loss: 0.251828134059906, train acc: 0.9204042081949059\n",
      "Epoch 150, train loss: 0.251828134059906, train acc: 0.9204042081949059\n",
      "Epoch 200, train loss: 0.251828134059906, train acc: 0.9204042081949059\n",
      "Accuracy for each output state:\n",
      "0.8931372549019607\n",
      "Value 's_m_h80_val15: 0.8931372549019607' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3156958520412445, train acc: 0.9060250094732853\n",
      "Epoch 50, train loss: 0.3156958520412445, train acc: 0.9060250094732853\n",
      "Epoch 100, train loss: 0.3156958520412445, train acc: 0.9060250094732853\n",
      "Epoch 150, train loss: 0.3156958520412445, train acc: 0.9060250094732853\n",
      "Epoch 200, train loss: 0.3156958520412445, train acc: 0.9060250094732853\n",
      "Accuracy for each output state:\n",
      "0.8795918367346939\n",
      "Value 's_m_h90_val15: 0.8795918367346939' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.3836838901042938, train acc: 0.891932658622032\n",
      "Epoch 50, train loss: 0.3836838901042938, train acc: 0.891932658622032\n",
      "Epoch 100, train loss: 0.3836838901042938, train acc: 0.891932658622032\n",
      "Epoch 150, train loss: 0.3836838901042938, train acc: 0.891932658622032\n",
      "Epoch 200, train loss: 0.3836838901042938, train acc: 0.891932658622032\n",
      "Accuracy for each output state:\n",
      "0.8648936170212765\n",
      "Value 's_m_h100_val15: 0.8648936170212765' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.4478420317173004, train acc: 0.8786514605842337\n",
      "Epoch 50, train loss: 0.4478420317173004, train acc: 0.8786514605842337\n",
      "Epoch 100, train loss: 0.4478420317173004, train acc: 0.8786514605842337\n",
      "Epoch 150, train loss: 0.4478420317173004, train acc: 0.8786514605842337\n",
      "Epoch 200, train loss: 0.4478420317173004, train acc: 0.8786514605842337\n",
      "Accuracy for each output state:\n",
      "0.8477777777777777\n",
      "Value 's_m_h110_val15: 0.8477777777777777' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5083447694778442, train acc: 0.8655310827501029\n",
      "Epoch 50, train loss: 0.5083447694778442, train acc: 0.8655310827501029\n",
      "Epoch 100, train loss: 0.5083447694778442, train acc: 0.8655310827501029\n",
      "Epoch 150, train loss: 0.5083447694778442, train acc: 0.8655310827501029\n",
      "Epoch 200, train loss: 0.5083447694778442, train acc: 0.8655310827501029\n",
      "Accuracy for each output state:\n",
      "0.8302325581395349\n",
      "Value 's_m_h120_val15: 0.8302325581395349' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.5690275430679321, train acc: 0.8523738872403561\n",
      "Epoch 50, train loss: 0.5690275430679321, train acc: 0.8523738872403561\n",
      "Epoch 100, train loss: 0.5690275430679321, train acc: 0.8523738872403561\n",
      "Epoch 150, train loss: 0.5690275430679321, train acc: 0.8523738872403561\n",
      "Epoch 200, train loss: 0.5690275430679321, train acc: 0.8523738872403561\n",
      "Accuracy for each output state:\n",
      "0.8109756097560976\n",
      "Value 's_m_h130_val15: 0.8109756097560976' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6252935528755188, train acc: 0.8411424202708606\n",
      "Epoch 50, train loss: 0.6252935528755188, train acc: 0.8411424202708606\n",
      "Epoch 100, train loss: 0.6252935528755188, train acc: 0.8411424202708606\n",
      "Epoch 150, train loss: 0.6252935528755188, train acc: 0.8411424202708606\n",
      "Epoch 200, train loss: 0.6252935528755188, train acc: 0.8411424202708606\n",
      "Accuracy for each output state:\n",
      "0.7897435897435898\n",
      "Value 's_m_h140_val15: 0.7897435897435898' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n",
      "Epoch 0, train loss: 0.6744848489761353, train acc: 0.8297093285263633\n",
      "Epoch 50, train loss: 0.6744848489761353, train acc: 0.8297093285263633\n",
      "Epoch 100, train loss: 0.6744848489761353, train acc: 0.8297093285263633\n",
      "Epoch 150, train loss: 0.6744848489761353, train acc: 0.8297093285263633\n",
      "Epoch 200, train loss: 0.6744848489761353, train acc: 0.8297093285263633\n",
      "Accuracy for each output state:\n",
      "0.7648648648648648\n",
      "Value 's_m_h150_val15: 0.7648648648648648' saved to 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/smh_results.txt' successfully.\n"
     ]
    }
   ],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/overfitting/results/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
