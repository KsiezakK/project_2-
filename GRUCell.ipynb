{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random \n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as torch_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import d2l\n",
    "import time\n",
    "import traceback\n",
    "import fastprogress\n",
    "from torchmetrics.classification import BinaryAccuracy, Accuracy \n",
    "import torch.nn.init as init\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from itertools import repeat\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Set the device to the first available GPU\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    # If GPU is not available, use the CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, bidirectional=False):\n",
    "        super(GRUNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)\n",
    "        #Xavier initialization for GRU weights\n",
    "        #for name, param in self.gru.named_parameters():\n",
    "        #    if 'weight' in name:\n",
    "        #        init.xavier_uniform_(param.data)\n",
    "        #    elif 'bias' in name:\n",
    "        #        init.constant_(param.data, 0.0)\n",
    "                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sotfplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        out, h = self.gru(x, h)\n",
    "        out = self.fc(self.sotfplus(out[:,-1]))\n",
    "        #out = self.fc(self.relu(out[:,-1]))\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out, h\n",
    "    \n",
    "    #def init_hidden(self, batch_size):\n",
    "        #weight = next(self.parameters()).data\n",
    "        #hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        #return hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        if batch_size > 1:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        else:\n",
    "            weight = next(self.parameters()).data\n",
    "            hidden = weight.new(self.n_layers, self.hidden_dim).zero_().to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None, hidden_states=None):\n",
    "        if h is None:\n",
    "            h = [torch.zeros(x.size(0), self.hidden_dim) for _ in range(self.num_layers)]\n",
    "\n",
    "        hidden_states = []  # Initialize a list to store hidden states at each time step\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            new_hidden_states = []\n",
    "\n",
    "            for layer_idx, gru_cell in enumerate(self.gru_cells):\n",
    "                h[layer_idx] = gru_cell(input_t, h[layer_idx])\n",
    "                new_hidden_states.append(h[layer_idx])\n",
    "                input_t = h[layer_idx]\n",
    "\n",
    "            hidden_states.append(new_hidden_states)  # Append hidden states for the current time step\n",
    "\n",
    "        last_hidden_states = [layer_states[-1] for layer_states in hidden_states]\n",
    "        last_hidden_states[-1] = self.batch_norm(last_hidden_states[-1])\n",
    "        out = self.fc(self.softplus(last_hidden_states[-1]))\n",
    "\n",
    "        # Return the output and the list of hidden states for the entire sequence\n",
    "        return out, last_hidden_states, hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for loading data\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Implement data retrieval for each index\n",
    "        input_data = self.X[idx]\n",
    "        target_data = self.y[idx]\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        \n",
    "        # Convert data to torch tensors if required\n",
    "        input_tensor = torch.Tensor(input_data)\n",
    "        target_tensor = torch.Tensor(target_data)\n",
    "        \n",
    "        return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(correct, total):\n",
    "    return float(correct)/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states_batch = []  # Initialize a list to store hidden states for the current batch        \n",
    "        out, last_hidden_states = model(x)\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "        #hidden_states.append(hidden)\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        # Append the batch's hidden states to the list\n",
    "        for layer_state in last_hidden_states:\n",
    "            hidden_states_batch.append(layer_state.cpu().detach().numpy())\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        # Append the batch's hidden states to the overall hidden_states list\n",
    "        hidden_states.append(hidden_states_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step(loss)\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    #hidden_states = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states = model(x)\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            #hidden_states.append(hidden)\n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    #train_hidden_states, val_hidden_states = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        #train_hidden_states.extend(train_hidden)\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            #val_hidden_states.extend(val_hidden)\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final\n",
    "    else: \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            \n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "        # Filter the DataFrame for the current trial\n",
    "        trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "        # Create shifted columns for each column in columns_to_shift\n",
    "        for col in columns_to_shift:\n",
    "            new_col_name = col + '_minus_' + str(shift)\n",
    "            trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "        # Drop the last 'i' records for each trial\n",
    "        trial_df = trial_df.dropna()\n",
    "\n",
    "        # Append the modified trial_df to the shifted_df\n",
    "        shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "    \n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "# split data into x and y \n",
    "X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "\n",
    "# reset index \n",
    "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "# Create custom datasets for training, validation, and testing\n",
    "full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['H_trial','H_id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['H_trial','H_id','s_1','s_2','s_3','s_4']] = df[['H_trial','H_id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['H_trial','H_id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,152,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['H_trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['H_trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['H_id', 'H_trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['H_trial']!=set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        test_set = shifted_df[shifted_df['H_trial']==set_values].drop(columns=['H_id', 'H_trial'])\n",
    "        full_set = shifted_df.drop(columns=['H_id', 'H_trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = ['s_1','s_2','s_3','s_4']\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 4\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=10, verbose=True, threshold=0.01, threshold_mode='rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 's_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'sm_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished smh, left set 14,15 to train \n",
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(10,101,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "      \n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "        \n",
    "        #shuffling \n",
    "        y_train = y_train.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y_test = y_test.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "        y = y.sample(frac=1, random_state=10).reset_index(drop=True)\n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "\n",
    "\n",
    "        train_losses, train_accs, val_losses, val_accs, train_predicted_labels, train_probs_final = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "\n",
    "\n",
    "\n",
    "        state_dict = model.state_dict()\n",
    "\n",
    "        # Specify the folder path and the model filename\n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models_shuffled' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        # Save the model to the specified folder\n",
    "        torch.save(state_dict, full_model)\n",
    "\n",
    "\n",
    "        \n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_test.columns, index=y_test.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_test).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "            \n",
    " # Iterate through the test data batches\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        test_accs = np.mean(accuracies)\n",
    "        \n",
    "        \n",
    "        file_name = 'smh_results_train.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/test/shuffled/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\": \"+str(test_accs)  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "# List of column names to drop\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "df.rename(columns={'H_id': 'id', 'H_trial': 'trial'}, inplace=True)\n",
    "data_to_scale = df.drop(columns=['trial','id','s_1','s_2','s_3','s_4'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['trial','id','s_1','s_2','s_3','s_4']] = df[['trial','id','s_1','s_2','s_3','s_4']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['trial','id']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 11\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id','trial'])\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sh_overfitting_check.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = df.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "shuffled_df = pd.concat(shuffled_trial_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shift = 2\n",
    "set_values = 5\n",
    "model_name = 's_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the shifted data\n",
    "shifted_df = pd.DataFrame()\n",
    "\n",
    "# Loop through unique trial values\n",
    "for trial_value in scaled_df['trial'].unique():\n",
    "    # Filter the DataFrame for the current trial\n",
    "    trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "    # Create shifted columns for each column in columns_to_shift\n",
    "    for col in columns_to_shift:\n",
    "        new_col_name = col + '_minus_' + str(shift)\n",
    "        trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "    # Drop the last 'i' records for each trial\n",
    "    trial_df = trial_df.dropna()\n",
    "\n",
    "    # Append the modified trial_df to the shifted_df\n",
    "    shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "\n",
    "#selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "selected_columns = ['id', 'trial', 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "shifted_df = shifted_df[selected_columns]\n",
    "# Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame 'train_set' with a 'trial' column\n",
    "# and you want to shuffle the trials while preserving the order of samples within each trial.\n",
    "\n",
    "# First, group your DataFrame by the 'trial' column\n",
    "trial_groups = train_set.groupby('trial')\n",
    "\n",
    "# Get a list of trial group names (trial IDs)\n",
    "trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "# Shuffle the trial IDs randomly\n",
    "random.shuffle(trial_ids)\n",
    "\n",
    "# Create an empty list to store shuffled trial DataFrames\n",
    "shuffled_trial_dfs = []\n",
    "\n",
    "# Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "for trial_id in trial_ids:\n",
    "    shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "# Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "train_set = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "print(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =151\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(1,142,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "'s_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                        # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "        \n",
    "        X_train = X_train.drop(columns=['id','trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        file_name = 'sm_shuffled.txt'\n",
    "        file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(columns_to_shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(1,16):\n",
    "    for i in np.arange(0,151,step=10):\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m_h'+str(shift)+'_'+'val'+str(set_values)\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4','H_headFront_x_minus_'+str(shift), 'H_headFront_y_minus_'+str(shift), \n",
    "        'H_headFront_z_minus_'+str(shift), 'H_neck_x_minus_'+str(shift),'H_neck_y_minus_'+str(shift), 'H_neck_z_minus_'+str(shift), 'H_lowerBack_x_minus_'+str(shift), 'H_lowerBack_y_minus_'+str(shift),'H_lowerBack_z_minus_'+str(shift),\n",
    "        'H_leftWrist_x_minus_'+str(shift), 'H_leftWrist_y_minus_'+str(shift), 'H_leftWrist_z_minus_'+str(shift),'H_leftShoulder_x_minus_'+str(shift), 'H_leftShoulder_y_minus_'+str(shift), 'H_leftShoulder_z_minus_'+str(shift),\n",
    "       'H_leftElbow_x_minus_'+str(shift), 'H_leftElbow_y_minus_'+str(shift), 'H_leftElbow_z_minus_'+str(shift)]\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values]\n",
    "        val_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "        \n",
    "        \n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_val, y_val = val_set.drop(columns=labels), val_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "                # First, group your DataFrame by the 'trial' column\n",
    "        trial_groups = X_train.groupby('trial')\n",
    "\n",
    "        # Get a list of trial group names (trial IDs)\n",
    "        trial_ids = list(trial_groups.groups.keys())\n",
    "\n",
    "        # Shuffle the trial IDs randomly\n",
    "        random.shuffle(trial_ids)\n",
    "\n",
    "        # Create an empty list to store shuffled trial DataFrames\n",
    "        shuffled_trial_dfs = []\n",
    "\n",
    "        # Iterate through the shuffled trial IDs and add the corresponding trial DataFrames to the list\n",
    "        for trial_id in trial_ids:\n",
    "            shuffled_trial_dfs.append(trial_groups.get_group(trial_id))\n",
    "\n",
    "        # Concatenate the shuffled trial DataFrames back into a single DataFrame\n",
    "        X_train = pd.concat(shuffled_trial_dfs, ignore_index=True)\n",
    "        X_train = train_set.drop(columns=['id', 'trial'])\n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True)\n",
    "        X_train = X_train.drop(columns=labels)\n",
    "        \n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        val_dataset = MyDataset(torch.tensor(X_val.values), torch.tensor(y_val.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=False)\n",
    "        \n",
    "        folder_path = 'C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/models/' \n",
    "        model_filename = model_name + '.pth'  \n",
    "\n",
    "        # Combine the folder path and model filename\n",
    "        full_model = os.path.join(folder_path, model_filename)\n",
    "\n",
    "        model.load_state_dict(torch.load(full_model))\n",
    "\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        # Initialize an empty list to store predictions\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "         # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, _ = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "\n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "\n",
    "\n",
    "\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds.numpy(), columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        accuracies = (predictions_df == y_train).mean()\n",
    "\n",
    "        print(\"Accuracy for each output state:\")\n",
    "        print(np.mean(accuracies))\n",
    "        #file_name = 'smh_shuffled.txt'\n",
    "        #file_path = \"C:/Users/kacpe/Desktop/study/research lab/lab_rotation_git/results_sm/\" + file_name  # Replace with your desired file path\n",
    "        value_to_save = model_name+\"_\"+\"oc\"+\": \"+str(np.mean(accuracies))  # Replace with the value you want to save\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"a\") as file:\n",
    "                # 2. Write the value to the file\n",
    "                file.write(value_to_save)\n",
    "                print(f\"Value '{value_to_save}' saved to '{file_path}' successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('models_comparison_v3.csv')\n",
    "\n",
    "# Set the \"shift\" column as the index (optional, for better x-axis labels)\n",
    "df = df[::-1]\n",
    "df.set_index('T', inplace=True)\n",
    "\n",
    "# Create a line plot for each model's performance\n",
    "#plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define custom colors for each model\n",
    "colors = {\n",
    "    'null': 'silver',\n",
    "    'solo': 'magenta',\n",
    "    'dyadic': 'blue'\n",
    "}\n",
    "\n",
    "# Create a line plot for each model's performance with custom colors\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for model in ['null','solo','dyadic']:\n",
    "    plt.plot(df.index, df[model], marker='o', label=model, color=colors[model])\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlim(max(df.index),min(df.index))\n",
    "plt.xlabel('T (shift in seconds)')\n",
    "plt.ylabel('Accuracy')\n",
    "#plt.title('Performance of Four Models')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "df_human = pd.read_csv(\"data_human_jarvis.csv\")\n",
    "df_human = df_human.drop(columns=['H_id','s_1','s_2','s_3','s_4'])\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "# List of column names to drop\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "# Concatenate the two dataframes vertically (along rows)\n",
    "df = pd.concat([df_human,df], axis=1)\n",
    "\n",
    "# Reset the index of the concatenated dataframe\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial','H_trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "scaled_df = scaled_df.dropna(axis=0)\n",
    "cols_to_omit = ['id','trial']\n",
    "# Use the drop method to exclude columns\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 46\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =201\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1','2','3','4']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Create the heatmap plot\n",
    "heatmap = ax.imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('Time Step')\n",
    "ax.set_ylabel('Predicted State')\n",
    "ax.set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels\n",
    "ax.set_yticks(np.arange(len(y_labels)))\n",
    "ax.set_yticklabels(y_labels)\n",
    "\n",
    "# Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Create the heatmap plot in the first subplot\n",
    "heatmap = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_xlabel('Time Step')\n",
    "axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Probabilities of Predicting Output States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second plot (line plot) in the second subplot\n",
    "axs[1].plot(np.arange(len(y_val)), y_val, color='green', label='Actual Values')\n",
    "axs[1].set_xlabel('Time Step')\n",
    "axs[1].set_ylabel('Actual Value')\n",
    "axs[1].set_title('Actual Test Values for Y')\n",
    "axs[1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_values = y_val\n",
    "# Define the labels for the y-axis (predicted states)\n",
    "y_labels = ['1', '2', '3', '4']\n",
    "\n",
    "# Create a figure with a grid of subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(2,1, figsize=(8,10))\n",
    "\n",
    "# Create the first heatmap plot in the first subplot\n",
    "heatmap1 = axs[0].imshow(all_probs_array.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the first heatmap subplot\n",
    "cbar1 = plt.colorbar(heatmap1, ax=axs[0])\n",
    "\n",
    "# Set the axis labels and title for the first heatmap subplot\n",
    "#axs[0].set_xlabel('Time Step')\n",
    "#axs[0].set_ylabel('Predicted State')\n",
    "axs[0].set_title('Predicted States Probabilities')\n",
    "\n",
    "# Set the y-axis ticks and labels for the first heatmap subplot\n",
    "axs[0].set_yticks(np.arange(len(y_labels)))\n",
    "axs[0].set_yticklabels(y_labels)\n",
    "\n",
    "# Create the second heatmap plot in the second subplot\n",
    "heatmap2 = axs[1].imshow(actual_values.T, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the second heatmap subplot\n",
    "cbar2 = plt.colorbar(heatmap2, ax=axs[1])\n",
    "\n",
    "# Set the axis labels and title for the second heatmap subplot\n",
    "axs[1].set_xlabel('Time Step')\n",
    "#axs[1].set_ylabel('Predicted State')\n",
    "axs[1].set_title('True States')\n",
    "\n",
    "# Set the y-axis ticks and labels for the second heatmap subplot\n",
    "axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "axs[1].set_yticklabels(y_labels)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gates and hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUCellNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=True)                \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.gru_cells = nn.ModuleList([\n",
    "            nn.GRUCell(input_dim, hidden_dim) if i == 0 else nn.GRUCell(hidden_dim, hidden_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add BatchNorm outside GRU cells\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h=None, hidden_states=None):\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), self.hidden_dim)  # Initialize hidden state for the single layer\n",
    "\n",
    "        hidden_states = []  # Initialize a list to store hidden states at each time step\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            input_t = x[:, t, :]\n",
    "            h = self.gru(input_t, h)  # Update the hidden state for the single layer\n",
    "            hidden_states.append(h)  # Append the hidden state for the current time step\n",
    "\n",
    "        h = self.batch_norm(h)  # Apply batch normalization to the final hidden state\n",
    "        out = self.fc(self.softplus(h))  # Calculate the output based on the final hidden state\n",
    "\n",
    "        # Return the output and the list of hidden states for the entire sequence\n",
    "        return out, h, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, batch_hidden_states = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(batch_hidden_states[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn, scheduler=None, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "        y_prob = torch.sigmoid(out)\n",
    "        # Append the predicted probabilities to the list\n",
    "        predicted_probs.append(y_prob.cpu().detach().numpy())\n",
    "        loss = loss_fn(out, y)\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "        \n",
    "        y_pred = torch.round(y_prob)\n",
    "        epoch_correct += sum((y == y_pred).flatten()).item()\n",
    "        epoch_total += y.numel()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch_utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels.extend(zip(y_pred.cpu().detach().cpu().numpy(), y.cpu().numpy()))\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(dataloader, model, loss_fn, device=None):\n",
    "    epoch_loss = []\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    model = model.to(device).float()\n",
    "    model.eval()\n",
    "    predicted_probs = []\n",
    "    predicted_labels = []\n",
    "    hidden_states = []  # Initialize a list to store hidden states\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device).float()\n",
    "            y = y.to(device).float()\n",
    "            \n",
    "            out, last_hidden_states, hidden_states_batch = model(x)  # Collect hidden states here\n",
    "            \n",
    "            loss = loss_fn(out, y)\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            hidden_states.append(hidden_states_batch)  # Append hidden states to the list\n",
    "            \n",
    "            y_pred = torch.sigmoid(out)\n",
    "            predicted_probs.append(y_pred.cpu().detach().numpy())\n",
    "            y_pred = torch.round(y_pred)\n",
    "            epoch_correct += sum((y == y_pred).flatten())\n",
    "            epoch_total += y.numel()\n",
    "            predicted_labels.extend(zip(y_pred.cpu().numpy(), y.cpu().numpy()))\n",
    "    \n",
    "    return np.mean(epoch_loss), accuracy(epoch_correct, epoch_total), predicted_labels, predicted_probs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_dataloader, val_dataloader, model, optimizer, loss_fn, num_epochs, scheduler=None, device=None, schedule_on_train=True, verbose=True):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    train_hidden_states, val_hidden_states = [], [] # Initialize lists to store hidden states\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_loss, epoch_train_acc, train_preds, train_probs, train_hidden = train(train_dataloader, model, optimizer, loss_fn, scheduler, device)\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        train_hidden_states.extend(train_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        if val_dataloader is not None:\n",
    "            epoch_val_loss, epoch_val_acc, val_preds, val_probs, val_hidden = validate(val_dataloader, model, loss_fn, device)\n",
    "        \n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accs.append(epoch_val_acc)\n",
    "\n",
    "            val_hidden_states.extend(val_hidden)  # Append hidden states to the list\n",
    "        \n",
    "        #if isinstance(scheduler, ReduceLROnPlateau):\n",
    "        #    scheduler.step(epoch_train_acc if schedule_on_train or val_dataloader is None else epoch_val_acc)\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            val_str = f\", val loss: {epoch_val_loss}, val acc: {epoch_val_acc}\" if val_dataloader is not None else \"\"\n",
    "            print(f\"Epoch {epoch}, train loss: {epoch_train_loss}, train acc: {epoch_train_acc}{val_str}\")\n",
    "        if epoch == num_epochs - 1:  # Store values only for the final epoch\n",
    "            train_predicted_labels = train_preds\n",
    "            #val_predicted_labels = val_preds\n",
    "            train_probs_final = train_probs\n",
    "            #val_probs_final = val_probs\n",
    "            if val_dataloader is not None:\n",
    "                val_predicted_labels = val_preds\n",
    "                val_probs_final = val_probs\n",
    "\n",
    "    if val_dataloader is not None:        \n",
    "        return train_losses, train_accs, val_losses, val_accs, train_predicted_labels, val_predicted_labels, train_probs_final, val_probs_final, train_hidden_states, val_hidden_states\n",
    "    else: \n",
    "        return train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nose_x', 'nose_y', 'nose_z', 'headTop_x', 'headTop_y', 'headTop_z',\n",
       "       'neck_x', 'neck_y', 'neck_z', 'tailBase_x', 'tailBase_y', 'tailBase_z',\n",
       "       'lEar_x', 'lEar_y', 'lEar_z', 'lShoulder_x', 'lShoulder_y',\n",
       "       'lShoulder_z', 'lElbow_x', 'lElbow_y', 'lElbow_z', 'lWrist_x',\n",
       "       'lWrist_y', 'lWrist_z', 'lHip_x', 'lHip_y', 'lHip_z', 'rEar_x',\n",
       "       'rEar_y', 'rEar_z', 'rShoulder_x', 'rShoulder_y', 'rShoulder_z',\n",
       "       'rElbow_x', 'rElbow_y', 'rElbow_z', 'rWrist_x', 'rWrist_y', 'rWrist_z',\n",
       "       'rHip_x', 'rHip_y', 'rHip_z', 's_1', 's_2', 's_3', 's_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_to_drop = ['lKnee_x','lKnee_y','lKnee_z','lAnkle_x','lAnkle_y','lAnkle_z','rKnee_x','rKnee_y','rKnee_z','rAnkle_x','rAnkle_y','rAnkle_z']\n",
    "file = \"data_model_v3.csv\"\n",
    "df = pd.read_csv(file)\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "labels = ['s_1','s_2','s_3','s_4']\n",
    "data_to_scale = df.drop(columns=['id', 'trial'])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_to_scale)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=data_to_scale.columns)\n",
    "scaled_df[['id', 'trial']] = df[['id', 'trial']]\n",
    "cols_to_omit = ['id','trial']\n",
    "columns_to_shift = scaled_df.drop(cols_to_omit, axis=1).columns\n",
    "columns_to_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_dataloader:\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out, last_hidden_states, batch_hidden_states = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hidden_states[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_hidden_states[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array = batch_hidden_states[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(1, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs.imshow(hidden_states_array, cmap='coolwarm', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs.set_ylabel('hidden unit')\n",
    "axs.set_xlabel('time step')\n",
    "axs.set_title('Heatmap of hidden states')\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.shape[1]-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_values = np.array([10,80,130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.99318927526474, train acc: 0.6004935459377373\n",
      "Epoch 50, train loss: 0.08497070521116257, train acc: 0.9844343204252088\n",
      "Epoch 0, train loss: 0.43113425374031067, train acc: 0.8743532560214095\n",
      "Epoch 50, train loss: 0.18211303651332855, train acc: 0.9378679750223016\n",
      "Epoch 0, train loss: 0.3486967086791992, train acc: 0.8874617737003058\n",
      "Epoch 50, train loss: 0.2525496780872345, train acc: 0.904638124362895\n",
      "Epoch 0, train loss: 0.43490543961524963, train acc: 0.9182550115018074\n",
      "Epoch 50, train loss: 0.07446524500846863, train acc: 0.9850065724613868\n",
      "Epoch 0, train loss: 0.3527289927005768, train acc: 0.8772032902467685\n",
      "Epoch 50, train loss: 0.20687879621982574, train acc: 0.9199471210340776\n",
      "Epoch 0, train loss: 0.3726874589920044, train acc: 0.8681343622333182\n",
      "Epoch 50, train loss: 0.34488749504089355, train acc: 0.8746028143440763\n",
      "Epoch 0, train loss: 0.20412753522396088, train acc: 0.9521866582158136\n",
      "Epoch 50, train loss: 0.16878153383731842, train acc: 0.9604262398985898\n",
      "Epoch 0, train loss: 0.1832096129655838, train acc: 0.9315794410054399\n",
      "Epoch 50, train loss: 0.18222378194332123, train acc: 0.9335490527105609\n",
      "Epoch 0, train loss: 0.3256382346153259, train acc: 0.876592528611531\n",
      "Epoch 50, train loss: 0.3240588307380676, train acc: 0.877780177067588\n",
      "Epoch 0, train loss: 0.16090203821659088, train acc: 0.9623314829500397\n",
      "Epoch 50, train loss: 0.16017913818359375, train acc: 0.9624504361617764\n",
      "Epoch 0, train loss: 0.18131472170352936, train acc: 0.9329107981220657\n",
      "Epoch 50, train loss: 0.18129833042621613, train acc: 0.9329107981220657\n",
      "Epoch 0, train loss: 0.3272723853588104, train acc: 0.8788648648648648\n",
      "Epoch 50, train loss: 0.3272233009338379, train acc: 0.8788648648648648\n",
      "Epoch 0, train loss: 0.17304927110671997, train acc: 0.956870043696391\n",
      "Epoch 50, train loss: 0.17301636934280396, train acc: 0.956870043696391\n",
      "Epoch 0, train loss: 0.18730944395065308, train acc: 0.9333044816310829\n",
      "Epoch 50, train loss: 0.18730823695659637, train acc: 0.9333044816310829\n",
      "Epoch 0, train loss: 0.33237120509147644, train acc: 0.8749722160480107\n",
      "Epoch 50, train loss: 0.33236998319625854, train acc: 0.8749722160480107\n",
      "Epoch 0, train loss: 0.1716112494468689, train acc: 0.9597658808120497\n",
      "Epoch 50, train loss: 0.17161104083061218, train acc: 0.9597658808120497\n",
      "Epoch 0, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 50, train loss: 0.19612698256969452, train acc: 0.9316010140405616\n",
      "Epoch 0, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 50, train loss: 0.33461084961891174, train acc: 0.8773148148148148\n",
      "Epoch 0, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 50, train loss: 0.1586802750825882, train acc: 0.9634836065573771\n",
      "Epoch 0, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 50, train loss: 0.18959751725196838, train acc: 0.926123046875\n",
      "Epoch 0, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 50, train loss: 0.3384883999824524, train acc: 0.8721719457013575\n",
      "Epoch 0, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 50, train loss: 0.15686364471912384, train acc: 0.9620009430996542\n",
      "Epoch 0, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 50, train loss: 0.1761421412229538, train acc: 0.9358045336306205\n",
      "Epoch 0, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 50, train loss: 0.31047919392585754, train acc: 0.8850384451089278\n",
      "Epoch 0, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 50, train loss: 0.15053516626358032, train acc: 0.9658147484942211\n",
      "Epoch 0, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 50, train loss: 0.19236014783382416, train acc: 0.9261572729033508\n",
      "Epoch 0, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 50, train loss: 0.34757092595100403, train acc: 0.87138695944432\n",
      "Epoch 0, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 50, train loss: 0.16347695887088776, train acc: 0.9592670401493931\n",
      "Epoch 0, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 50, train loss: 0.17688846588134766, train acc: 0.9364671318398825\n",
      "Epoch 0, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 50, train loss: 0.3195754885673523, train acc: 0.8832701222081754\n",
      "Epoch 0, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 50, train loss: 0.15976805984973907, train acc: 0.9597117202268431\n",
      "Epoch 0, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 50, train loss: 0.18115605413913727, train acc: 0.9354042473919523\n",
      "Epoch 0, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 50, train loss: 0.3274189233779907, train acc: 0.8799271636675235\n",
      "Epoch 0, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 50, train loss: 0.1595577448606491, train acc: 0.9623202001250781\n",
      "Epoch 0, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 50, train loss: 0.17294396460056305, train acc: 0.9335764401772526\n",
      "Epoch 0, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 50, train loss: 0.31131380796432495, train acc: 0.883481764206955\n",
      "Epoch 0, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 50, train loss: 0.15928839147090912, train acc: 0.9598129359543437\n",
      "Epoch 0, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 50, train loss: 0.178154855966568, train acc: 0.9354823573573574\n",
      "Epoch 0, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 50, train loss: 0.3210528492927551, train acc: 0.8809961106309421\n",
      "Epoch 0, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 50, train loss: 0.15765558183193207, train acc: 0.9605516356638871\n",
      "Epoch 0, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 50, train loss: 0.17631839215755463, train acc: 0.9354547184170472\n",
      "Epoch 0, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 50, train loss: 0.3071252107620239, train acc: 0.8826821773485514\n",
      "Epoch 0, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 50, train loss: 0.15200155973434448, train acc: 0.9619014194353455\n",
      "Epoch 0, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 50, train loss: 0.1814945787191391, train acc: 0.930445590130731\n",
      "Epoch 0, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n",
      "Epoch 50, train loss: 0.32141032814979553, train acc: 0.8780384696681462\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(1,16):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "\n",
    "\n",
    "        columns = ['s_1','s_2','s_3','s_4']\n",
    "        all_probs_df = pd.DataFrame(all_probs_array)\n",
    "        all_probs_df.columns = columns\n",
    "        # Convert the tensor of predictions to a DataFrame\n",
    "        predictions_df = pd.DataFrame(all_preds_array, columns=y_train.columns, index=y_train.index)\n",
    "        # Calculate accuracy for each output state\n",
    "        \n",
    "        # Create a figure with a single subplot\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(16,10))  # Adjust the figsize as needed\n",
    "\n",
    "        # Create the heatmap plot in the single subplot\n",
    "        heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('hidden unit')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        axs[0].set_title('Heatmap of hidden states for s='+str(i)+' trial='+str(j))\n",
    "        \n",
    "        axs[1].imshow(all_probs_array.T, cmap='seismic', aspect='auto', interpolation='none')\n",
    "        y_labels = ['1', '2', '3', '4']\n",
    "        axs[1].set_yticks(np.arange(len(y_labels)))\n",
    "        axs[1].set_yticklabels(y_labels)\n",
    "        \n",
    "        # Add colorbar to the heatmap subplot\n",
    "        #cbar = plt.colorbar(heatmap, ax=axs)\n",
    "        plot_title = 'solo_s'+str(i)+'_val'+str(j)\n",
    "        #plt.title(plot_title)\n",
    "        # Set the axis labels and title for the heatmap subplot\n",
    "        axs[0].set_ylabel('predictions')\n",
    "        axs[0].set_xlabel('time step')\n",
    "        #axs[0].set_title()\n",
    "        # Adjust spacing between the subplot\n",
    "        plt.tight_layout()\n",
    "        # Show the plot\n",
    "        # Specify the file path\n",
    "        # Specify the directory where you want to save the plots\n",
    "        save_directory = r'C:\\Users\\kacpe\\Desktop\\study\\research lab\\lab_rotation_git\\hidden_states_heatmaps_train'\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        # Define the plot title\n",
    "        plot_title = f'solo_s{i}_val{j}'\n",
    "\n",
    "        # Specify the file path using os.path.join\n",
    "        file_path = os.path.join(save_directory, f'{plot_title}.png')\n",
    "\n",
    "        # Save the plot to the specified path\n",
    "        fig.savefig(file_path)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = scaled_df.shape[1]-2\n",
    "hidden_dim = 34\n",
    "output_dim = 4\n",
    "num_layers = 1\n",
    "n_epochs =100\n",
    "lr = 0.01\n",
    "\n",
    "# Create an instance of GRUCellNet\n",
    "model = GRUCellNet(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "class_weights = torch.tensor([1.8]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights)  \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 50, train loss: 0.14604417979717255, train acc: 0.9692057704502219\n",
      "Epoch 0, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 50, train loss: 0.19220440089702606, train acc: 0.9305086336336337\n",
      "Epoch 0, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 50, train loss: 0.32724639773368835, train acc: 0.8729472774416595\n",
      "Epoch 0, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 50, train loss: 0.14533913135528564, train acc: 0.9692511225144324\n",
      "Epoch 0, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 50, train loss: 0.19234581291675568, train acc: 0.9297945205479452\n",
      "Epoch 0, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n",
      "Epoch 50, train loss: 0.3151167333126068, train acc: 0.8735733099209834\n"
     ]
    }
   ],
   "source": [
    "#hidden_states_all = []\n",
    "hidden_states_dict = {}\n",
    "for j in np.arange(13,15):\n",
    "    for i in shift_values:\n",
    "        shift = i\n",
    "        set_values = j\n",
    "        model_name = 's_m'+str(shift)+'_'+'val'+str(set_values)+'_gates'\n",
    "\n",
    "\n",
    "        # Create an empty DataFrame to store the shifted data\n",
    "        shifted_df = pd.DataFrame()\n",
    "\n",
    "        # Loop through unique trial values\n",
    "        for trial_value in scaled_df['trial'].unique():\n",
    "            # Filter the DataFrame for the current trial\n",
    "            trial_df = scaled_df[scaled_df['trial'] == trial_value].copy()\n",
    "\n",
    "            # Create shifted columns for each column in columns_to_shift\n",
    "            for col in columns_to_shift:\n",
    "                new_col_name = col + '_minus_' + str(shift)\n",
    "                trial_df[new_col_name] = trial_df[col].shift(shift)\n",
    "\n",
    "            # Drop the last 'i' records for each trial\n",
    "            trial_df = trial_df.dropna()\n",
    "\n",
    "            # Append the modified trial_df to the shifted_df\n",
    "            shifted_df = shifted_df.append(trial_df, ignore_index=True)\n",
    "        \n",
    "        #selected_columns = ['id', 'trial','s_1_minus_'+str(shift),'s_2_minus_'+str(shift),'s_3_minus_'+str(shift),'s_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        selected_columns = ['id', 'trial','nose_x_minus_'+str(shift), 'nose_y_minus_'+str(shift),\n",
    "        'nose_z_minus_'+str(shift), 'headTop_x_minus_'+str(shift), 'headTop_y_minus_'+str(shift),\n",
    "        'headTop_z_minus_'+str(shift), 'neck_x_minus_'+str(shift), 'neck_y_minus_'+str(shift),\n",
    "        'neck_z_minus_'+str(shift), 'tailBase_x_minus_'+str(shift), 'tailBase_y_minus_'+str(shift),\n",
    "        'tailBase_z_minus_'+str(shift), 'lEar_x_minus_'+str(shift), 'lEar_y_minus_'+str(shift),\n",
    "        'lEar_z_minus_'+str(shift), 'lShoulder_x_minus_'+str(shift), 'lShoulder_y_minus_'+str(shift),\n",
    "        'lShoulder_z_minus_'+str(shift), 'lElbow_x_minus_'+str(shift), 'lElbow_y_minus_'+str(shift),\n",
    "        'lElbow_z_minus_'+str(shift), 'lWrist_x_minus_'+str(shift), 'lWrist_y_minus_'+str(shift),\n",
    "        'lWrist_z_minus_'+str(shift), 'lHip_x_minus_'+str(shift), 'lHip_y_minus_'+str(shift),\n",
    "        'lHip_z_minus_'+str(shift), 'rEar_x_minus_'+str(shift), 'rEar_y_minus_'+str(shift), 'rEar_z_minus_'+str(shift),\n",
    "        'rShoulder_x_minus_'+str(shift), 'rShoulder_y_minus_'+str(shift), 'rShoulder_z_minus_'+str(shift),\n",
    "        'rElbow_x_minus_'+str(shift), 'rElbow_y_minus_'+str(shift), 'rElbow_z_minus_'+str(shift),\n",
    "        'rWrist_x_minus_'+str(shift), 'rWrist_y_minus_'+str(shift), 'rWrist_z_minus_'+str(shift),\n",
    "        'rHip_x_minus_'+str(shift), 'rHip_y_minus_'+str(shift), 'rHip_z_minus_'+str(shift), 's_1_minus_'+str(shift),\n",
    "        's_2_minus_'+str(shift), 's_3_minus_'+str(shift), 's_4_minus_'+str(shift),'s_1','s_2','s_3','s_4']\n",
    "        shifted_df = shifted_df[selected_columns]\n",
    "        # Step 5: Split the data into training and test sets based on the 'trial' column\n",
    "        train_set = shifted_df[shifted_df['trial']!=set_values].drop(columns=['id', 'trial'])\n",
    "        test_set = shifted_df[shifted_df['trial']==set_values].drop(columns=['id', 'trial'])\n",
    "        full_set = shifted_df.drop(columns=['id','trial'])\n",
    "\n",
    "        # split data into x and y \n",
    "        X_train, y_train = train_set.drop(columns=labels), train_set[labels]\n",
    "        X_test, y_test = test_set.drop(columns=labels), test_set[labels]\n",
    "        X, y = full_set.drop(columns=labels), full_set[labels]\n",
    "        \n",
    "        # reset index \n",
    "        X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
    "        X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        X, y = X.reset_index(drop=True), y.reset_index(drop=True) \n",
    "\n",
    "        # Create custom datasets for training, validation, and testing\n",
    "        full_dataset = MyDataset(torch.tensor(X.values), torch.tensor(y.values))\n",
    "        train_dataset = MyDataset(torch.tensor(X_train.values), torch.tensor(y_train.values))\n",
    "        test_dataset = MyDataset(torch.tensor(X_test.values), torch.tensor(y_test.values))\n",
    "\n",
    "        fullset_dataloader = DataLoader(full_dataset, batch_size=X.shape[0], shuffle=False)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=False)\n",
    "        #print(X_train.shape[0])\n",
    "        \n",
    "\n",
    "        train_losses, train_accs, train_predicted_labels, train_probs_final, train_hidden_states = run_training(\n",
    "            train_dataloader, val_dataloader=None, model=model, optimizer=optimizer, loss_fn=loss_fn, num_epochs=n_epochs, scheduler=scheduler)\n",
    "        \n",
    "        #hidden_states_dict[(shift, set_values)] = train_hidden_states\n",
    "        \n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "        hidden_states = []\n",
    "\n",
    "        # Iterate through the test data batches\n",
    "        for inputs, _ in test_dataloader:\n",
    "            inputs = inputs.float()\n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                predictions, h, test_hidden_states = model(inputs)\n",
    "                probabilities = torch.sigmoid(predictions)\n",
    "                preds = torch.round(probabilities)\n",
    "\n",
    "            # Append predictions to the list\n",
    "            all_preds.append(preds)\n",
    "            all_probs.append(probabilities)\n",
    "            hidden_states.append(test_hidden_states)\n",
    "        \n",
    "        #print(hidden_states[0][0])    \n",
    "        hidden_states_array = hidden_states[0][0].detach().numpy()\n",
    "        hidden_states_array = hidden_states_array.T\n",
    "        #print(hidden_states_array.shape) \n",
    "        \n",
    "        # Concatenate the predicted batches\n",
    "        all_preds = torch.cat(all_preds, dim=0)\n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "        all_preds_array = all_preds.numpy()\n",
    "        all_probs_array = all_probs.numpy()\n",
    "        h_name = 'hidden_s'+str(i)+'_trial'+str(j)\n",
    "        p_name = 'probs_s'+str(i)+'_trial'+str(j)\n",
    "        np.save(h_name, hidden_states_array)\n",
    "        np.save(p_name, all_probs_array)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "        \n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('hidden_states.npy', hidden_states_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_value = list(hidden_states_dict.values())[0]\n",
    "type(first_value)\n",
    "hidden_states_array = first_value[0][0].detach().numpy()\n",
    "hidden_states_array = hidden_states_array.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a single subplot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14,6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create the heatmap plot in the single subplot\n",
    "heatmap = axs[0].imshow(hidden_states_array, cmap='seismic', aspect='auto', interpolation='none')\n",
    "\n",
    "# Add colorbar to the heatmap subplot\n",
    "cbar = plt.colorbar(heatmap, ax=axs)\n",
    "\n",
    "# Set the axis labels and title for the heatmap subplot\n",
    "axs[0].set_ylabel('hidden unit')\n",
    "axs[0].set_xlabel('time step')\n",
    "axs[0].set_title('Heatmap of hidden states')\n",
    "\n",
    "\n",
    "\n",
    "# Adjust spacing between the subplot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
